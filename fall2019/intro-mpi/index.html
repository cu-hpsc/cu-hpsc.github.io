<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Jed Brown">

  
  
  
    
  
  <meta name="description" content="def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(&quot;```c\n&quot; &#43; contents &#43; &quot;```\n&quot;)  Processes and Threads Threads and processes are very similar * Both created via clone system call on Linux * Scheduled in the same way by the operating system * Separate stacks (automatic variables) * Access to same memory before fork() or clone()
with some important distinctions
 Threads set CLONE_VM  threads share the same virtual-to-physical address mapping threads can access the same data at the same addresses; private data is private only because other threads don&rsquo;t know its address  Threads set CLONE_FILES  threads share file descriptors  Threads set CLONE_THREAD, CLONE_SIGHAND  process id and signal handlers shared   Myths  Processes can&rsquo;t share memory  mmap(), shm_open(), and MPI_Win_allocate_shared()  Processes are &ldquo;heavy&rdquo;  same data structures and kernel scheduling; no difference in context switching data from parent is inherited copy-on-write at very low overhead startup costs ~100 microseconds to duplicate page tables caches are physically tagged; processes can share L1 cache   MPI: Message Passing Interface  Just a library: plain C, C&#43;&#43;, or Fortran compiler  Two active open source libraries: MPICH and Open MPI Numerous vendor implementations modify/extend these open source implementations MVAPICH is an MPICH-derived open source implementation for InfiniBand and related networks  Bindings from many other languages; mpi4py is popular Scales to millions of processes across ~100k nodes  Shared memory systems can be scaled up to ~4000 cores, but latency and price ($) increase  Standard usage: processes are separate on startup Timeline">

  
  <link rel="alternate" hreflang="en-us" href="https://cucs-hpsc.github.io/fall2019/intro-mpi/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.dd629241ea9333c62c071f4a25f829ff.css">

  

  
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://cucs-hpsc.github.io/fall2019/intro-mpi/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@five9a2">
  <meta property="twitter:creator" content="@five9a2">
  
  <meta property="og:site_name" content="HPSC">
  <meta property="og:url" content="https://cucs-hpsc.github.io/fall2019/intro-mpi/">
  <meta property="og:title" content="Introduction to MPI | HPSC">
  <meta property="og:description" content="def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(&quot;```c\n&quot; &#43; contents &#43; &quot;```\n&quot;)  Processes and Threads Threads and processes are very similar * Both created via clone system call on Linux * Scheduled in the same way by the operating system * Separate stacks (automatic variables) * Access to same memory before fork() or clone()
with some important distinctions
 Threads set CLONE_VM  threads share the same virtual-to-physical address mapping threads can access the same data at the same addresses; private data is private only because other threads don&rsquo;t know its address  Threads set CLONE_FILES  threads share file descriptors  Threads set CLONE_THREAD, CLONE_SIGHAND  process id and signal handlers shared   Myths  Processes can&rsquo;t share memory  mmap(), shm_open(), and MPI_Win_allocate_shared()  Processes are &ldquo;heavy&rdquo;  same data structures and kernel scheduling; no difference in context switching data from parent is inherited copy-on-write at very low overhead startup costs ~100 microseconds to duplicate page tables caches are physically tagged; processes can share L1 cache   MPI: Message Passing Interface  Just a library: plain C, C&#43;&#43;, or Fortran compiler  Two active open source libraries: MPICH and Open MPI Numerous vendor implementations modify/extend these open source implementations MVAPICH is an MPICH-derived open source implementation for InfiniBand and related networks  Bindings from many other languages; mpi4py is popular Scales to millions of processes across ~100k nodes  Shared memory systems can be scaled up to ~4000 cores, but latency and price ($) increase  Standard usage: processes are separate on startup Timeline"><meta property="og:image" content="https://cucs-hpsc.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://cucs-hpsc.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-09-24T06:49:25-06:00">
    
    <meta property="article:modified_time" content="2019-09-25T11:04:09-06:00">
  

  


  





  <title>Introduction to MPI | HPSC</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">HPSC</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/fall2019/"><span>Fall 2019</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      





<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/fall2019/">Logistics</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/fall2019/syllabus/">Syllabus</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/fall2019/intro-architecture/">Lecture Notes</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/fall2019/intro-architecture/">2019-08-28 Architecture</a>
      </li>
      
      <li >
        <a href="/fall2019/intro-vectorization/">2019-08-30 Vectorization</a>
      </li>
      
      <li >
        <a href="/fall2019/intro-modeling/">2019-09-04 Modeling</a>
      </li>
      
      <li >
        <a href="/fall2019/intro-parallel-scaling/">2019-09-06 Parallel Scaling</a>
      </li>
      
      <li >
        <a href="/fall2019/openmp/">2019-09-11 OpenMP Basics</a>
      </li>
      
      <li >
        <a href="/fall2019/openmp-2/">2019-09-13 More OpenMP</a>
      </li>
      
      <li >
        <a href="/fall2019/openmp-3/">2019-09-16 OpenMP Tasks</a>
      </li>
      
      <li >
        <a href="/fall2019/strategies/">2019-09-18 Reductions and Scans</a>
      </li>
      
      <li >
        <a href="/fall2019/sorting-graphs/">2019-09-23 More bitonic sorting, graphs</a>
      </li>
      
      <li class="active">
        <a href="/fall2019/intro-mpi/">2019-09-24 Introduction to MPI</a>
      </li>
      
      <li >
        <a href="/fall2019/dense-linalg/">2019-09-30 Dense Linear Algebra</a>
      </li>
      
      <li >
        <a href="/fall2019/dense-linalg-2/">2019-10-02 Dense Linear Algebra and Orthogonality</a>
      </li>
      
      <li >
        <a href="/fall2019/dense-linalg-3/">2019-10-04 Orthogonality and Conditioning</a>
      </li>
      
      <li >
        <a href="/fall2019/elemental/">2019-10-07 Elemental</a>
      </li>
      
      <li >
        <a href="/fall2019/iterative-solvers/">2019-10-09 Sparse and Iterative</a>
      </li>
      
      <li >
        <a href="/fall2019/preconditioning/">2019-10-11 Preconditioning</a>
      </li>
      
      <li >
        <a href="/fall2019/dd-preconditioning/">2019-10-14 DD Preconditioning</a>
      </li>
      
      <li >
        <a href="/fall2019/dd-preconditioning-2/">2019-10-16 DD Preconditioning 2</a>
      </li>
      
      <li >
        <a href="/fall2019/mg-preconditioning/">2019-10-18 Multilevel Preconditioning</a>
      </li>
      
      <li >
        <a href="/fall2019/nonlinear/">2019-10-21 Nonlinear</a>
      </li>
      
      <li >
        <a href="/fall2019/transient/">2019-10-23 Transient</a>
      </li>
      
      <li >
        <a href="/fall2019/libceed/">2019-10-25 libCEED</a>
      </li>
      
      <li >
        <a href="/fall2019/coprocessor/">2019-10-28 Coprocessors</a>
      </li>
      
      <li >
        <a href="/fall2019/cuda/">2019-10-30 GPUs and CUDA</a>
      </li>
      
      <li >
        <a href="/fall2019/cuda-practical/">2019-11-01 Practical CUDA</a>
      </li>
      
      <li >
        <a href="/fall2019/openmp-target/">2019-11-04 ISP/OpenMP/OpenACC</a>
      </li>
      
      <li >
        <a href="/fall2019/io/">2019-11-06 HPC I/O</a>
      </li>
      
      <li >
        <a href="/fall2019/mpi-io/">2019-11-08 MPI-IO</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/fall2019/trends/">Resources</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/fall2019/trends/">Trends</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#processes-and-threads">Processes and Threads</a>
<ul>
<li>
<ul>
<li><a href="#myths">Myths</a></li>
</ul></li>
</ul></li>
<li><a href="#mpi-message-passing-interface">MPI: Message Passing Interface</a>
<ul>
<li>
<ul>
<li><a href="#advice-from-bill-gropp">Advice from Bill Gropp</a></li>
</ul></li>
<li><a href="#communicators">Communicators</a></li>
<li><a href="#collective-operations">Collective operations</a></li>
<li><a href="#point-to-point-messaging">Point-to-point messaging</a></li>
<li><a href="#neighbors">Neighbors</a></li>
</ul></li>
</ul></li>
</ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article" itemscope itemtype="http://schema.org/Article">

        <div class="docs-article-container">
          <h1 itemprop="name">Introduction to MPI</h1>

          <div class="article-style" itemprop="articleBody">
            

<pre><code class="language-python">def render_c(filename):
    from IPython.display import Markdown
    with open(filename) as f:
        contents = f.read()
    return Markdown(&quot;```c\n&quot; + contents + &quot;```\n&quot;)
</code></pre>

<h2 id="processes-and-threads">Processes and Threads</h2>

<p>Threads and processes are very similar
* Both created via <a href="https://linux.die.net/man/2/clone" target="_blank"><code>clone</code> system call</a> on Linux
* Scheduled in the same way by the operating system
* Separate stacks (automatic variables)
* Access to same memory before <code>fork()</code> or <code>clone()</code></p>

<p>with some important distinctions</p>

<ul>
<li>Threads set <code>CLONE_VM</code>

<ul>
<li>threads share the same virtual-to-physical address mapping</li>
<li>threads can access the same data at the same addresses; private data is private only because other threads don&rsquo;t know its address</li>
</ul></li>
<li>Threads set <code>CLONE_FILES</code>

<ul>
<li>threads share file descriptors</li>
</ul></li>
<li>Threads set <code>CLONE_THREAD</code>, <code>CLONE_SIGHAND</code>

<ul>
<li>process id and signal handlers shared</li>
</ul></li>
</ul>

<h4 id="myths">Myths</h4>

<ul>
<li>Processes can&rsquo;t share memory

<ul>
<li><code>mmap()</code>, <code>shm_open()</code>, and <code>MPI_Win_allocate_shared()</code></li>
</ul></li>
<li>Processes are &ldquo;heavy&rdquo;

<ul>
<li>same data structures and kernel scheduling; no difference in context switching</li>
<li>data from parent is inherited copy-on-write at very low overhead</li>
<li>startup costs ~100 microseconds to duplicate page tables</li>
<li>caches are physically tagged; processes can share L1 cache</li>
</ul></li>
</ul>

<h2 id="mpi-message-passing-interface">MPI: Message Passing Interface</h2>

<ul>
<li>Just a library: plain C, C++, or Fortran compiler

<ul>
<li>Two active open source libraries: <a href="https://www.mpich.org/" target="_blank">MPICH</a> and <a href="https://www.open-mpi.org/" target="_blank">Open MPI</a></li>
<li>Numerous vendor implementations modify/extend these open source implementations</li>
<li>MVAPICH is an MPICH-derived open source implementation for InfiniBand and related networks</li>
</ul></li>
<li>Bindings from many other languages; <a href="https://mpi4py.readthedocs.io/en/stable/" target="_blank">mpi4py</a> is popular</li>
<li>Scales to millions of processes across ~100k nodes

<ul>
<li>Shared memory systems can be scaled up to <a href="https://www.uvhpc.com/sgi-uv-3000" target="_blank">~4000 cores</a>, but latency and price ($) increase</li>
</ul></li>
<li>Standard usage: processes are separate on startup</li>

<li><p>Timeline</p>

<ul>
<li>MPI-1 (1994) point-to-point messaging, collectives</li>
<li>MPI-2 (1997) parallel IO, dynamic processes, one-sided</li>

<li><p>MPI-3 (2012) nonblocking collectives, neighborhood collectives, improved one-sided</p>

<pre><code class="language-python">render_c('mpi-demo.c')
</code></pre>

<pre><code class="language-c">#include &lt;mpi.h&gt;
#include &lt;stdio.h&gt;

int main(int argc, char **argv) {
MPI_Init(&amp;argc, &amp;argv);   // Must call before any other MPI functions
int size, rank, sum;
MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;size);
MPI_Allreduce(&amp;rank, &amp;sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);
printf(&quot;I am rank %d of %d: sum=%d\n&quot;, rank, size, sum);
MPI_Finalize();
}
</code></pre></li>
</ul></li>
</ul>

<p>This may remind you of the top-level OpenMP strategy</p>

<pre><code class="language-c">int main() {
    #pragma omp parallel
    {
        int rank = omp_get_thread_num();
        int size = omp_get_num_threads();
        // your code
    }
}
</code></pre>

<ul>
<li><p>We use the compiler wrapper <code>mpicc</code>, but it just passes some flags to the real compiler.</p>

<pre><code class="language-python">! mpicc -show
</code></pre>

<p>gcc -pthread -Wl,-rpath -Wl,/usr/lib/openmpi -Wl,&ndash;enable-new-dtags -L/usr/lib/openmpi -lmpi</p>

<pre><code class="language-python">! make CC=mpicc CFLAGS=-Wall mpi-demo
</code></pre>

<p>mpicc -Wall    mpi-demo.c   -o mpi-demo</p></li>

<li><p>We use <code>mpiexec</code> to run locally.  Clusters/supercomputers often have different job launching programs.</p>

<pre><code class="language-python">! mpiexec -n 2 ./mpi-demo
</code></pre>

<p>I am rank 0 of 2: sum=1
I am rank 1 of 2: sum=1</p></li>

<li><p>We can run more MPI processes than cores (or hardware threads), but you might need to use the <code>--oversubscribe</code> option because <strong>oversubscription is usually expensive</strong>.</p>

<pre><code class="language-python">! mpiexec -n 6 --oversubscribe ./mpi-demo
</code></pre>

<p>I am rank 1 of 6: sum=15
I am rank 3 of 6: sum=15
I am rank 4 of 6: sum=15
I am rank 5 of 6: sum=15
I am rank 0 of 6: sum=15
I am rank 2 of 6: sum=15</p></li>

<li><p>You can use OpenMP within ranks of MPI (but use <code>MPI_Init_thread()</code>)</p></li>

<li><p>Everything is private by default</p></li>
</ul>

<h4 id="advice-from-bill-gropp">Advice from Bill Gropp</h4>

<blockquote>
<p>You want to think about how you decompose your data structures, how
    you think about them globally.  [&hellip;]  If you were building a house,
    you&rsquo;d start with a set of blueprints that give you a picture of what
    the whole house looks like.  You wouldn&rsquo;t start with a bunch of
    tiles and say. &ldquo;Well I&rsquo;ll put this tile down on the ground, and
    then I&rsquo;ll find a tile to go next to it.&rdquo;  But all too many people
    try to build their parallel programs by creating the smallest
    possible tiles and then trying to have the structure of their code
    emerge from the chaos of all these little pieces.  You have to have
    an organizing principle if you&rsquo;re going to survive making your code
    parallel.
    &ndash; <a href="https://www.rce-cast.com/Podcast/rce-28-mpich2.html" target="_blank">https://www.rce-cast.com/Podcast/rce-28-mpich2.html</a></p>
</blockquote>

<h3 id="communicators">Communicators</h3>

<ul>
<li><code>MPI_COMM_WORLD</code> contains all ranks in the <code>mpiexec</code>.  Those ranks may be on different nodes, even in different parts of the world.</li>
<li><code>MPI_COMM_SELF</code> contains only one rank</li>

<li><p>Can create new communicators from existing ones</p>

<pre><code class="language-c">int MPI_Comm_dup(MPI_Comm comm, MPI_Comm *newcomm);
int MPI_Comm_split(MPI_Comm comm, int color, int key, MPI_Comm *newcomm);
int MPI_Comm_create(MPI_Comm comm, MPI_Group group, MPI_Comm *newcomm);
</code></pre></li>

<li><p>Can spawn new processes (but not supported on all machines)</p>

<pre><code class="language-c">int MPI_Comm_spawn(const char *command, char *argv[], int maxprocs,
        MPI_Info info, int root, MPI_Comm comm,
        MPI_Comm *intercomm, int array_of_errcodes[]);
</code></pre></li>

<li><p>Can attach <em>attributes</em> to communicators (useful for library composition)</p></li>
</ul>

<h3 id="collective-operations">Collective operations</h3>

<p>MPI has a rich set of collective operations scoped by communicator, including the following.</p>

<pre><code class="language-c">int MPI_Allreduce(const void *sendbuf, void *recvbuf, int count,
        MPI_Datatype datatype, MPI_Op op, MPI_Comm comm);
int MPI_Reduce(const void *sendbuf, void *recvbuf, int count,
        MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm);
int MPI_Scan(const void *sendbuf, void *recvbuf, int count,
        MPI_Datatype datatype, MPI_Op op, MPI_Comm comm);
int MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype,
        void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm);
int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,
        void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm);
</code></pre>

<ul>
<li>Implementations are optimized by vendors for their custom networks, and can be very fast.</li>
</ul>

<p><img src="https://www.mcs.anl.gov/~fischer/gop/bgp_gop_png.png" alt="Fischer BGP" /></p>

<p>Notice how the time is basically independent of number of processes $P$, and only a small multiple of the cost to send a single message. Not all networks are this good.</p>

<h3 id="point-to-point-messaging">Point-to-point messaging</h3>

<p>In addition to collectives, MPI supports messaging directly between individual ranks.</p>

<p><img src="mpi-send-recv.png" alt="send-recv" /></p>

<ul>
<li>Interfaces can be:

<ul>
<li>blocking like <code>MPI_Send()</code> and <code>MPI_Recv()</code>, or</li>
<li>&ldquo;immediate&rdquo; (asynchronous), like <code>MPI_Isend()</code> and <code>MPI_Irecv()</code>.  The immediate varliants return an <code>MPI_Request</code>, which must be waited on to complete the send or receive.</li>
</ul></li>
<li>Be careful of deadlock when using blocking interfaces.

<ul>
<li>I never use blocking send/recv.</li>
<li>There are also &ldquo;synchronous&rdquo; <code>MPI_Ssend</code> and &ldquo;buffered&rdquo; <code>MPI_Bsend</code>, and nonblocking variants of these, <code>MPI_Issend</code>, etc.</li>
<li>I never use these either (with one cool exception that we&rsquo;ll talk about).</li>
</ul></li>
<li>Point-to-point messaging is like the assembly of parallel computing

<ul>
<li>It can be good for building libraries, but it&rsquo;s a headache to use directly for most purposes</li>
<li>Better to use collectives when possible, or higher level libraries</li>
</ul></li>
</ul>

<h3 id="neighbors">Neighbors</h3>

<p>A common pattern involves communicating with neighbors, often many times in sequence (such as each iteration or time step).</p>

<p><img src="mpi-neighbor-grid.png" alt="Neighbor comm" /></p>

<p>This can be achieved with
* Point-to-point: <code>MPI_Isend</code>, <code>MPI_Irecv</code>, <code>MPI_Waitall</code>
* Persistent: <code>MPI_Send_init</code> (once), <code>MPI_Startall</code>, <code>MPI_Waitall</code>.
* Neighborhood collectives (need to create special communicator)
* One-sided (need to manage safety yourself)</p>

          </div>

          



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/fall2019/dense-linalg/" rel="next">Dense Linear Algebra and Networks</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/fall2019/sorting-graphs/" rel="prev">More bitonic sorting, graphs</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">
          <p>Last updated on Sep 25, 2019</p>

          
<p class="edit-page">
  <a href="https://github.com/cucs-hpsc/hpsc-class/edit/master/content/fall2019/intro-mpi/index.md">
    <i class="fas fa-pen pr-2"></i>Edit this page
  </a>
</p>



          

        </div>

      </article>

      <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>


      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/c.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.16bbb3750feb7244c9bc409a5a4fe678.js"></script>

    






  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
