<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Jed Brown">

  
  
  
    
  
  <meta name="description" content="GPU vs CPU characterization CUDA preview Execution heirarchy Memory managerie Optimizations  Graphics Processing Units Graphics Processing Units (GPUs) evolved from commercial demand for high-definition graphics. HPC general purpose computing with GPUs picked up after programmable shaders were added in early 2000s.
GPU compute performance relative to CPU is not magic, rather it is based on difference in goals; GPUs were unpolluted by CPU demands for user-adaptability.
Nvidia.com: real-estate difference">

  
  <link rel="alternate" hreflang="en-us" href="https://cucs-hpsc.github.io/fall2019/cuda/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.dd629241ea9333c62c071f4a25f829ff.css">

  

  
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://cucs-hpsc.github.io/fall2019/cuda/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@five9a2">
  <meta property="twitter:creator" content="@five9a2">
  
  <meta property="og:site_name" content="HPSC">
  <meta property="og:url" content="https://cucs-hpsc.github.io/fall2019/cuda/">
  <meta property="og:title" content="GPUs and CUDA | HPSC">
  <meta property="og:description" content="GPU vs CPU characterization CUDA preview Execution heirarchy Memory managerie Optimizations  Graphics Processing Units Graphics Processing Units (GPUs) evolved from commercial demand for high-definition graphics. HPC general purpose computing with GPUs picked up after programmable shaders were added in early 2000s.
GPU compute performance relative to CPU is not magic, rather it is based on difference in goals; GPUs were unpolluted by CPU demands for user-adaptability.
Nvidia.com: real-estate difference"><meta property="og:image" content="https://cucs-hpsc.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://cucs-hpsc.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-10-30T06:49:25-06:00">
    
    <meta property="article:modified_time" content="2019-10-30T13:41:28-06:00">
  

  


  





  <title>GPUs and CUDA | HPSC</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">HPSC</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/fall2019/"><span>Fall 2019</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      





<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/fall2019/">Logistics</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/fall2019/syllabus/">Syllabus</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/fall2019/intro-architecture/">Lecture Notes</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/fall2019/intro-architecture/">2019-08-28 Architecture</a>
      </li>
      
      <li >
        <a href="/fall2019/intro-vectorization/">2019-08-30 Vectorization</a>
      </li>
      
      <li >
        <a href="/fall2019/intro-modeling/">2019-09-04 Modeling</a>
      </li>
      
      <li >
        <a href="/fall2019/intro-parallel-scaling/">2019-09-06 Parallel Scaling</a>
      </li>
      
      <li >
        <a href="/fall2019/openmp/">2019-09-11 OpenMP Basics</a>
      </li>
      
      <li >
        <a href="/fall2019/openmp-2/">2019-09-13 More OpenMP</a>
      </li>
      
      <li >
        <a href="/fall2019/openmp-3/">2019-09-16 OpenMP Tasks</a>
      </li>
      
      <li >
        <a href="/fall2019/strategies/">2019-09-18 Reductions and Scans</a>
      </li>
      
      <li >
        <a href="/fall2019/sorting-graphs/">2019-09-23 More bitonic sorting, graphs</a>
      </li>
      
      <li >
        <a href="/fall2019/intro-mpi/">2019-09-24 Introduction to MPI</a>
      </li>
      
      <li >
        <a href="/fall2019/dense-linalg/">2019-09-30 Dense Linear Algebra</a>
      </li>
      
      <li >
        <a href="/fall2019/dense-linalg-2/">2019-10-02 Dense Linear Algebra and Orthogonality</a>
      </li>
      
      <li >
        <a href="/fall2019/dense-linalg-3/">2019-10-04 Orthogonality and Conditioning</a>
      </li>
      
      <li >
        <a href="/fall2019/elemental/">2019-10-07 Elemental</a>
      </li>
      
      <li >
        <a href="/fall2019/iterative-solvers/">2019-10-09 Sparse and Iterative</a>
      </li>
      
      <li >
        <a href="/fall2019/preconditioning/">2019-10-11 Preconditioning</a>
      </li>
      
      <li >
        <a href="/fall2019/dd-preconditioning/">2019-10-14 DD Preconditioning</a>
      </li>
      
      <li >
        <a href="/fall2019/dd-preconditioning-2/">2019-10-16 DD Preconditioning 2</a>
      </li>
      
      <li >
        <a href="/fall2019/mg-preconditioning/">2019-10-18 Multilevel Preconditioning</a>
      </li>
      
      <li >
        <a href="/fall2019/nonlinear/">2019-10-21 Nonlinear</a>
      </li>
      
      <li >
        <a href="/fall2019/transient/">2019-10-23 Transient</a>
      </li>
      
      <li >
        <a href="/fall2019/libceed/">2019-10-25 libCEED</a>
      </li>
      
      <li >
        <a href="/fall2019/coprocessor/">2019-10-28 Coprocessors</a>
      </li>
      
      <li class="active">
        <a href="/fall2019/cuda/">2019-10-30 GPUs and CUDA</a>
      </li>
      
      <li >
        <a href="/fall2019/cuda-practical/">2019-11-01 Practical CUDA</a>
      </li>
      
      <li >
        <a href="/fall2019/openmp-target/">2019-11-04 ISP/OpenMP/OpenACC</a>
      </li>
      
      <li >
        <a href="/fall2019/io/">2019-11-06 HPC I/O</a>
      </li>
      
      <li >
        <a href="/fall2019/mpi-io/">2019-11-08 MPI-IO</a>
      </li>
      
      <li >
        <a href="/fall2019/data-methods/">2019-11-11 Data-intensive</a>
      </li>
      
      <li >
        <a href="/fall2019/data-probability/">2019-11-13 Data and Probability</a>
      </li>
      
      <li >
        <a href="/fall2019/interactive/">2019-11-15 Dynamic/Interactive</a>
      </li>
      
      <li >
        <a href="/fall2019/nbody/">2019-11-18 Intro N-body</a>
      </li>
      
      <li >
        <a href="/fall2019/nbody-longrange/">2019-11-20 Long-range N-body</a>
      </li>
      
      <li >
        <a href="/fall2019/md/">2019-11-22 Molecular Dynamics</a>
      </li>
      
      <li >
        <a href="/fall2019/git/">2019-12-02 Git Workflows</a>
      </li>
      
      <li >
        <a href="/fall2019/fft/">2019-12-04 Fourier</a>
      </li>
      
      <li >
        <a href="/fall2019/multigrid/">2019-12-06 Multigrid Intro</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/fall2019/trends/">Resources</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/fall2019/trends/">Trends</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#graphics-processing-units">Graphics Processing Units</a></li>
<li><a href="#cuda">CUDA</a>
<ul>
<li><a href="#kernel-syntax-example">Kernel syntax example</a></li>
<li><a href="#thread-heirarchy">Thread Heirarchy</a></li>
<li><a href="#memory">Memory</a></li>
<li><a href="#memory-transfer-example">Memory transfer example</a></li>
</ul></li>
<li><a href="#optimization-details">Optimization Details</a>
<ul>
<li><a href="#intrinsic-function-instructions">Intrinsic function instructions</a></li>
<li><a href="#memory-1">Memory</a>
<ul>
<li><a href="#hiding-memory-transfers">Hiding Memory Transfers:</a></li>
<li><a href="#global-memory-access-size-and-alignment">Global memory access size and alignment:</a></li>
<li><a href="#coalescence">Coalescence:</a></li>
<li><a href="#bank-distribution">Bank Distribution:</a></li>
<li><a href="#texture-specific-memory-features">Texture-specific memory features:</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article" itemscope itemtype="http://schema.org/Article">

        <div class="docs-article-container">
          <h1 itemprop="name">GPUs and CUDA</h1>

          <div class="article-style" itemprop="articleBody">
            

<ul>
<li>GPU vs CPU characterization</li>
<li>CUDA preview</li>
<li>Execution heirarchy</li>
<li>Memory managerie</li>
<li>Optimizations</li>
</ul>

<h2 id="graphics-processing-units">Graphics Processing Units</h2>

<p>Graphics Processing Units (GPUs) evolved from commercial demand for high-definition graphics.  HPC general purpose computing with GPUs picked up after programmable shaders were added in early 2000s.</p>

<p>GPU compute performance relative to CPU is not magic, rather it is based on difference in goals; GPUs were unpolluted by CPU demands for user-adaptability.</p>

<p><img src="gpu-devotes-more-transistors-to-data-processing.png" width="600"/>
<center><i>Nvidia.com: real-estate difference</i></center></p>

<p>GPUs have no* branch prediction and no speculative execution.  (In the early days, computational uses even needed to implement their own error correction in software!)  Longer memory access latencies from tiny cache size is meant to be hidden behind co-resident compute.  The difference in mentality allowed GPUs to far surpass CPU compute efficiency.</p>

<p><em>*: recent devices use branch prediction to group divergent threads</em></p>

<p><img src="karlrupp.net_costpercomputetrend.png" alt="karlrupp.net: compute efficiency" />
<center><i>karlrupp.net: compute efficiency</i></center></p>

<p>Power can dominant the cost in HPC. Consider the Summit supercomputer:</p>

<ul>
<li>#2 GREEN500 (was #3, but #1 was decomissioned)</li>
<li>cost \$200 million to build</li>
<li>13 MW to run compute+interconnect+file systems =&gt; roughly \$7 million/year in raw electricity to power</li>
<li>(does not count facilities/infrastructure cost to actually supply this power, nor cooling)</li>
</ul>

<p>The drawbacks: GPU efficiency needs the problem to fit well into SIMD and have a relatively high computation intensity.</p>

<h2 id="cuda">CUDA</h2>

<p>Early general purpose computing GPU efforts required formulating problems in terms of graphics primatives (e.g. DirectX).</p>

<p>NVIDIA publicly launched CUDA in 2006, allowing programming in C (and Fortran).</p>

<p>Flash forward to 2019: AMD has its own language and there are also several vendor-independent languages (dominant: OpenCL), but CUDA still dominates overall.</p>

<p>Nvidia maintains good documentation to ease adoption, like its <a href="programming guide" target="_blank">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html</a>.</p>

<h3 id="kernel-syntax-example">Kernel syntax example</h3>

<pre><code>// Add two matrices A and B of size NxN and stores the result into matrix C:
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i &lt; N &amp;&amp; j &lt; N)
        C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);
    ...
}
</code></pre>

<p>CUDA-specific additions:
* Kernels are defined with a <code>__global__</code> specifier (when called by the host).
* <code>&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;</code> gives the <em>execution configuration</em>.
* Ways for threads to query their location: <code>threadIdx</code>, <code>blockIdx</code>.</p>

<h3 id="thread-heirarchy">Thread Heirarchy</h3>

<p>Threads each have their own register allocation.  They are always executed in &ldquo;SIMT&rdquo; <strong>warps</strong> of up to 32 (could change, but hasn&rsquo;t yet).  This means:
* any divergence of instructions between threads within a warp causes some of the threads to no-op (relaxed recently);
* <code>product(threadsPerBlock)</code> should be a multiple of 32 (maximum 1024) where possible.</p>

<p>Blocks each have their own shared memory allocation.  All threads in a block are resident on the same processing core.  Thread layout can be up to three dimensions.
* can perform a lightweight synchronization within a block;
* co-resident blocks can be helpful at masking latency, but this is limited by block memory and register use.</p>

<p>Blocks themselves are layed out on a <strong>grid</strong> of up to three dimensions (on recent compute capabilities).  They must be logically executable in parallel or any serial order.
* no synchronization across blocks within a kernel;
* embarassingly parallel only, although caches can be reused.</p>

<p><img src="grid-of-thread-blocks.png" width="400"/>
<center><i>Nvidia.com: 2d grid and threads</i></center></p>

<h3 id="memory">Memory</h3>

<p><img src="hardware-model.png" width="400"/>
<center><i>Nvidia.com: model of memory connections</i></center></p>

<p><strong>Global</strong>, <strong>constant</strong>, and <strong>texture</strong> memories persist across kernel calls, and each has its own cache per SM (L2 cache shared by SMs).  By default, host and device are assumed to maintain separate memory:
* explicit device allocation and deallocation;
* explicit transfer between host and device.</p>

<p>Alternatively, there is a &ldquo;Unified Memory&rdquo; configuration that automates these on an as-needed basis, pretending there is one common address space.</p>

<p>Each block has <strong>shared</strong> memory which tends to be fast (equivalent to a user-managed L1 cache).</p>

<p>Each thread has &ldquo;<strong>local</strong>&rdquo; memory (that is actually no more local than global memory!), which is mostly used for register spilling.  (Register and shared memory usage are reported by the compiler when compiling with the <code>-ptxas-options=-v</code> option.)</p>

<h3 id="memory-transfer-example">Memory transfer example</h3>

<pre><code>__global__ void VecAdd(float* A, float* B, float* C, int N)
{
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i &lt; N)
        C[i] = A[i] + B[i];
}

int main()
{
    int N = ...;
    size_t size = N * sizeof(float);
    
    // Allocate input vectors h_A and h_B in host memory
    float* h_A = (float*)malloc(size);
    float* h_B = (float*)malloc(size);
    float* h_C = (float*)malloc(size);

    // Initialize input vectors
    ...

    
    // Allocate vectors in device memory
    float* d_A;
    cudaMalloc(&amp;d_A, size);
    float* d_B;
    cudaMalloc(&amp;d_B, size);
    float* d_C;
    cudaMalloc(&amp;d_C, size);

    // Copy vectors from host memory to device memory
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);


    // Invoke kernel
    int threadsPerBlock = 256;
    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
    VecAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_A, d_B, d_C, N);


    // Copy result from device memory to host memory
    // h_C contains the result in host memory
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
            
    // Free host memory
    ...
}
</code></pre>

<p><hline></p>

<pre><code class="language-python">
</code></pre>

<h2 id="optimization-details">Optimization Details</h2>

<p>Often details depend on the particular &ldquo;compute capability&rdquo; of the device.</p>

<h3 id="intrinsic-function-instructions">Intrinsic function instructions</h3>

<ul>
<li>similar tradeoffs to &ndash;ffast-math</li>
</ul>

<h3 id="memory-1">Memory</h3>

<h4 id="hiding-memory-transfers">Hiding Memory Transfers:</h4>

<p>Memory transfers between host and device generally have the greatest latency.  Modern capabilities can hide data transfer between host and device by giving the device other tasks to work on and having the host use asynchronous versions of the transfer functions.</p>

<p>This is managed through  <strong>streams</strong> on the host, where cuda calls within a stream are guaranteed to execute on the device in order, but those between streams may be out of order or overlap <em>depending on the compute capability</em>.</p>

<p>To minimize waiting with the following code, the compute capability needs to allow concurrent data transfers, concurrent kernel execution, and overlap of data transfer and kernel execution.</p>

<pre><code>for (int i = 0; i &lt; 2; ++i) {
    cudaMemcpyAsync(inputDevPtr + i * size, hostPtr + i * size,
                    size, cudaMemcpyHostToDevice, stream[i]);
    MyKernel &lt;&lt;&lt;100, 512, 0, stream[i]&gt;&gt;&gt;
          (outputDevPtr + i * size, inputDevPtr + i * size, size);
    cudaMemcpyAsync(hostPtr + i * size, outputDevPtr + i * size,
                    size, cudaMemcpyDeviceToHost, stream[i]);
}
</code></pre>

<h4 id="global-memory-access-size-and-alignment">Global memory access size and alignment:</h4>

<p>Example: an array of this struct would have elements that aren&rsquo;t aligned if not for the <code>__align__(16)</code>:</p>

<pre><code>struct __align__(16) {
    float x;
    float y;
    float z;
};
</code></pre>

<p>This usually crops up with 2d arrays, which are more efficient if width-padded to a multiple of the warp size.</p>

<h4 id="coalescence">Coalescence:</h4>

<p>Global (and local*) memory requests must be <strong>coalesced</strong>&mdash;falling into the same 128-byte wide+aligned window (for all modern capabilities)&mdash;or they will require multiple instructions.</p>

<p><em>*: the compiler will generally ensure that local memory use is coalesced</em></p>

<h4 id="bank-distribution">Bank Distribution:</h4>

<p>Similar, but different from global-memory coalescence.  Shared memory is divided into <strong>banks</strong> (typically 32), where each bank can be accessed simultaneously.</p>

<p><img src="six-examples-of-shared-memory-accesses.png" width="450"/>
<center><i>Nvidia.com: A) conflict-free, B) conflict depth 2, C) conflict-free, D) conflict-free, E) conflict-free, F) conflict-free</i></center></p>

<p><em>My impression is that most programmers rely on the compiler to sensibly structure bank accesses for temporary variables, but occasionally breaking into the &ldquo;CUDA assembly&rdquo; language <code>PTX</code> will yeild significant performance improvements.</em></p>

<p>The combination of coalescence and shared banks can cause an interesting interplay for certain problems.  Consider:
* Mx31 array made of structs of 2 32-bit floats.
* Coalescence would suggest padding the array to 32 wide when reading from global memory, but then once it resides in a shared memory with 32-bit strided banks, a warp of threads accessing the first of the pair of floats will cause bank conflicts of depth 2.
* Shared memory would be better served by padding the array width to 31.5.</p>

<p>(the better solution might be to pull the struct apart&hellip;)</p>

<h4 id="texture-specific-memory-features">Texture-specific memory features:</h4>

<ul>
<li>Optimized for 2d locality; can be faster than non-coalesced global/constant memory requests.</li>

<li><p>Ability to automatically cast <sup>8</sup>&frasl;<sub>16</sub>-bit integers into [0,1] 32-bit floats.</p>

<pre><code class="language-python">
</code></pre></li>
</ul>

          </div>

          



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/fall2019/cuda-practical/" rel="next">Practical CUDA</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/fall2019/coprocessor/" rel="prev">Coprocessor architectures</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">
          <p>Last updated on Oct 30, 2019</p>

          
<p class="edit-page">
  <a href="https://github.com/cucs-hpsc/hpsc-class/edit/master/content/fall2019/cuda/index.md">
    <i class="fas fa-pen pr-2"></i>Edit this page
  </a>
</p>



          

        </div>

      </article>

      <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>


      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/c.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.16bbb3750feb7244c9bc409a5a4fe678.js"></script>

    






  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
