<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Logistics | HPSC</title>
    <link>https://cucs-hpsc.github.io/fall2019/</link>
      <atom:link href="https://cucs-hpsc.github.io/fall2019/index.xml" rel="self" type="application/rss+xml" />
    <description>Logistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language>
    <image>
      <url>https://cucs-hpsc.github.io/img/icon-192.png</url>
      <title>Logistics</title>
      <link>https://cucs-hpsc.github.io/fall2019/</link>
    </image>
    
    <item>
      <title>Syllabus</title>
      <link>https://cucs-hpsc.github.io/fall2019/syllabus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/syllabus/</guid>
      <description>

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;This course will develop the skills necessary to reason about
performance of applications and modern architectures, to identify
opportunities and side-effects of changes, to develop high-performance
software, to transfer algorithmic patterns and lessons learned from
different domains, and to communicate such analyses with diverse
stakeholders.  These skills are important for research and development
of numerical methods and performance-sensitive science and engineering
applications, obtaining allocations via NSF&amp;rsquo;s
&lt;a href=&#34;https://www.xsede.org/&#34; target=&#34;_blank&#34;&gt;XSEDE&lt;/a&gt; and DOE &lt;a href=&#34;https://science.osti.gov/ascr/Facilities/Accessing-ASCR-Facilities&#34; target=&#34;_blank&#34;&gt;ASCR
facilities&lt;/a&gt;,
as well as in jobs affiliated with computing facilities at &lt;a href=&#34;https://www.alcf.anl.gov/about/careers&#34; target=&#34;_blank&#34;&gt;national
labs&lt;/a&gt;, industry, and academia.&lt;/p&gt;

&lt;p&gt;We will introduce widely-used parallel programming models such as
OpenMP, MPI, and CUDA, as well as ubiquitous parallel libraries, but
the purpose of the course is not to teach interfaces, but to develop
skills that will be durable and transferrable.&lt;/p&gt;

&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;

&lt;p&gt;This course does not assume prior experience with parallel
programming.  It will use Linux command-line tools, and some
activities will involve batch computing environments (SLURM).  Most
exercises will use the C programming language, though you can use any
appropriate language for projects.  Some of the exercises will involve
techniques from numerical computing (e.g., CSCI-3656).  I will do my
best to avoid assuming prior knowledge of these topics, and to provide
resources for you to learn or refresh your memory as we use them.&lt;/p&gt;

&lt;p&gt;Everyone here is capable of succeeding in the course, but the effort
level will be higher if most of the topics above are new to you.
Regardless of your preparation, it is normal to feel lost sometimes.
A big part of pragmatic HPC is learning to efficiently answer your
questions through documentation, online resources, and even consulting
the code or running experiments.  (Most of our software stack is open
source.)  That said, it&amp;rsquo;s easy to lose lots of time in a rabbit hole.
My hope is that you will have the courage to dive into that rabbit
hole occasionally, but also to &lt;a href=&#34;https://jvns.ca/blog/good-questions/&#34; target=&#34;_blank&#34;&gt;ask questions&lt;/a&gt; when stuck and to budget
your time for such excursions so that you can complete assignments
on-time without compromising your work/life balance.&lt;/p&gt;

&lt;h2 id=&#34;approximate-timeline&#34;&gt;Approximate timeline&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Week&lt;/th&gt;
&lt;th&gt;Topics&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Aug 26&lt;/td&gt;
&lt;td&gt;Introduction and modern computer architecture (vectorization and memory hierarchy)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sep 2&lt;/td&gt;
&lt;td&gt;Performance modeling, analysis, and scaling; profiling&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sep 9&lt;/td&gt;
&lt;td&gt;Intro to OpenMP and non-numerical algorithms (sorting and searching)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sep 16&lt;/td&gt;
&lt;td&gt;Parallel algorithmic patterns&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sep 23&lt;/td&gt;
&lt;td&gt;Dense linear algebra&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sep 30&lt;/td&gt;
&lt;td&gt;Intro to MPI and distributed memory parallelism&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Oct 7&lt;/td&gt;
&lt;td&gt;Sparse and iterative linear algebra&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Oct 14&lt;/td&gt;
&lt;td&gt;Domain decomposition&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Oct 21&lt;/td&gt;
&lt;td&gt;Graph algorithms&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Oct 28&lt;/td&gt;
&lt;td&gt;GPU programming via OpenMP-5 and CUDA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Nov 4&lt;/td&gt;
&lt;td&gt;Parallel file systems and IO&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Nov 11&lt;/td&gt;
&lt;td&gt;Data analysis/machine learning algorithms and dynamic cloud environments&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Nov 18&lt;/td&gt;
&lt;td&gt;Particles and N-body systems&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Nov 25&lt;/td&gt;
&lt;td&gt;Fall Break&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Dec 2&lt;/td&gt;
&lt;td&gt;Multigrid, FFT, and FMM&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Dec 9&lt;/td&gt;
&lt;td&gt;Special topics&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Activity&lt;/th&gt;
&lt;th&gt;Percentage&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Participation&lt;/td&gt;
&lt;td&gt;10%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Labs and homework assignments&lt;/td&gt;
&lt;td&gt;40%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Community contribution&lt;/td&gt;
&lt;td&gt;15%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Community analysis&lt;/td&gt;
&lt;td&gt;15%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Final project (written + presentation)&lt;/td&gt;
&lt;td&gt;20%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;git-and-github&#34;&gt;Git and GitHub&lt;/h3&gt;

&lt;p&gt;Homework assignments and in-class activities will be submitted via Git.  This class will use GitHub classroom.
Homeworks will be completed by cloning GitHub repositories, completing coding and analysis activities, and pushing completed assignments back to GitHub.&lt;/p&gt;

&lt;p&gt;Assignments may be completed using &lt;a href=&#34;https://coding.csel.io/&#34; target=&#34;_blank&#34;&gt;Coding CSEL Hub&lt;/a&gt; and/or &lt;a href=&#34;https://www.colorado.edu/rc/resources/summit/specifications&#34; target=&#34;_blank&#34;&gt;RMACC Summit&lt;/a&gt; (&lt;a href=&#34;https://rcamp.rc.colorado.edu/accounts/account-request/create/organization&#34; target=&#34;_blank&#34;&gt;request an account&lt;/a&gt;).
Assignments will typically have written analysis, for which I recommend &lt;a href=&#34;https://jupyter.org/&#34; target=&#34;_blank&#34;&gt;Jupyter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It is notoriously difficult to predict the time required to develop
quality code and understand its performance, so please start early to
give yourself plenty of time.  You are welcome to work together on all
assignments, but must acknowledge collaborators.  You should ensure
that your written work is entirely your own.&lt;/p&gt;

&lt;h3 id=&#34;community-contributions-and-analysis&#34;&gt;Community contributions and analysis&lt;/h3&gt;

&lt;p&gt;Over the course of the semester, you will follow the development
activities of an active open source project of your choosing.  This
should be a project with an active developer community from multiple
institutions that discuss their rationale in public, such as a mailing
list and/or GitHub/GitLab issues and pull requests.  You will write
and present about the performance and capability needs of key
stakeholders, the way project resources are allocated, their metrics
for success, and any notable achievements made over the course of the
semester.&lt;/p&gt;

&lt;p&gt;You will also make a contribution to be merged by the project.  Adding
new examples and/or improving documentation are extremely valuable
contributions, but you may also add features or improve
implementations.  Please respect the time of project maintainers and
reviewers by learning about the project and its expectations and
process, communicating in advance if appropriate, and leaving plenty
of time for multiple rounds of review and revision.&lt;/p&gt;

&lt;h3 id=&#34;distance-sections-and-labs&#34;&gt;Distance sections and labs&lt;/h3&gt;

&lt;p&gt;The lectures for this class can be joined synchronously via Zoom (see
&lt;a href=&#34;fall2019/&#34; target=&#34;_blank&#34;&gt;instructions&lt;/a&gt;); they are also recorded and will be posted
&lt;a href=&#34;fall2019/#videos&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; (and automatically on Canvas).  Some labs
will be activities that can be completed within the time period (with
group discussion and compare/contrast) while others will be a jump
start for homeworks.  I envision that distance students will form
groups and set a time for virtual discussion in lieu of synchronous
discussion during the lab period.  In both settings, there will be a
peer evaluation component during which each participant credits one or
more peers with some specific contributions to the conversation.&lt;/p&gt;

&lt;h2 id=&#34;moodle&#34;&gt;Moodle&lt;/h2&gt;

&lt;p&gt;Moodle will be used to maintain grades.  Please enroll yourself at &lt;a href=&#34;https://moodle.cs.colorado.edu&#34; target=&#34;_blank&#34;&gt;https://moodle.cs.colorado.edu&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;

&lt;p&gt;This course will use a variety of online resources and papers.
There is no required textbook, but the following resources may be helpful.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blogs.fau.de/hager/hpc-book&#34; target=&#34;_blank&#34;&gt;Hager and Wellein (2010), &lt;strong&gt;Introduction to High Performance Computing for Scientists and Engineers&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.utexas.edu/users/flame/laff/pfhp/index.html&#34; target=&#34;_blank&#34;&gt;van de Geijn, Myers, Parikh (2019): &lt;strong&gt;LAFF on Programming for High Performance&lt;/strong&gt;&lt;/a&gt; (free online)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://pages.tacc.utexas.edu/~eijkhout/istc/istc.html&#34; target=&#34;_blank&#34;&gt;Eijkhout, Chow, van de Geijn (2017), &lt;strong&gt;Introduction to High-Performance Scientific Computing&lt;/strong&gt;&lt;/a&gt; (free PDF)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www-users.cs.umn.edu/~karypis/parbook/&#34; target=&#34;_blank&#34;&gt;Grama, Gupta, Karypis, Kumar (2003), &lt;strong&gt;Introduction to Parallel Computing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;additional-resources&#34;&gt;Additional resources&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://press.princeton.edu/titles/9763.html&#34; target=&#34;_blank&#34;&gt;Greenbaum and Chartier (2012), &lt;strong&gt;Numerical Methods Design, Analysis, and Computer Implementation of Algorithms&lt;/strong&gt;&lt;/a&gt; &amp;ndash; an excellent, comprehensive book.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/~boyd/vmls/&#34; target=&#34;_blank&#34;&gt;Boyd and Vandenberghe (2018), &lt;strong&gt;Introduction to Applied Linear Algebra&lt;/strong&gt;&lt;/a&gt; &amp;ndash; practical introduction to linear algebra for computer scientists; free PDF&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://bookstore.siam.org/ot50/&#34; target=&#34;_blank&#34;&gt;Trefethen and Bau (1997), &lt;strong&gt;Numerical Linear Algebra&lt;/strong&gt;&lt;/a&gt; &amp;ndash; fantastic, but limited to numerical linear algebra and covers more advanced topics.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://physics.codes/&#34; target=&#34;_blank&#34;&gt;Scopatz and Huff (2015), &lt;strong&gt;Effective Computation in Physics&lt;/strong&gt;&lt;/a&gt; &amp;ndash; Python language, data science workflow, and computation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A &lt;a href=&#34;http://www.siam.org/students/memberships.php&#34; target=&#34;_blank&#34;&gt;SIAM Membership&lt;/a&gt; is free for CU students and provides a 30% discount on SIAM books.&lt;/p&gt;

&lt;h2 id=&#34;disability-accommodations&#34;&gt;Disability Accommodations&lt;/h2&gt;

&lt;p&gt;If you qualify for accommodations because of a disability, please submit to your professor a letter from Disability Services in a timely manner (for exam accommodations provide your letter at least one week prior to the exam) so that your needs can be addressed. Disability Services determines accommodations based on documented disabilities. Contact Disability Services at 303-492-8671 or by e-mail at dsinfo@colorado.edu. If you have a temporary medical condition or injury, see the Temporary Injuries guidelines under the Quick Links at the Disability Services website and discuss your needs with your professor.&lt;/p&gt;

&lt;h2 id=&#34;religious-observances&#34;&gt;Religious Observances&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.colorado.edu/policies/fac_relig.html&#34; target=&#34;_blank&#34;&gt;Campus policy regarding religious observances&lt;/a&gt; requires that faculty make every effort to deal reasonably and fairly with all students who, because of religious obligations, have conflicts with scheduled exams, assignments or required assignments/attendance. If this applies to you, please speak with me directly as soon as possible at the beginning of the term. See the &lt;a href=&#34;http://www.colorado.edu/policies/observance-religious-holidays-and-absences-classes-andor-exams&#34; target=&#34;_blank&#34;&gt;campus policy regarding religious observances&lt;/a&gt; for full details.&lt;/p&gt;

&lt;h2 id=&#34;classroom-behavior&#34;&gt;Classroom Behavior&lt;/h2&gt;

&lt;p&gt;Students and faculty each have responsibility for maintaining an appropriate learning environment. Those who fail to adhere to such behavioral standards may be subject to discipline. Professional courtesy and sensitivity are especially important with respect to individuals and topics dealing with differences of race, color, culture, religion, creed, politics, veteran&amp;rsquo;s status, sexual orientation, gender, gender identity and gender expression, age, disability,and nationalities. Class rosters are provided to the instructor with the student&amp;rsquo;s legal name. I will gladly honor your request to address you by an alternate name or gender pronoun. Please advise me of this preference early in the semester so that I may make appropriate changes to my records. For more information, see the policies on &lt;a href=&#34;http://www.colorado.edu/policies/student-classroom-and-course-related-behavior&#34; target=&#34;_blank&#34;&gt;classroom behavior&lt;/a&gt; and the &lt;a href=&#34;http://www.colorado.edu/osc/sites/default/files/attached-files/studentconductcode_16-17-a.pdf&#34; target=&#34;_blank&#34;&gt;student code&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;discrimination-and-harassment&#34;&gt;Discrimination and Harassment&lt;/h2&gt;

&lt;p&gt;The University of Colorado Boulder (CU Boulder) is committed to maintaining a positive learning, working, and living environment. CU Boulder will not tolerate acts of sexual misconduct, discrimination, harassment or related retaliation against or by any employee or student. CU&amp;rsquo;s Sexual Misconduct Policy prohibits sexual assault, sexual exploitation, sexual harassment,intimate partner abuse (dating or domestic violence), stalking or related retaliation. CU Boulder&amp;rsquo;s Discrimination and Harassment Policy prohibits discrimination, harassment or related retaliation based on race, color,national origin, sex, pregnancy, age, disability, creed, religion, sexual orientation, gender identity, gender expression, veteran status, political affiliation or political philosophy. Individuals who believe they have been subject to misconduct under either policy should contact the Office of Institutional Equity and Compliance (OIEC) at 303-492-2127. Information about the OIEC, the above referenced policies, and the campus resources available to assist individuals regarding sexual misconduct, discrimination, harassment or related retaliation can be found at the &lt;a href=&#34;http://www.colorado.edu/institutionalequity/&#34; target=&#34;_blank&#34;&gt;OIEC website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;honor-code&#34;&gt;Honor Code&lt;/h2&gt;

&lt;p&gt;All students enrolled in a University of Colorado Boulder course are responsible for knowing and adhering to the &lt;a href=&#34;http://www.colorado.edu/policies/academic-integrity-policy&#34; target=&#34;_blank&#34;&gt;academic integrity policy&lt;/a&gt; of the institution. Violations of the policy may include: plagiarism, cheating,fabrication, lying, bribery, threat, unauthorized access, clicker fraud,resubmission, and aiding academic dishonesty.  All incidents of academic misconduct will be reported to the Honor Code Council (honor@colorado.edu; 303-735-2273). Students who are found responsible for violating the academic integrity policy will be subject to nonacademic sanctions from the Honor Code Council as well as academic sanctions from the faculty member. Additional information regarding the academic integrity policy can be found at &lt;a href=&#34;http://honorcode.colorado.edu&#34; target=&#34;_blank&#34;&gt;http://honorcode.colorado.edu&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dense Linear Algebra and Orthogonality</title>
      <link>https://cucs-hpsc.github.io/fall2019/dense-linalg-2/</link>
      <pubDate>Wed, 02 Oct 2019 06:49:25 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/dense-linalg-2/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#pragma omp parallel
    {
      for (size_t rep=0; rep&amp;lt;args.repetitions; rep++) {
#pragma omp for
        for (size_t i=0; i&amp;lt;args.array_len; i++)
          y[i] += 3.14 * x[i];
      }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;    for (size_t rep=0; rep&amp;lt;args.repetitions; rep++) {
#pragma omp parallel for
      for (size_t i=0; i&amp;lt;args.array_len; i++)
        y[i] += 3.14 * x[i];
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;! make CFLAGS=&#39;-O3 -march=native -fopenmp -Wall&#39; -B omp-test
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cc -O3 -march=native -fopenmp -Wall    omp-test.c   -o omp-test
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;! ./omp-test -r 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;omp for         : 0.770512 ticks per entry
omp parallel for: 1.576261 ticks per entry
omp for         : 0.419312 ticks per entry
omp parallel for: 1.475976 ticks per entry
omp for         : 0.426896 ticks per entry
omp parallel for: 1.021856 ticks per entry
omp for         : 0.494572 ticks per entry
omp parallel for: 1.270378 ticks per entry
omp for         : 0.444213 ticks per entry
omp parallel for: 1.009316 ticks per entry
omp for         : 0.579121 ticks per entry
omp parallel for: 1.024148 ticks per entry
omp for         : 0.531494 ticks per entry
omp parallel for: 1.174585 ticks per entry
omp for         : 0.442223 ticks per entry
omp parallel for: 1.147614 ticks per entry
omp for         : 0.446249 ticks per entry
omp parallel for: 1.084162 ticks per entry
omp for         : 0.576802 ticks per entry
omp parallel for: 1.325817 ticks per entry
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;matrix-matrix-multiply&#34;&gt;Matrix-matrix multiply&lt;/h2&gt;

&lt;h3 id=&#34;start-local&#34;&gt;Start local&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;blis-gemm-kernels.png&#34; alt=&#34;BLIS GEMM kernels&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;blis-cache.png&#34; alt=&#34;BLIS cache levels&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;further-reading&#34;&gt;Further reading&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.utexas.edu/users/flame/pubs/blis2_toms_rev3.pdf&#34; target=&#34;_blank&#34;&gt;http://www.cs.utexas.edu/users/flame/pubs/blis2_toms_rev3.pdf&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cs.utexas.edu/users/flame/pubs/blis3_ipdps14.pdf&#34; target=&#34;_blank&#34;&gt;http://www.cs.utexas.edu/users/flame/pubs/blis3_ipdps14.pdf&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
plt.style.use(&#39;seaborn&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;orthogonalization-and-qr-factorization&#34;&gt;Orthogonalization and QR factorization&lt;/h2&gt;

&lt;p&gt;Given a collection of vectors (columns of a matrix), we can find an orthogonal basis by applying the above procedure one column at a time and saving the result.  Let&amp;rsquo;s think of the first two columns,
$$ \Bigg[ a_0 \, \Bigg| \, a_1 \Bigg] = \Bigg[ q_0 \,\Bigg|\, q&lt;em&gt;1 \Bigg]
\begin{bmatrix} r&lt;/em&gt;{00} &amp;amp; r&lt;em&gt;{01} \ 0 &amp;amp; r&lt;/em&gt;{11} \end{bmatrix} . $$&lt;/p&gt;

&lt;h4 id=&#34;column-0&#34;&gt;Column 0&lt;/h4&gt;

&lt;p&gt;The equation for column 0 reads
$$ a_0 = q&lt;em&gt;0 r&lt;/em&gt;{00} $$
and we require that $\lVert q&lt;em&gt;0 \rVert = 1$, thus
$$ r&lt;/em&gt;{00} = \lVert a_0 \rVert $$
and
$$ q_0 = a&lt;em&gt;0 / r&lt;/em&gt;{00} . $$&lt;/p&gt;

&lt;h4 id=&#34;column-1&#34;&gt;Column 1&lt;/h4&gt;

&lt;p&gt;This equation reads
$$ a_1 = q&lt;em&gt;0 r&lt;/em&gt;{01} + q&lt;em&gt;1 r&lt;/em&gt;{11} $$
where $a_1$ and $q_0$ are known and we will require that $q_0^T q_1 = 0$.
We can find the part of $a_1$ that is orthogonal to $q_0$ via
$$ (I - q_0 q_0^T) a_1 = a_1 - q_0 \underbrace{q_0^T a&lt;em&gt;1}&lt;/em&gt;{r_{01}} $$
leaving a sub-problem equivalent to that of column 0.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gram_schmidt_naive(A):
    &amp;quot;&amp;quot;&amp;quot;Compute a QR factorization of A using the Gram-Schmidt algorithm&amp;quot;&amp;quot;&amp;quot;
    Q = np.zeros_like(A)
    R = np.zeros((A.shape[1], A.shape[1]))
    for i in range(len(Q.T)):
        v = A[:,i].copy()
        for j in range(i):
            r = Q[:,j] @ v
            R[j,i] = r
            v -= Q[:,j] * r # &amp;quot;modified Gram-Schmidt&amp;quot;
        R[i,i] = np.linalg.norm(v)
        Q[:,i] = v / R[i,i]
    return Q, R

x = np.linspace(-1, 1)
A = np.vander(x, 4, increasing=True)
Q, R = gram_schmidt_naive(A)
print(Q.T @ Q)
print(np.linalg.norm(Q @ R - A))
plt.plot(x, Q);
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[[ 1.00000000e+00  2.06727448e-17 -7.22457952e-17 -2.05232865e-16]
 [ 2.06727448e-17  1.00000000e+00  1.13635722e-16 -5.08904737e-16]
 [-7.22457952e-17  1.13635722e-16  1.00000000e+00  4.66276733e-17]
 [-2.05232865e-16 -5.08904737e-16  4.66276733e-17  1.00000000e+00]]
4.744563050812836e-16
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_9_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;theorem-all-full-rank-m-times-n-matrices-m-ge-n-have-a-unique-q-r-factorization-with-r-j-j-0&#34;&gt;Theorem: all full-rank $m\times n$ matrices ($m \ge n$) have a unique $Q R$ factorization with $R_{j,j} &amp;gt; 0$.&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m = 20
V = np.vander(np.linspace(-1,1,m), increasing=True)
Q, R = gram_schmidt_naive(V)

def qr_test(qr, V):
    Q, R = qr(V)
    m = len(Q.T)
    print(&#39;{:20} {:.2e} {:.2e}&#39;.format(
        qr.__name__,
        np.linalg.norm(Q @ R - V),
        np.linalg.norm(Q.T @ Q - np.eye(m))))
    
qr_test(gram_schmidt_naive, V)
qr_test(np.linalg.qr, V)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;gram_schmidt_naive   9.52e-16 3.04e-09
qr                   2.74e-15 2.39e-15
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;left-looking-algorithms-reducing-the-number-of-inner-products&#34;&gt;Left-looking algorithms: reducing the number of inner products&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gram_schmidt_classical(A):
    Q = np.zeros_like(A)
    R = np.zeros((len(A.T),len(A.T)))
    for i in range(len(Q.T)):
        v = A[:,i].copy()
        R[:i,i] = Q[:,:i].T @ v
        v -= Q[:,:i] @ R[:i,i]
        R[i,i] = np.linalg.norm(v)
        Q[:,i] = v / R[i,i]
    return Q, R

qr_test(gram_schmidt_classical, V)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;gram_schmidt_classical 9.14e-16 1.42e+00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Classical Gram-Schmidt is highly parallel, but unstable, as evidenced by the lack of orthogonality in $Q$.&lt;/p&gt;

&lt;h3 id=&#34;right-looking-algorithms&#34;&gt;Right-looking algorithms&lt;/h3&gt;

&lt;p&gt;The implementations above have been &amp;ldquo;left-looking&amp;rdquo;; when working on column $i$, we compare it only to columns to the left (i.e., $j &amp;lt; i$).  We can reorder the algorithm to look to the right by projecting $q_i$ out of all columns $j &amp;gt; i$.  This algorithm is stable while being just as parallel as &lt;code&gt;gram_schmidt_classical&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gram_schmidt_modified(A):
    Q = A.copy()
    R = np.zeros((len(A.T), len(A.T)))
    for i in range(len(Q.T)):
        R[i,i] = np.linalg.norm(Q[:,i])
        Q[:,i] /= R[i,i]
        R[i,i+1:] = Q[:,i].T @ Q[:,i+1:]
        Q[:,i+1:] -= np.outer(Q[:,i], R[i,i+1:])
    return Q, R

qr_test(gram_schmidt_modified, V)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;gram_schmidt_modified 8.32e-16 1.32e-08
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;one-reduction-cholesky-qr&#34;&gt;One reduction: Cholesky QR&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def chol_qr(A):
    import scipy.linalg as la
    B = A.T @ A
    R = la.cholesky(B)
    Q = A @ la.inv(R)
    return Q, R
    
qr_test(chol_qr, V)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;chol_qr              8.12e-15 1.07e-01
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def chol_qr2(A):
    import scipy.linalg as la
    B = A.T @ A
    R = la.cholesky(B)
    Q = A @ la.inv(R)
    R2 = la.cholesky(Q.T @ Q)
    Q = Q @ la.inv(R2)
    R = R2 @ R
    return Q, R

qr_test(chol_qr2, V)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;chol_qr2             8.36e-15 1.29e-15
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Dense Linear Algebra and Networks</title>
      <link>https://cucs-hpsc.github.io/fall2019/dense-linalg/</link>
      <pubDate>Mon, 30 Sep 2019 06:49:25 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/dense-linalg/</guid>
      <description>

&lt;h2 id=&#34;inner-products&#34;&gt;Inner products&lt;/h2&gt;

&lt;p&gt;$$ x^T y = \sum_{i=1}^N x_i y_i $$&lt;/p&gt;

&lt;h4 id=&#34;openmp&#34;&gt;OpenMP&lt;/h4&gt;

&lt;p&gt;The vectors &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; of length &lt;code&gt;N&lt;/code&gt; are stored in a contiguous array in shared memory.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;double sum = 0;
#pragma omp parallel for reduction(+:sum)
for (int i=0; i&amp;lt;N; i++)
    sum += x[i] * y[i];
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;mpi&#34;&gt;MPI&lt;/h4&gt;

&lt;p&gt;The vectors &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are partitioned into $P$ parts of length $n&lt;em&gt;p$ such that
$$ N = \sum&lt;/em&gt;{p=1}^P n_p . $$
The inner product is computed via&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;double sum = 0;
for (int i=0; i&amp;lt;n; i++)
    sum += x[i] * y[i];
MPI_Allreduce(MPI_IN_PLACE, &amp;amp;sum, 1, MPI_DOUBLE, MPI_SUM, comm);
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Work: $2N$ flops processed rate $R$&lt;/li&gt;
&lt;li&gt;Execution time: $\frac{2N}{RP} + \text{latency}$&lt;/li&gt;

&lt;li&gt;&lt;p&gt;How big is latency?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
import matplotlib.pyplot as plt
plt.style.use(&#39;seaborn&#39;)
import numpy as np

P = np.geomspace(2, 1e6)
N = 1e9       # length of vectors
R = 10e9/8    # (10 GB/s per core) (2 flops/16 bytes) = 10/8 GF/s per core
t1 = 2e-6     # 2 Âµs message latency

def time_compute(P):
return 2*N / (R*P)

plt.loglog(P, time_compute(P) + t1*(P-1), label=&#39;linear&#39;)
plt.loglog(P, time_compute(P) + t1*2*(np.sqrt(P)-1), label=&#39;2D mesh&#39;)
plt.loglog(P, time_compute(P) + t1*np.log2(P), label=&#39;hypercube&#39;)
plt.xlabel(&#39;P processors&#39;)
plt.ylabel(&#39;Execution time&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_1_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;FischerBGQAllReduce.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;torus-topology&#34;&gt;Torus topology&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/60/Torus_from_rectangle.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/1/1f/3d_torus.png&#34; width=&#34;800px&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3D torus: IBM BlueGene/L (2004) and BlueGene/P (2007)&lt;/li&gt;
&lt;li&gt;5D torus: IBM BlueGene/Q (2011)&lt;/li&gt;
&lt;li&gt;6D torus: Fujitsu K computer (2011)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dragonfly-topology&#34;&gt;Dragonfly topology&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;CrayAriesDragonfly.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;today-s-research-reducing-contention-and-interference&#34;&gt;Today&amp;rsquo;s research: reducing contention and interference&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://3s81si1s5ygj3mzby34dq6qf-wpengine.netdna-ssl.com/wp-content/uploads/2019/08/cray-slingshot-congestion-control.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://3s81si1s5ygj3mzby34dq6qf-wpengine.netdna-ssl.com/wp-content/uploads/2019/08/cray-slingshot-trace-latency.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Images from &lt;a href=&#34;https://www.nextplatform.com/2019/08/16/how-cray-makes-ethernet-suited-for-hpc-and-ai-with-slingshot/&#34; target=&#34;_blank&#34;&gt;this article&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;compare-to-bg-q&#34;&gt;Compare to BG/Q&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Each job gets an electrically isolated 5D torus&lt;/li&gt;
&lt;li&gt;Excellent performance and reproducibility&lt;/li&gt;
&lt;li&gt;Awkward constraints on job size, lower system utilization.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;outer-product&#34;&gt;Outer product&lt;/h2&gt;

&lt;p&gt;$$ C_{ij} = x_i y_j $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data in: $2N$&lt;/li&gt;
&lt;li&gt;Data out: $N^2$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;matrix-vector-products&#34;&gt;Matrix-vector products&lt;/h2&gt;

&lt;p&gt;$$ y&lt;em&gt;i = \sum&lt;/em&gt;{j} A_{ij} x_j $$&lt;/p&gt;

&lt;p&gt;How to partition the matrix $A$ across $P$ processors?&lt;/p&gt;

&lt;h3 id=&#34;1d-row-partition&#34;&gt;1D row partition&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Every process needs entire vector $x$: &lt;code&gt;MPI_Allgather&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Matrix data does not move&lt;/li&gt;
&lt;li&gt;Execution time
$$ \underbrace{\frac{2N^2}{RP}}_{\text{compute}} + \underbrace{t_1 \log&lt;em&gt;2 P}&lt;/em&gt;{\text{latency}} + \underbrace{t&lt;em&gt;b N \frac{P-1}{P}}&lt;/em&gt;{\text{bandwidth}} $$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;05-matvec-row.png&#34; alt=&#34;Thanks to Abtin Rahimian&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;2d-partition&#34;&gt;2D partition&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Blocks of size $N/\sqrt{P}$&lt;/li&gt;
&lt;li&gt;&amp;ldquo;diagonal&amp;rdquo; ranks hold the input vector&lt;/li&gt;
&lt;li&gt;Broadcast $x$ along columns: &lt;code&gt;MPI_Bcast&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Perform local compute&lt;/li&gt;
&lt;li&gt;Sum &lt;code&gt;y&lt;/code&gt; along rows: &lt;code&gt;MPI_Reduce&lt;/code&gt; with roots on diagonal&lt;/li&gt;
&lt;li&gt;Execution time
$$ \underbrace{\frac{2N^2}{RP}}_{\text{compute}} + \underbrace{2 t_1 \log&lt;em&gt;2 P}&lt;/em&gt;{\text{latency}} + \underbrace{\frac{2 t&lt;em&gt;b N}{\sqrt{P}}}&lt;/em&gt;{\text{bandwidth}} $$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;05-matvec-block.png&#34; alt=&#34;Thanks to Abtin Rahimian&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;N = 1e4
tb = 8 / 1e9 # 8 bytes / (1 GB/s) ~ bandwidth per core in units of double
tb *= 100

plt.loglog(P, (2*N**2)/(R*P) + t1*np.log2(P) + tb*N*(P-1)/P, label=&#39;1D distribution&#39;)
plt.loglog(P, (2*N**2)/(R*P) + 2*t1*np.log2(P) + 2*tb*N/np.sqrt(P), label=&#39;2D distribution&#39;)
plt.xlabel(&#39;P processors&#39;)
plt.ylabel(&#39;Execution time&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_8_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to MPI</title>
      <link>https://cucs-hpsc.github.io/fall2019/intro-mpi/</link>
      <pubDate>Tue, 24 Sep 2019 06:49:25 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/intro-mpi/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def render_c(filename):
    from IPython.display import Markdown
    with open(filename) as f:
        contents = f.read()
    return Markdown(&amp;quot;```c\n&amp;quot; + contents + &amp;quot;```\n&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;processes-and-threads&#34;&gt;Processes and Threads&lt;/h2&gt;

&lt;p&gt;Threads and processes are very similar
* Both created via &lt;a href=&#34;https://linux.die.net/man/2/clone&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;clone&lt;/code&gt; system call&lt;/a&gt; on Linux
* Scheduled in the same way by the operating system
* Separate stacks (automatic variables)
* Access to same memory before &lt;code&gt;fork()&lt;/code&gt; or &lt;code&gt;clone()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;with some important distinctions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Threads set &lt;code&gt;CLONE_VM&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;threads share the same virtual-to-physical address mapping&lt;/li&gt;
&lt;li&gt;threads can access the same data at the same addresses; private data is private only because other threads don&amp;rsquo;t know its address&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Threads set &lt;code&gt;CLONE_FILES&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;threads share file descriptors&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Threads set &lt;code&gt;CLONE_THREAD&lt;/code&gt;, &lt;code&gt;CLONE_SIGHAND&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;process id and signal handlers shared&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;myths&#34;&gt;Myths&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Processes can&amp;rsquo;t share memory

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mmap()&lt;/code&gt;, &lt;code&gt;shm_open()&lt;/code&gt;, and &lt;code&gt;MPI_Win_allocate_shared()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Processes are &amp;ldquo;heavy&amp;rdquo;

&lt;ul&gt;
&lt;li&gt;same data structures and kernel scheduling; no difference in context switching&lt;/li&gt;
&lt;li&gt;data from parent is inherited copy-on-write at very low overhead&lt;/li&gt;
&lt;li&gt;startup costs ~100 microseconds to duplicate page tables&lt;/li&gt;
&lt;li&gt;caches are physically tagged; processes can share L1 cache&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;mpi-message-passing-interface&#34;&gt;MPI: Message Passing Interface&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Just a library: plain C, C++, or Fortran compiler

&lt;ul&gt;
&lt;li&gt;Two active open source libraries: &lt;a href=&#34;https://www.mpich.org/&#34; target=&#34;_blank&#34;&gt;MPICH&lt;/a&gt; and &lt;a href=&#34;https://www.open-mpi.org/&#34; target=&#34;_blank&#34;&gt;Open MPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Numerous vendor implementations modify/extend these open source implementations&lt;/li&gt;
&lt;li&gt;MVAPICH is an MPICH-derived open source implementation for InfiniBand and related networks&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Bindings from many other languages; &lt;a href=&#34;https://mpi4py.readthedocs.io/en/stable/&#34; target=&#34;_blank&#34;&gt;mpi4py&lt;/a&gt; is popular&lt;/li&gt;
&lt;li&gt;Scales to millions of processes across ~100k nodes

&lt;ul&gt;
&lt;li&gt;Shared memory systems can be scaled up to &lt;a href=&#34;https://www.uvhpc.com/sgi-uv-3000&#34; target=&#34;_blank&#34;&gt;~4000 cores&lt;/a&gt;, but latency and price ($) increase&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Standard usage: processes are separate on startup&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Timeline&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MPI-1 (1994) point-to-point messaging, collectives&lt;/li&gt;
&lt;li&gt;MPI-2 (1997) parallel IO, dynamic processes, one-sided&lt;/li&gt;

&lt;li&gt;&lt;p&gt;MPI-3 (2012) nonblocking collectives, neighborhood collectives, improved one-sided&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;mpi-demo.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;mpi.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;

int main(int argc, char **argv) {
MPI_Init(&amp;amp;argc, &amp;amp;argv);   // Must call before any other MPI functions
int size, rank, sum;
MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank);
MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size);
MPI_Allreduce(&amp;amp;rank, &amp;amp;sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);
printf(&amp;quot;I am rank %d of %d: sum=%d\n&amp;quot;, rank, size, sum);
MPI_Finalize();
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This may remind you of the top-level OpenMP strategy&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int main() {
    #pragma omp parallel
    {
        int rank = omp_get_thread_num();
        int size = omp_get_num_threads();
        // your code
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We use the compiler wrapper &lt;code&gt;mpicc&lt;/code&gt;, but it just passes some flags to the real compiler.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;! mpicc -show
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;gcc -pthread -Wl,-rpath -Wl,/usr/lib/openmpi -Wl,&amp;ndash;enable-new-dtags -L/usr/lib/openmpi -lmpi&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;! make CC=mpicc CFLAGS=-Wall mpi-demo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mpicc -Wall    mpi-demo.c   -o mpi-demo&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We use &lt;code&gt;mpiexec&lt;/code&gt; to run locally.  Clusters/supercomputers often have different job launching programs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;! mpiexec -n 2 ./mpi-demo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am rank 0 of 2: sum=1
I am rank 1 of 2: sum=1&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We can run more MPI processes than cores (or hardware threads), but you might need to use the &lt;code&gt;--oversubscribe&lt;/code&gt; option because &lt;strong&gt;oversubscription is usually expensive&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;! mpiexec -n 6 --oversubscribe ./mpi-demo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am rank 1 of 6: sum=15
I am rank 3 of 6: sum=15
I am rank 4 of 6: sum=15
I am rank 5 of 6: sum=15
I am rank 0 of 6: sum=15
I am rank 2 of 6: sum=15&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You can use OpenMP within ranks of MPI (but use &lt;code&gt;MPI_Init_thread()&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Everything is private by default&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;advice-from-bill-gropp&#34;&gt;Advice from Bill Gropp&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;You want to think about how you decompose your data structures, how
    you think about them globally.  [&amp;hellip;]  If you were building a house,
    you&amp;rsquo;d start with a set of blueprints that give you a picture of what
    the whole house looks like.  You wouldn&amp;rsquo;t start with a bunch of
    tiles and say. &amp;ldquo;Well I&amp;rsquo;ll put this tile down on the ground, and
    then I&amp;rsquo;ll find a tile to go next to it.&amp;rdquo;  But all too many people
    try to build their parallel programs by creating the smallest
    possible tiles and then trying to have the structure of their code
    emerge from the chaos of all these little pieces.  You have to have
    an organizing principle if you&amp;rsquo;re going to survive making your code
    parallel.
    &amp;ndash; &lt;a href=&#34;https://www.rce-cast.com/Podcast/rce-28-mpich2.html&#34; target=&#34;_blank&#34;&gt;https://www.rce-cast.com/Podcast/rce-28-mpich2.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;communicators&#34;&gt;Communicators&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;MPI_COMM_WORLD&lt;/code&gt; contains all ranks in the &lt;code&gt;mpiexec&lt;/code&gt;.  Those ranks may be on different nodes, even in different parts of the world.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MPI_COMM_SELF&lt;/code&gt; contains only one rank&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Can create new communicators from existing ones&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int MPI_Comm_dup(MPI_Comm comm, MPI_Comm *newcomm);
int MPI_Comm_split(MPI_Comm comm, int color, int key, MPI_Comm *newcomm);
int MPI_Comm_create(MPI_Comm comm, MPI_Group group, MPI_Comm *newcomm);
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Can spawn new processes (but not supported on all machines)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int MPI_Comm_spawn(const char *command, char *argv[], int maxprocs,
        MPI_Info info, int root, MPI_Comm comm,
        MPI_Comm *intercomm, int array_of_errcodes[]);
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Can attach &lt;em&gt;attributes&lt;/em&gt; to communicators (useful for library composition)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;collective-operations&#34;&gt;Collective operations&lt;/h3&gt;

&lt;p&gt;MPI has a rich set of collective operations scoped by communicator, including the following.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int MPI_Allreduce(const void *sendbuf, void *recvbuf, int count,
        MPI_Datatype datatype, MPI_Op op, MPI_Comm comm);
int MPI_Reduce(const void *sendbuf, void *recvbuf, int count,
        MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm);
int MPI_Scan(const void *sendbuf, void *recvbuf, int count,
        MPI_Datatype datatype, MPI_Op op, MPI_Comm comm);
int MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype,
        void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm);
int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,
        void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm);
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Implementations are optimized by vendors for their custom networks, and can be very fast.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.mcs.anl.gov/~fischer/gop/bgp_gop_png.png&#34; alt=&#34;Fischer BGP&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how the time is basically independent of number of processes $P$, and only a small multiple of the cost to send a single message. Not all networks are this good.&lt;/p&gt;

&lt;h3 id=&#34;point-to-point-messaging&#34;&gt;Point-to-point messaging&lt;/h3&gt;

&lt;p&gt;In addition to collectives, MPI supports messaging directly between individual ranks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;mpi-send-recv.png&#34; alt=&#34;send-recv&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Interfaces can be:

&lt;ul&gt;
&lt;li&gt;blocking like &lt;code&gt;MPI_Send()&lt;/code&gt; and &lt;code&gt;MPI_Recv()&lt;/code&gt;, or&lt;/li&gt;
&lt;li&gt;&amp;ldquo;immediate&amp;rdquo; (asynchronous), like &lt;code&gt;MPI_Isend()&lt;/code&gt; and &lt;code&gt;MPI_Irecv()&lt;/code&gt;.  The immediate varliants return an &lt;code&gt;MPI_Request&lt;/code&gt;, which must be waited on to complete the send or receive.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Be careful of deadlock when using blocking interfaces.

&lt;ul&gt;
&lt;li&gt;I never use blocking send/recv.&lt;/li&gt;
&lt;li&gt;There are also &amp;ldquo;synchronous&amp;rdquo; &lt;code&gt;MPI_Ssend&lt;/code&gt; and &amp;ldquo;buffered&amp;rdquo; &lt;code&gt;MPI_Bsend&lt;/code&gt;, and nonblocking variants of these, &lt;code&gt;MPI_Issend&lt;/code&gt;, etc.&lt;/li&gt;
&lt;li&gt;I never use these either (with one cool exception that we&amp;rsquo;ll talk about).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Point-to-point messaging is like the assembly of parallel computing

&lt;ul&gt;
&lt;li&gt;It can be good for building libraries, but it&amp;rsquo;s a headache to use directly for most purposes&lt;/li&gt;
&lt;li&gt;Better to use collectives when possible, or higher level libraries&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;neighbors&#34;&gt;Neighbors&lt;/h3&gt;

&lt;p&gt;A common pattern involves communicating with neighbors, often many times in sequence (such as each iteration or time step).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;mpi-neighbor-grid.png&#34; alt=&#34;Neighbor comm&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This can be achieved with
* Point-to-point: &lt;code&gt;MPI_Isend&lt;/code&gt;, &lt;code&gt;MPI_Irecv&lt;/code&gt;, &lt;code&gt;MPI_Waitall&lt;/code&gt;
* Persistent: &lt;code&gt;MPI_Send_init&lt;/code&gt; (once), &lt;code&gt;MPI_Startall&lt;/code&gt;, &lt;code&gt;MPI_Waitall&lt;/code&gt;.
* Neighborhood collectives (need to create special communicator)
* One-sided (need to manage safety yourself)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>More bitonic sorting, graphs</title>
      <link>https://cucs-hpsc.github.io/fall2019/sorting-graphs/</link>
      <pubDate>Mon, 23 Sep 2019 06:49:25 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/sorting-graphs/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
import matplotlib.pyplot as plt
import pandas
import numpy as np
import itertools
plt.style.use(&#39;seaborn&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;bitonic-sorting&#34;&gt;Bitonic sorting&lt;/h2&gt;

&lt;h3 id=&#34;definition-bitonic-sequence&#34;&gt;Definition: bitonic sequence&lt;/h3&gt;

&lt;p&gt;A bitonic sequence of length $n$ satisfies
$$ x_0 \le x_1 \le \dotsb \le x&lt;em&gt;k \ge x&lt;/em&gt;{k+1} \ge \dotsb \ge x_{n-1} $$
&lt;strong&gt;or a cyclic shift&lt;/strong&gt; thereof,
$$\hat x&lt;em&gt;i = x&lt;/em&gt;{i+c \bmod n} .$$&lt;/p&gt;

&lt;p&gt;sorting relies on the bitonic swapping operation,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bitonic_split(x):
    L = len(x) // 2
    for i in range(L):    # each pair is independent
        if (x[i] &amp;gt; x[L + i]):
            x[i], x[L + i] = x[L + i], x[i]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;after which the resulting subsequences &lt;code&gt;x[:L]&lt;/code&gt; and &lt;code&gt;x[L:]&lt;/code&gt; are bitonic and &lt;code&gt;max(x[:L]) &amp;lt;= min(x[L:])&lt;/code&gt;. (It is beyond the scope of this class to formally prove this property, but we&amp;rsquo;ll show examples.)&lt;/p&gt;

&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = list(range(10, 22)) + [16, 13, 10, 7]
idx = np.arange(len(x))
plt.plot(x, &#39;s&#39;, label=&#39;orig&#39;)
bitonic_split(x)
plt.plot(x, &#39;^k&#39;, label=&#39;swapped&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_4_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the bitonic sequence on the right side is cyclicly permuted.&lt;/p&gt;

&lt;h3 id=&#34;putting-it-together&#34;&gt;Putting it together&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/b/bd/BitonicSort1.svg&#34; alt=&#34;Bitonic sort from Wikipedia&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The blue blocks are forward bitonic merge networks, consisting of &lt;code&gt;bitonic_split&lt;/code&gt; followed by recursive splits.&lt;/li&gt;
&lt;li&gt;The green blocks are reverse networks.&lt;/li&gt;
&lt;li&gt;The left half of the diagram constructs a global bitonic sequence that is increasing in the top half and decreasing in the bottom half. This bitonic sequence is balanced and not &amp;ldquo;shifted&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;The right half of the diagram merges a global bitonic sequence. The bitonic sequences produced in each stage of the merge may be unbalanced and/or cyclicly shifted.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;demo&#34;&gt;Demo&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bitonic_sort(up, x, start=0, end=None, plot=[]):
    if end is None:
        end = len(x)
    if end - start &amp;lt;= 1:
        return
    mid = start + (end - start) // 2
    bitonic_sort(True, x, start, mid)
    bitonic_sort(False, x, mid, end)
    if (start, end) in plot:
        bitonic_plot(x, start, end, &#39;sort&#39;)
    bitonic_merge(up, x, start, end, plot=plot)

def bitonic_merge(up, x, start, end, plot=False): 
    # assume input x is bitonic, and sorted list is returned 
    if end - start == 1:
        return
    bitonic_split2(up, x, start, end)
    if (start, end) in plot:
        bitonic_plot(x, start, end, &#39;merge&#39;)
    mid = start + (end - start) // 2
    bitonic_merge(up, x, start, mid, plot=plot)
    bitonic_merge(up, x, mid, end, plot=plot)

def bitonic_split2(up, x, start, end):
    L = (end - start) // 2
    for i in range(start, start + L):
        if (x[i] &amp;gt; x[L + i]) == up:
            x[i], x[L + i] = x[L + i], x[i] # swap

def bitonic_plot(x, start, end, phase):
    plt.plot(range(start, end), x[start:end], next(marker), label=f&#39;{phase} {start}:{end}&#39;)
            
marker = itertools.cycle([&#39;s&#39;, &#39;o&#39;, &#39;^&#39;, &#39;&amp;lt;&#39;, &#39;*&#39;])
x = list(range(10, 38)) + [16, 13, 10, 7]
plt.plot(x, next(marker), label=&#39;orig&#39;)
bitonic_sort(True, x, plot=[(0, 32), (0,16), (8,16)])
#plt.plot(x, next(marker), label=&#39;sorted&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_7_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;further-resources-on-sorting&#34;&gt;Further resources on Sorting&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Chapter 9 of &lt;a href=&#34;https://www-users.cs.umn.edu/~karypis/parbook/&#34; target=&#34;_blank&#34;&gt;Grama, Gupta, Karypis, Kumar (2003), &lt;strong&gt;Introduction to Parallel Computing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www-users.cs.umn.edu/~karypis/parbook/Lectures/AG/chap9_slides.pdf&#34; target=&#34;_blank&#34;&gt;Grama slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;graphs&#34;&gt;Graphs&lt;/h1&gt;

&lt;p&gt;An (undirected) graph $(V, E)$ is a set of vertices $V$ and unordered pairs $(u,v) = (v,u) \in E$ of vertices $u,v \in V$.&lt;/p&gt;

&lt;p&gt;Graphs are often expressed by their adjacency matrix of dimension $n\times n$ where $n = |V|$,
$$ A_{ij} = \begin{cases}
    1, &amp;amp; \text{if } (i,j) \in E &lt;br /&gt;
    0,              &amp;amp; \text{otherwise}
\end{cases}
$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import networkx as nx

G = nx.grid_2d_graph(3, 3)
nx.draw(G, with_labels=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_10_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;A = nx.adjacency_matrix(G)
A.todense()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;matrix([[0, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 1, 0, 1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 1, 0, 0, 0],
        [1, 0, 0, 0, 1, 0, 1, 0, 0],
        [0, 1, 0, 1, 0, 1, 0, 1, 0],
        [0, 0, 1, 0, 1, 0, 0, 0, 1],
        [0, 0, 0, 1, 0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1, 0, 1, 0, 1],
        [0, 0, 0, 0, 0, 1, 0, 1, 0]], dtype=int64)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;compressed-representation&#34;&gt;Compressed representation&lt;/h3&gt;

&lt;p&gt;Adjacency matrices often have many zeros so it&amp;rsquo;s common to store a compressed representation.
We&amp;rsquo;ll revisit such formats for sparse matrices.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;A.indptr, A.indices
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(array([ 0,  2,  5,  7, 10, 14, 17, 19, 22, 24], dtype=int32),
 array([1, 3, 0, 2, 4, 1, 5, 0, 4, 6, 1, 3, 5, 7, 2, 4, 8, 3, 7, 4, 6, 8,
        5, 7], dtype=int32))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for row in range(A.shape[0]):
    print(A.indices[A.indptr[row]:A.indptr[row+1]])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[1 3]
[0 2 4]
[1 5]
[0 4 6]
[1 3 5 7]
[2 4 8]
[3 7]
[4 6 8]
[5 7]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;maximal-independent-set-mis&#34;&gt;Maximal independent set (MIS)&lt;/h2&gt;

&lt;p&gt;An independent set is a set of vertices $S \subset V$ such that $(u,v) \notin E$ for any pair $u,v \in S$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mis = nx.maximal_independent_set(G)
mis
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[(2, 0), (1, 2), (0, 0)]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plot_mis(G, mis):
    node_colors = [&#39;red&#39; if n in mis else &#39;#1f78b4&#39; for n in G.nodes()]
    nx.draw_networkx(G, node_color = node_colors)
plot_mis(G, mis)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_17_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Maximal independent sets are not unique
plot_mis(G, [(0,0), (0,2), (1,1), (2,0), (2,2)])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_18_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# We can coax the greedy algorithm to give a better MIS by specifying
# some nodes to include
plot_mis(G, nx.maximal_independent_set(G, [(1,1)]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_19_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;greedy-algorithms&#34;&gt;Greedy Algorithms&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Start with all vertices in candidate set $C = V$, empty $S$&lt;/li&gt;
&lt;li&gt;While $C \ne \emptyset$: Choose a vertex $v \in C$

&lt;ul&gt;
&lt;li&gt;Add $v$ to $S$&lt;/li&gt;
&lt;li&gt;Remove $v$ and all neighbors of $v$ from $C$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Algorithms differ in how they choose the next vertex $v \in C$.&lt;/p&gt;

&lt;h3 id=&#34;tiebreaking&#34;&gt;Tiebreaking&lt;/h3&gt;

&lt;p&gt;Suppose we index the vertices by integer and allow parallel selection of any $v$ for which
$$ v &amp;lt; \mathcal N(v) . $$&lt;/p&gt;

&lt;h4 id=&#34;hash-variant&#34;&gt;Hash variant&lt;/h4&gt;

&lt;p&gt;Consider a hash function $h(v)$ and allow any time&lt;/p&gt;

&lt;p&gt;$$ h(v) &amp;lt; \min_{u\in \mathcal N(v)} h(u). $$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;G = nx.karate_club_graph()
plot_mis(G, nx.maximal_independent_set(G))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;/usr/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: 
The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.
  if not cb.iterable(width):
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_21_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;further-resources&#34;&gt;Further resources&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Chapter 10 of &lt;a href=&#34;https://www-users.cs.umn.edu/~karypis/parbook/&#34; target=&#34;_blank&#34;&gt;Grama, Gupta, Karypis, Kumar (2003), &lt;strong&gt;Introduction to Parallel Computing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www-users.cs.umn.edu/~karypis/parbook/Lectures/AG/chap10_slides.pdf&#34; target=&#34;_blank&#34;&gt;Grama slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Parallel Reductions and Scans</title>
      <link>https://cucs-hpsc.github.io/fall2019/strategies/</link>
      <pubDate>Wed, 18 Sep 2019 06:49:25 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/strategies/</guid>
      <description>

&lt;h2 id=&#34;reductions&#34;&gt;Reductions&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;double reduce(int n, double x[]) {
    double y = 0;
    for (int i=0; i&amp;lt;n; i++)
        y += x[i];
    return y;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;abtin-reduction-iterative.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;dag-properties&#34;&gt;DAG properties&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Work $W(n) = n$&lt;/li&gt;
&lt;li&gt;Depth $D(n) = n$&lt;/li&gt;
&lt;li&gt;Parallelism $P(n) = \frac{W(n)}{D(n)} = 1$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;a-2-level-method&#34;&gt;A 2-level method&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;double reduce(int n, double x[]) {
    int P = sqrt(n); // ways of parallelism
    double y[P];
    #pragma omp parallel for shared(y)
    for (int p=0; p&amp;lt;P; p++) {
        y[p] = 0;
        for (int i=0; i&amp;lt;n/P; i++)
            y[p] += x[p*(n/P) + i];
    }
    double sum = 0;
    for (int p=0; p&amp;lt;P; p++)
        sum += y[p];
    return sum;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;dag-properties-1&#34;&gt;DAG properties&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Work $W(n) = n + \sqrt{n}$&lt;/li&gt;
&lt;li&gt;Depth $D(n) = 2 \sqrt{n}$&lt;/li&gt;
&lt;li&gt;Parallelism $P(n) = \sqrt{n}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;pram-performance-model&#34;&gt;PRAM performance model&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Processing units (e.g., OpenMP threads) execute local programs&lt;/li&gt;
&lt;li&gt;Communication through shared memory with no access cost&lt;/li&gt;
&lt;li&gt;Synchronous operation on a common clock

&lt;ul&gt;
&lt;li&gt;Barrier-like constructs are free&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Multiple Instruction, Multiple Data (MIMD)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;scheduling&#34;&gt;Scheduling&lt;/h4&gt;

&lt;p&gt;How much time does it take to execute a DAG on $p$ processors?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Sum work of each node $i$ along critical path of length $D(n)$
$$ \sum_{i=1}^{D(n)} W_i $$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Partition total work $W(n)$ over $p \le P(n)$ processors (as though there were no data dependencies)
$$ \left\lceil \frac{W(n)}{p} \right\rceil $$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Total time must be at least as large as either of these
$$ T(n,p) \ge \max\left( D(n), \left\lceil \frac{W(n)}{p} \right\rceil \right) $$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;more-levels&#34;&gt;More levels?&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;double reduce(int n, double x[]) {
    if (n == 1) return x[0];
    double y[n/2];
    #pragma omp parallel for shared(y)
    for (int i=0; i&amp;lt;n/2; i++)
        y[i] = x[2*i] + x[2*i+1];
    return reduce(n/2, y);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;abtin-reduction-recursive.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;dag-properties-2&#34;&gt;DAG properties&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;$W(n) = n/2 + n/4 + n/8 + \dotsb = n$&lt;/li&gt;
&lt;li&gt;$D(n) = \log_2 n$&lt;/li&gt;
&lt;li&gt;$P(n) = n/2$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;parallel-scans&#34;&gt;Parallel scans&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void scan(int n, double x[], double y[]) {
    y[0] = x[0];
    for (int i=1; i&amp;lt;n; i++)
        y[i] = y[i-1] + x[i];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;What are the DAG properties of this algorithm?&lt;/li&gt;
&lt;li&gt;How fast can we make it?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;abtin-scan-recursive.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void scan_inplace(int n, double y[], int stride) {
    if (2*stride &amp;gt; n) return;
    #pragma omp parallel for
    for (int i=2*stride-1; i&amp;lt;n; i+=2*stride)
        y[i] += [i - stride];

    scan(n, y, 2*stride);

    #pragma omp parallel for
    for (int i=3*stride-1; i&amp;lt;n; i+=2*stride)
        y[i] += y[i - stride];
}

// call like
scan_inplace(n, x, 1);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;application-of-scans-parallel-select&#34;&gt;Application of scans: parallel select&lt;/h3&gt;

&lt;p&gt;Select elements of array &lt;code&gt;x[]&lt;/code&gt; that satisfy a condition.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int c[n];
#pragma omp parallel for
for (int i=0; i&amp;lt;n; i++)
    c[i] = cond(x[i]); // returns 1 or 0

scan_inplace(n, c, 1);

double results[c[n-1]]; // allocate array with total number of items
#pragma omp parallel for
for (int i=0; i&amp;lt;n; i++)
    if (cond(x[i])) // Can use `c[i] - c[i-1]` to avoid recomputing
        results[c[i]-1] = x[i];
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Figures courtesy Abtin Rahimian&amp;rsquo;s course notes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenMP Tasks</title>
      <link>https://cucs-hpsc.github.io/fall2019/openmp-3/</link>
      <pubDate>Mon, 16 Sep 2019 06:49:25 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/openmp-3/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def render_c(filename):
    from IPython.display import Markdown
    with open(filename) as f:
        contents = f.read()
    return Markdown(&amp;quot;```c\n&amp;quot; + contents + &amp;quot;```\n&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;using-pragma-omp-task&#34;&gt;Using &lt;code&gt;#pragma omp task&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Up to now, we&amp;rsquo;ve been expressing parallelism for iterating over an array.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;task_dep.4.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
int main() {
  int x = 1;
  #pragma omp parallel
  #pragma omp single
  {
    #pragma omp task shared(x) depend(out: x)
    x = 2;
    #pragma omp task shared(x) depend(in: x)
    printf(&amp;quot;x + 1 = %d. &amp;quot;, x+1);
    #pragma omp task shared(x) depend(in: x)
    printf(&amp;quot;x + 2 = %d. &amp;quot;, x+2);
  }
  puts(&amp;quot;&amp;quot;);
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!make CFLAGS=-fopenmp -B task_dep.4
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cc -fopenmp    task_dep.4.c   -o task_dep.4
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!for i in {1..10}; do ./task_dep.4; done
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;x + 2 = 4. x + 1 = 3. 
x + 1 = 3. x + 2 = 4. 
x + 2 = 4. x + 1 = 3. 
x + 1 = 3. x + 2 = 4. 
x + 2 = 4. x + 1 = 3. 
x + 1 = 3. x + 2 = 4. 
x + 1 = 3. x + 2 = 4. 
x + 2 = 4. x + 1 = 3. 
x + 2 = 4. x + 1 = 3. 
x + 2 = 4. x + 1 = 3. 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;task_dep.4inout.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
int main() {
  int x = 1;
  #pragma omp parallel
  #pragma omp single
  {
    #pragma omp task shared(x) depend(out: x)
    x = 2;
    #pragma omp task shared(x) depend(inout: x)
    printf(&amp;quot;x + 1 = %d. &amp;quot;, x+1);
    #pragma omp task shared(x) depend(in: x)
    printf(&amp;quot;x + 2 = %d. &amp;quot;, x+2);
  }
  puts(&amp;quot;&amp;quot;);
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!make CFLAGS=-fopenmp -B task_dep.4inout
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cc -fopenmp    task_dep.4inout.c   -o task_dep.4inout
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!for i in {1..10}; do ./task_dep.4inout; done
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;x + 1 = 3. x + 2 = 4. 
x + 1 = 3. x + 2 = 4. 
x + 1 = 3. x + 2 = 4. 
x + 1 = 3. x + 2 = 4. 
x + 1 = 3. x + 2 = 4. 
x + 1 = 3. x + 2 = 4. 
x + 1 = 3. x + 2 = 4. 
x + 1 = 3. x + 2 = 4. 
x + 1 = 3. x + 2 = 4. 
x + 1 = 3. x + 2 = 4. 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;computing-the-fibonacci-numbers-https-en-wikipedia-org-wiki-fibonacci-number-with-openmp&#34;&gt;Computing the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fibonacci_number&#34; target=&#34;_blank&#34;&gt;Fibonacci numbers&lt;/a&gt; with OpenMP&lt;/h2&gt;

&lt;p&gt;Fibonacci numbers are defined by the recurrence
\begin{align}
  F_0 &amp;amp;= 0 &lt;br /&gt;
  F_1 &amp;amp;= 1 &lt;br /&gt;
  F&lt;em&gt;n &amp;amp;= F&lt;/em&gt;{n-1} + F_{n-2}
\end{align}&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;fib.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

long fib(long n) {
  if (n &amp;lt; 2) return n;
  return fib(n - 1) + fib(n - 2);
}

int main(int argc, char **argv) {
  if (argc != 2) {
    fprintf(stderr, &amp;quot;Usage: %s N\n&amp;quot;, argv[0]);
    return 1;
  }
  long N = atol(argv[1]);
  long fibs[N];
  #pragma omp parallel for
  for (long i=0; i&amp;lt;N; i++)
    fibs[i] = fib(i+1);
  for (long i=0; i&amp;lt;N; i++)
    printf(&amp;quot;%2ld: %5ld\n&amp;quot;, i+1, fibs[i]);
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!make CFLAGS=&#39;-O2 -march=native -fopenmp -Wall&#39; -B fib
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cc -O2 -march=native -fopenmp -Wall    fib.c   -o fib
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!OMP_NUM_THREADS=4 time ./fib 40
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt; 1:     1
 2:     1
 3:     2
 4:     3
 5:     5
 6:     8
 7:    13
 8:    21
 9:    34
10:    55
11:    89
12:   144
13:   233
14:   377
15:   610
16:   987
17:  1597
18:  2584
19:  4181
20:  6765
21: 10946
22: 17711
23: 28657
24: 46368
25: 75025
26: 121393
27: 196418
28: 317811
29: 514229
30: 832040
31: 1346269
32: 2178309
33: 3524578
34: 5702887
35: 9227465
36: 14930352
37: 24157817
38: 39088169
39: 63245986
40: 102334155
0.85user 0.00system 0:00.78elapsed 109%CPU (0avgtext+0avgdata 2044maxresident)k
0inputs+0outputs (0major+99minor)pagefaults 0swaps
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;use-tasks&#34;&gt;Use tasks&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;fib2.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

long fib(long n) {
  if (n &amp;lt; 2) return n;
  long n1, n2;
  #pragma omp task shared(n1)
  n1 = fib(n - 1);
  #pragma omp task shared(n2)
  n2 = fib(n - 2);
  #pragma omp taskwait
  return n1 + n2;
}

int main(int argc, char **argv) {
  if (argc != 2) {
    fprintf(stderr, &amp;quot;Usage: %s N\n&amp;quot;, argv[0]);
    return 1;
  }
  long N = atol(argv[1]);
  long fibs[N];
  #pragma omp parallel
  #pragma omp single nowait
  {
    for (long i=0; i&amp;lt;N; i++)
      fibs[i] = fib(i+1);
  }
  for (long i=0; i&amp;lt;N; i++)
    printf(&amp;quot;%2ld: %5ld\n&amp;quot;, i+1, fibs[i]);
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!make CFLAGS=&#39;-O2 -march=native -fopenmp -Wall&#39; fib2
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;make: &#39;fib2&#39; is up to date.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!OMP_NUM_THREADS=2 time ./fib2 30
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt; 1:     1
 2:     1
 3:     2
 4:     3
 5:     5
 6:     8
 7:    13
 8:    21
 9:    34
10:    55
11:    89
12:   144
13:   233
14:   377
15:   610
16:   987
17:  1597
18:  2584
19:  4181
20:  6765
21: 10946
22: 17711
23: 28657
24: 46368
25: 75025
26: 121393
27: 196418
28: 317811
29: 514229
30: 832040
2.42user 0.81system 0:02.54elapsed 127%CPU (0avgtext+0avgdata 2028maxresident)k
0inputs+0outputs (0major+100minor)pagefaults 0swaps
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It&amp;rsquo;s expensive to create tasks when &lt;code&gt;n&lt;/code&gt; is small, even with only one thread.  How can we cut down on that overhead?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;fib3.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

long fib(long n) {
if (n &amp;lt; 2) return n;
if (n &amp;lt; 30)
return fib(n - 1) + fib(n - 2);
long n1, n2;
#pragma omp task shared(n1)
n1 = fib(n - 1);
#pragma omp task shared(n2)
n2 = fib(n - 2);
#pragma omp taskwait
return n1 + n2;
}

int main(int argc, char **argv) {
if (argc != 2) {
fprintf(stderr, &amp;quot;Usage: %s N\n&amp;quot;, argv[0]);
return 1;
}
long N = atol(argv[1]);
long fibs[N];
#pragma omp parallel
#pragma omp single nowait
{
for (long i=0; i&amp;lt;N; i++)
  fibs[i] = fib(i+1);
}
for (long i=0; i&amp;lt;N; i++)
printf(&amp;quot;%2ld: %5ld\n&amp;quot;, i+1, fibs[i]);
return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!make CFLAGS=&#39;-O2 -march=native -fopenmp -Wall&#39; fib3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cc -O2 -march=native -fopenmp -Wall    fib3.c   -o fib3&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!OMP_NUM_THREADS=3 time ./fib3 40
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1:     1
 2:     1
 3:     2
 4:     3
 5:     5
 6:     8
 7:    13
 8:    21
 9:    34
10:    55
11:    89
12:   144
13:   233
14:   377
15:   610
16:   987
17:  1597
18:  2584
19:  4181
20:  6765
21: 10946
22: 17711
23: 28657
24: 46368
25: 75025
26: 121393
27: 196418
28: 317811
29: 514229
30: 832040
31: 1346269
32: 2178309
33: 3524578
34: 5702887
35: 9227465
36: 14930352
37: 24157817
38: 39088169
39: 63245986
40: 102334155
3.56user 0.00system 0:01.27elapsed 280%CPU (0avgtext+0avgdata 1920maxresident)k
0inputs+0outputs (0major+103minor)pagefaults 0swaps&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This is just slower, even with one thread.  Why might that be?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;fib4.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

long fib_seq(long n) {
if (n &amp;lt; 2) return n;
return fib_seq(n - 1) + fib_seq(n - 2);
}

long fib(long n) {
if (n &amp;lt; 30)
return fib_seq(n);
long n1, n2;
#pragma omp task shared(n1)
n1 = fib(n - 1);
#pragma omp task shared(n2)
n2 = fib(n - 2);
#pragma omp taskwait
return n1 + n2;
}

int main(int argc, char **argv) {
if (argc != 2) {
fprintf(stderr, &amp;quot;Usage: %s N\n&amp;quot;, argv[0]);
return 1;
}
long N = atol(argv[1]);
long fibs[N];
#pragma omp parallel
#pragma omp single nowait
{
for (long i=0; i&amp;lt;N; i++)
  fibs[i] = fib(i+1);
}
for (long i=0; i&amp;lt;N; i++)
printf(&amp;quot;%2ld: %5ld\n&amp;quot;, i+1, fibs[i]);
return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!make CFLAGS=&#39;-O2 -march=native -fopenmp -Wall&#39; fib4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;make: &amp;lsquo;fib4&amp;rsquo; is up to date.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!OMP_NUM_THREADS=2 time ./fib4 40
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1:     1
 2:     1
 3:     2
 4:     3
 5:     5
 6:     8
 7:    13
 8:    21
 9:    34
10:    55
11:    89
12:   144
13:   233
14:   377
15:   610
16:   987
17:  1597
18:  2584
19:  4181
20:  6765
21: 10946
22: 17711
23: 28657
24: 46368
25: 75025
26: 121393
27: 196418
28: 317811
29: 514229
30: 832040
31: 1346269
32: 2178309
33: 3524578
34: 5702887
35: 9227465
36: 14930352
37: 24157817
38: 39088169
39: 63245986
40: 102334155
0.94user 0.00system 0:00.53elapsed 177%CPU (0avgtext+0avgdata 2040maxresident)k
8inputs+0outputs (0major+97minor)pagefaults 0swaps&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;alt-schedule-static-1&#34;&gt;Alt: &lt;code&gt;schedule(static,1)&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;fib5.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

long fib(long n) {
  if (n &amp;lt; 2) return n;
  return fib(n - 1) + fib(n - 2);
}

int main(int argc, char **argv) {
  if (argc != 2) {
    fprintf(stderr, &amp;quot;Usage: %s N\n&amp;quot;, argv[0]);
    return 1;
  }
  long N = atol(argv[1]);
  long fibs[N];
  #pragma omp parallel for schedule(static,1)
  for (long i=0; i&amp;lt;N; i++)
    fibs[i] = fib(i+1);
  for (long i=0; i&amp;lt;N; i++)
    printf(&amp;quot;%2ld: %5ld\n&amp;quot;, i+1, fibs[i]);
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!make CFLAGS=&#39;-O2 -march=native -fopenmp -Wall&#39; fib5
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;make: &#39;fib5&#39; is up to date.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!OMP_NUM_THREADS=2 time ./fib5 40
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt; 1:     1
 2:     1
 3:     2
 4:     3
 5:     5
 6:     8
 7:    13
 8:    21
 9:    34
10:    55
11:    89
12:   144
13:   233
14:   377
15:   610
16:   987
17:  1597
18:  2584
19:  4181
20:  6765
21: 10946
22: 17711
23: 28657
24: 46368
25: 75025
26: 121393
27: 196418
28: 317811
29: 514229
30: 832040
31: 1346269
32: 2178309
33: 3524578
34: 5702887
35: 9227465
36: 14930352
37: 24157817
38: 39088169
39: 63245986
40: 102334155
0.88user 0.00system 0:00.54elapsed 161%CPU (0avgtext+0avgdata 1908maxresident)k
8inputs+0outputs (0major+93minor)pagefaults 0swaps
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;better-math&#34;&gt;Better math&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;fib6.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

int main(int argc, char **argv) {
  if (argc != 2) {
    fprintf(stderr, &amp;quot;Usage: %s N\n&amp;quot;, argv[0]);
    return 1;
  }
  long N = atol(argv[1]);
  long fibs[N];
  fibs[0] = 1;
  fibs[1] = 2;
  for (long i=2; i&amp;lt;N; i++)
    fibs[i] = fibs[i-1] + fibs[i-2];
  for (long i=0; i&amp;lt;N; i++)
    printf(&amp;quot;%2ld: %5ld\n&amp;quot;, i+1, fibs[i]);
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!make CFLAGS=&#39;-O2 -march=native -fopenmp -Wall&#39; fib6
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cc -O2 -march=native -fopenmp -Wall    fib6.c   -o fib6
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!time ./fib6 100
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt; 1:     1
 2:     2
 3:     3
 4:     5
 5:     8
 6:    13
 7:    21
 8:    34
 9:    55
10:    89
11:   144
12:   233
13:   377
14:   610
15:   987
16:  1597
17:  2584
18:  4181
19:  6765
20: 10946
21: 17711
22: 28657
23: 46368
24: 75025
25: 121393
26: 196418
27: 317811
28: 514229
29: 832040
30: 1346269
31: 2178309
32: 3524578
33: 5702887
34: 9227465
35: 14930352
36: 24157817
37: 39088169
38: 63245986
39: 102334155
40: 165580141
41: 267914296
42: 433494437
43: 701408733
44: 1134903170
45: 1836311903
46: 2971215073
47: 4807526976
48: 7778742049
49: 12586269025
50: 20365011074
51: 32951280099
52: 53316291173
53: 86267571272
54: 139583862445
55: 225851433717
56: 365435296162
57: 591286729879
58: 956722026041
59: 1548008755920
60: 2504730781961
61: 4052739537881
62: 6557470319842
63: 10610209857723
64: 17167680177565
65: 27777890035288
66: 44945570212853
67: 72723460248141
68: 117669030460994
69: 190392490709135
70: 308061521170129
71: 498454011879264
72: 806515533049393
73: 1304969544928657
74: 2111485077978050
75: 3416454622906707
76: 5527939700884757
77: 8944394323791464
78: 14472334024676221
79: 23416728348467685
80: 37889062373143906
81: 61305790721611591
82: 99194853094755497
83: 160500643816367088
84: 259695496911122585
85: 420196140727489673
86: 679891637638612258
87: 1100087778366101931
88: 1779979416004714189
89: 2880067194370816120
90: 4660046610375530309
91: 7540113804746346429
92: -6246583658587674878
93: 1293530146158671551
94: -4953053512429003327
95: -3659523366270331776
96: -8612576878699335103
97: 6174643828739884737
98: -2437933049959450366
99: 3736710778780434371
100: 1298777728820984005
0.002 real   0.002 user   0.000 sys   99.42 cpu
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;to-fork-join-or-to-task&#34;&gt;To fork/join or to task?&lt;/h2&gt;

&lt;p&gt;When the work unit &lt;strong&gt;size&lt;/strong&gt; and &lt;strong&gt;compute speed&lt;/strong&gt; is predictable, we can partition work in advance and schedule with &lt;code&gt;omp for&lt;/code&gt; to achieve load balance.
Satisfying both criteria is often hard:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Adaptive algorithms, adaptive physics, implicit constitutive models&lt;/li&gt;
&lt;li&gt;AVX throttling, thermal throttling, network or file system contention, OS jitter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fork/join and barriers are also high overhead, so we might want to express data dependencies more precisely.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://community.arm.com/resized-image/__size/1040x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-37-98/Screenshot-2019_2D00_09_2D00_02-at-17.50.07.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For tasking to be efficient, it relies on &lt;strong&gt;overdecomposition&lt;/strong&gt;, creating more work units than there are processing units.
For many numerical algorithms, there is some overhead to overdecomposition.  For example, in array processing, a halo/fringe/ghost/overlap region might need to be computed as part of each work patch, leading to time models along the lines of
$$ t&lt;em&gt;{\text{tile}}(n) = t&lt;/em&gt;{\text{latency}} + \frac{(n+2)^3}{R} $$
where $R$ is the processing rate.
In addition to the latency, the overhead fraction is
$$ \frac{(n+2)^3 - n^3}{n^3} \approx 6/n $$
indicating that larger $n$ should be more efficient.&lt;/p&gt;

&lt;p&gt;However, if this overhead is acceptable and you still have load balancing challenges, tasking can be a solution.
(Example from a recent &lt;a href=&#34;https://community.arm.com/developer/research/b/articles/posts/tasking-lives-up-to-its-promises&#34; target=&#34;_blank&#34;&gt;blog/talk&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://community.arm.com/resized-image/__size/2080x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-37-98/timestamp.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;computational-depth-and-the-critical-path&#34;&gt;Computational depth and the critical path&lt;/h2&gt;

&lt;p&gt;Consider the block Cholesky factorization algorithm (applying to the lower-triangular matrix $A$).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./chol-alg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Expressing essential data dependencies, this results in the following directed acyclic graph (DAG).
No parallel algorithm can complete in less time than it takes for a sequential algorithm to perform each operation along the critical path (i.e., the minimum depth of this graph such that all arrows point downward).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./chol-graph.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figures from &lt;a href=&#34;https://doi.org/10.1109/IPDPS.2016.9&#34; target=&#34;_blank&#34;&gt;Agullo et al (2016): Are Static Schedules so Bad? A Case Study on Cholesky Factorization&lt;/a&gt;, which is an interesting counterpoint to the common narrative pushing dynamic scheduling.&lt;/p&gt;

&lt;h3 id=&#34;question-what-is-the-computational-depth-of-summing-an-array&#34;&gt;Question: what is the computational depth of summing an array?&lt;/h3&gt;

&lt;p&gt;$$ \sum_{i=0}^{N-1} a_i $$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;double sum = 0;
for (int i=0; i&amp;lt;N; i++)
    sum += array[i];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Given an arbitrarily large number $P$ of processing units, what is the smallest computational depth to compute this mathematical result?  (You&amp;rsquo;re free to use any associativity.)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>More OpenMP</title>
      <link>https://cucs-hpsc.github.io/fall2019/openmp-2/</link>
      <pubDate>Fri, 13 Sep 2019 06:49:25 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/openmp-2/</guid>
      <description>

&lt;p&gt;What does the compiler do when we add &lt;code&gt;#pragma omp parallel&lt;/code&gt;?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;static double dot_opt3(size_t n, const double *a, const double *b) {
  double sum = 0;
  omp_set_num_threads(4);
  #pragma omp parallel
  {
    #pragma omp for reduction(+:sum)
    for (size_t i=0; i&amp;lt;n; i++)
      sum += a[i] * b[i];
  }
  return sum;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcc -Os -march=native -fopenmp dot.c -o dot
objdump -d --prefix-addresses -M intel dot | grep dot_opt3
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;000000000000129f &amp;lt;main+0x1af&amp;gt; call   0000000000001779 &amp;lt;dot_opt3&amp;gt;
0000000000001779 &amp;lt;dot_opt3&amp;gt; push   r12
000000000000177b &amp;lt;dot_opt3+0x2&amp;gt; mov    r12,rdx
000000000000177e &amp;lt;dot_opt3+0x5&amp;gt; push   rbp
000000000000177f &amp;lt;dot_opt3+0x6&amp;gt; mov    rbp,rsi
0000000000001782 &amp;lt;dot_opt3+0x9&amp;gt; push   rbx
0000000000001783 &amp;lt;dot_opt3+0xa&amp;gt; mov    rbx,rdi
0000000000001786 &amp;lt;dot_opt3+0xd&amp;gt; mov    edi,0x4
000000000000178b &amp;lt;dot_opt3+0x12&amp;gt; sub    rsp,0x30
000000000000178f &amp;lt;dot_opt3+0x16&amp;gt; mov    rax,QWORD PTR fs:0x28
0000000000001798 &amp;lt;dot_opt3+0x1f&amp;gt; mov    QWORD PTR [rsp+0x28],rax
000000000000179d &amp;lt;dot_opt3+0x24&amp;gt; xor    eax,eax
000000000000179f &amp;lt;dot_opt3+0x26&amp;gt; call   0000000000001070 &amp;lt;omp_set_num_threads@plt&amp;gt;
00000000000017a4 &amp;lt;dot_opt3+0x2b&amp;gt; xor    ecx,ecx
00000000000017a6 &amp;lt;dot_opt3+0x2d&amp;gt; xor    edx,edx
00000000000017a8 &amp;lt;dot_opt3+0x2f&amp;gt; lea    rsi,[rsp+0x8]
00000000000017ad &amp;lt;dot_opt3+0x34&amp;gt; lea    rdi,[rip+0xc1]        # 0000000000001875 &amp;lt;dot_opt3._omp_fn.0&amp;gt;
00000000000017b4 &amp;lt;dot_opt3+0x3b&amp;gt; mov    QWORD PTR [rsp+0x18],r12
00000000000017b9 &amp;lt;dot_opt3+0x40&amp;gt; mov    QWORD PTR [rsp+0x10],rbp
00000000000017be &amp;lt;dot_opt3+0x45&amp;gt; mov    QWORD PTR [rsp+0x8],rbx
00000000000017c3 &amp;lt;dot_opt3+0x4a&amp;gt; mov    QWORD PTR [rsp+0x20],0x0
00000000000017cc &amp;lt;dot_opt3+0x53&amp;gt; call   00000000000010e0 &amp;lt;GOMP_parallel@plt&amp;gt;
00000000000017d1 &amp;lt;dot_opt3+0x58&amp;gt; mov    rax,QWORD PTR [rsp+0x28]
00000000000017d6 &amp;lt;dot_opt3+0x5d&amp;gt; xor    rax,QWORD PTR fs:0x28
00000000000017df &amp;lt;dot_opt3+0x66&amp;gt; vmovsd xmm0,QWORD PTR [rsp+0x20]
00000000000017e5 &amp;lt;dot_opt3+0x6c&amp;gt; je     00000000000017ec &amp;lt;dot_opt3+0x73&amp;gt;
00000000000017e7 &amp;lt;dot_opt3+0x6e&amp;gt; call   0000000000001080 &amp;lt;__stack_chk_fail@plt&amp;gt;
00000000000017ec &amp;lt;dot_opt3+0x73&amp;gt; add    rsp,0x30
00000000000017f0 &amp;lt;dot_opt3+0x77&amp;gt; pop    rbx
00000000000017f1 &amp;lt;dot_opt3+0x78&amp;gt; pop    rbp
00000000000017f2 &amp;lt;dot_opt3+0x79&amp;gt; pop    r12
00000000000017f4 &amp;lt;dot_opt3+0x7b&amp;gt; ret    
0000000000001875 &amp;lt;dot_opt3._omp_fn.0&amp;gt; push   r12
0000000000001877 &amp;lt;dot_opt3._omp_fn.0+0x2&amp;gt; push   rbp
0000000000001878 &amp;lt;dot_opt3._omp_fn.0+0x3&amp;gt; mov    rbp,rdi
000000000000187b &amp;lt;dot_opt3._omp_fn.0+0x6&amp;gt; push   rbx
000000000000187c &amp;lt;dot_opt3._omp_fn.0+0x7&amp;gt; sub    rsp,0x10
0000000000001880 &amp;lt;dot_opt3._omp_fn.0+0xb&amp;gt; mov    rbx,QWORD PTR [rdi]
0000000000001883 &amp;lt;dot_opt3._omp_fn.0+0xe&amp;gt; test   rbx,rbx
0000000000001886 &amp;lt;dot_opt3._omp_fn.0+0x11&amp;gt; jne    00000000000018b5 &amp;lt;dot_opt3._omp_fn.0+0x40&amp;gt;
0000000000001888 &amp;lt;dot_opt3._omp_fn.0+0x13&amp;gt; vxorpd xmm0,xmm0,xmm0
000000000000188c &amp;lt;dot_opt3._omp_fn.0+0x17&amp;gt; mov    rax,QWORD PTR [rbp+0x18]
0000000000001890 &amp;lt;dot_opt3._omp_fn.0+0x1b&amp;gt; lea    rdx,[rbp+0x18]
0000000000001894 &amp;lt;dot_opt3._omp_fn.0+0x1f&amp;gt; mov    QWORD PTR [rsp],rax
0000000000001898 &amp;lt;dot_opt3._omp_fn.0+0x23&amp;gt; vaddsd xmm1,xmm0,QWORD PTR [rsp]
000000000000189d &amp;lt;dot_opt3._omp_fn.0+0x28&amp;gt; vmovsd QWORD PTR [rsp+0x8],xmm1
00000000000018a3 &amp;lt;dot_opt3._omp_fn.0+0x2e&amp;gt; mov    rdi,QWORD PTR [rsp+0x8]
00000000000018a8 &amp;lt;dot_opt3._omp_fn.0+0x33&amp;gt; lock cmpxchg QWORD PTR [rdx],rdi
00000000000018ad &amp;lt;dot_opt3._omp_fn.0+0x38&amp;gt; cmp    QWORD PTR [rsp],rax
00000000000018b1 &amp;lt;dot_opt3._omp_fn.0+0x3c&amp;gt; je     000000000000190c &amp;lt;dot_opt3._omp_fn.0+0x97&amp;gt;
00000000000018b3 &amp;lt;dot_opt3._omp_fn.0+0x3e&amp;gt; jmp    0000000000001894 &amp;lt;dot_opt3._omp_fn.0+0x1f&amp;gt;
00000000000018b5 &amp;lt;dot_opt3._omp_fn.0+0x40&amp;gt; call   00000000000010b0 &amp;lt;omp_get_num_threads@plt&amp;gt;
00000000000018ba &amp;lt;dot_opt3._omp_fn.0+0x45&amp;gt; mov    r12d,eax
00000000000018bd &amp;lt;dot_opt3._omp_fn.0+0x48&amp;gt; call   0000000000001060 &amp;lt;omp_get_thread_num@plt&amp;gt;
00000000000018c2 &amp;lt;dot_opt3._omp_fn.0+0x4d&amp;gt; movsxd rcx,eax
00000000000018c5 &amp;lt;dot_opt3._omp_fn.0+0x50&amp;gt; movsxd rsi,r12d
00000000000018c8 &amp;lt;dot_opt3._omp_fn.0+0x53&amp;gt; mov    rax,rbx
00000000000018cb &amp;lt;dot_opt3._omp_fn.0+0x56&amp;gt; xor    edx,edx
00000000000018cd &amp;lt;dot_opt3._omp_fn.0+0x58&amp;gt; div    rsi
00000000000018d0 &amp;lt;dot_opt3._omp_fn.0+0x5b&amp;gt; cmp    rcx,rdx
00000000000018d3 &amp;lt;dot_opt3._omp_fn.0+0x5e&amp;gt; jb     0000000000001905 &amp;lt;dot_opt3._omp_fn.0+0x90&amp;gt;
00000000000018d5 &amp;lt;dot_opt3._omp_fn.0+0x60&amp;gt; imul   rcx,rax
00000000000018d9 &amp;lt;dot_opt3._omp_fn.0+0x64&amp;gt; vxorpd xmm0,xmm0,xmm0
00000000000018dd &amp;lt;dot_opt3._omp_fn.0+0x68&amp;gt; add    rdx,rcx
00000000000018e0 &amp;lt;dot_opt3._omp_fn.0+0x6b&amp;gt; add    rax,rdx
00000000000018e3 &amp;lt;dot_opt3._omp_fn.0+0x6e&amp;gt; cmp    rdx,rax
00000000000018e6 &amp;lt;dot_opt3._omp_fn.0+0x71&amp;gt; jae    000000000000188c &amp;lt;dot_opt3._omp_fn.0+0x17&amp;gt;
00000000000018e8 &amp;lt;dot_opt3._omp_fn.0+0x73&amp;gt; mov    rcx,QWORD PTR [rbp+0x10]
00000000000018ec &amp;lt;dot_opt3._omp_fn.0+0x77&amp;gt; mov    rsi,QWORD PTR [rbp+0x8]
00000000000018f0 &amp;lt;dot_opt3._omp_fn.0+0x7b&amp;gt; vmovsd xmm2,QWORD PTR [rsi+rdx*8]
00000000000018f5 &amp;lt;dot_opt3._omp_fn.0+0x80&amp;gt; vfmadd231sd xmm0,xmm2,QWORD PTR [rcx+rdx*8]
00000000000018fb &amp;lt;dot_opt3._omp_fn.0+0x86&amp;gt; inc    rdx
00000000000018fe &amp;lt;dot_opt3._omp_fn.0+0x89&amp;gt; cmp    rax,rdx
0000000000001901 &amp;lt;dot_opt3._omp_fn.0+0x8c&amp;gt; jne    00000000000018f0 &amp;lt;dot_opt3._omp_fn.0+0x7b&amp;gt;
0000000000001903 &amp;lt;dot_opt3._omp_fn.0+0x8e&amp;gt; jmp    000000000000188c &amp;lt;dot_opt3._omp_fn.0+0x17&amp;gt;
0000000000001905 &amp;lt;dot_opt3._omp_fn.0+0x90&amp;gt; inc    rax
0000000000001908 &amp;lt;dot_opt3._omp_fn.0+0x93&amp;gt; xor    edx,edx
000000000000190a &amp;lt;dot_opt3._omp_fn.0+0x95&amp;gt; jmp    00000000000018d5 &amp;lt;dot_opt3._omp_fn.0+0x60&amp;gt;
000000000000190c &amp;lt;dot_opt3._omp_fn.0+0x97&amp;gt; add    rsp,0x10
0000000000001910 &amp;lt;dot_opt3._omp_fn.0+0x9b&amp;gt; pop    rbx
0000000000001911 &amp;lt;dot_opt3._omp_fn.0+0x9c&amp;gt; pop    rbp
0000000000001912 &amp;lt;dot_opt3._omp_fn.0+0x9d&amp;gt; pop    r12
0000000000001914 &amp;lt;dot_opt3._omp_fn.0+0x9f&amp;gt; ret    
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;anatomy-of-a-parallel-region&#34;&gt;Anatomy of a parallel region&lt;/h2&gt;

&lt;div class=&#34;mermaid&#34;&gt;
graph LR;
  A[&lt;tt&gt;dot_opt3&lt;/tt&gt;]--&gt;B[&lt;tt&gt;GOMP_parallel&lt;/tt&gt;];
  B-- id=0_ --&gt;C{&lt;tt&gt;dot_opt3._omp_fn.0&lt;/tt&gt;};
  B-- id=1_ --&gt;C{&lt;tt&gt;dot_opt3._omp_fn.0&lt;/tt&gt;};
  B-- id=2_ --&gt;C{&lt;tt&gt;dot_opt3._omp_fn.0&lt;/tt&gt;};
  A-. &#34;Body inside__&#34; .-&gt;C;
  C--&gt;D[&lt;tt&gt;omp_get_num_threads&lt;/tt&gt;];
  C--&gt;E[&lt;tt&gt;omp_get_thread_num&lt;/tt&gt;];
  style A fill:#9b9,stroke:#686,stroke-width:4px;
  style C fill:#9b9,stroke:#668,stroke-width:8px;
&lt;/div&gt;

&lt;h2 id=&#34;memory-semantics&#34;&gt;Memory semantics&lt;/h2&gt;

&lt;p&gt;For each variable accessed within the parallel region, we can specify
whether it is&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;private&lt;/code&gt; to the thread, with value undefined inside the region&lt;/li&gt;
&lt;li&gt;&lt;code&gt;firstprivate&lt;/code&gt;, which is like private, but initialized by the value
upon entering the parallel region&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;shared&lt;/code&gt;, meaning that every thread accesses the same value in
memory (but changes are not immediately visible)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int a=0, b=1, c=2;
#pragma omp parallel private(a) firstprivate(b) shared(c)
{
int id = omp_get_thread_num();
a++;
b++;
c++;
printf(&amp;quot;[%d] %d %d %d\n&amp;quot;, id, a, b, c);
}
printf(&amp;quot;END: %d %d %d\n&amp;quot;, a, b, c);
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make CFLAGS=&#39;-fopenmp -Wall&#39; -B omp-mem
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cc -fopenmp -Wall    omp-mem.c   -o omp-mem
omp-mem.c: In function âmain._omp_fn.0â:
omp-mem.c:8:6: warning: âaâ is used uninitialized in this function [-Wuninitialized]
8 |     a++;
  |     ~^~
omp-mem.c:5:7: note: âaâ was declared here
5 |   int a=1, b=2, c=3;
  |       ^
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; How could the compiler get &lt;code&gt;firstprivate&lt;/code&gt; and &lt;code&gt;shared&lt;/code&gt; variables into
the scope of &lt;code&gt;dot_opt3._omp_fn.0&lt;/code&gt;?&lt;/p&gt;

&lt;h3 id=&#34;programming-style&#34;&gt;Programming style&lt;/h3&gt;

&lt;p&gt;I find &lt;code&gt;private&lt;/code&gt; semantics unnecessary and error-prone.  We can just
declare those variables at inner-most scope.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;  int b=1, c=2;
  #pragma omp parallel firstprivate(b) shared(c)
  {
    int a = 0;
    int id = omp_get_thread_num();
    a++;
    b++;
    c++;
    printf(&amp;quot;[%d] %d %d %d\n&amp;quot;, id, a, b, c);
  }
  printf(&amp;quot;END: %d %d %d\n&amp;quot;, a, b, c); // Error: a not in scope here
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;updating-shared-variables&#34;&gt;Updating shared variables&lt;/h3&gt;

&lt;p&gt;We see that the shared variable &lt;code&gt;c&lt;/code&gt; has lots of opportunities for conflict.&lt;/p&gt;

&lt;div class=&#34;mermaid&#34;&gt;
sequenceDiagram
Thread0--&gt;Memory: load c=2
Thread1--&gt;Memory: load c=2
Note left of Thread0: c++ (=3)
Note right of Thread1: c++ (=3)
Note right of Thread1: print c
Thread0--&gt;Memory: store c=3
Thread1--&gt;Memory: store c=3
Note left of Thread0: print c
&lt;/div&gt;

&lt;p&gt;If we run the above many times, we may sometimes find that multiple
processes have the same value of &lt;code&gt;c&lt;/code&gt;, each thread observes different
increments from others, and the total number of increments may vary.&lt;/p&gt;

&lt;p&gt;We can define ordering semantics using &lt;code&gt;atomic&lt;/code&gt;, &lt;code&gt;critical&lt;/code&gt;, and &lt;code&gt;barrier&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;  int b=1, c=2;
  #pragma omp parallel firstprivate(b) shared(c)
  {
    int a = 1;
    int id = omp_get_thread_num();
    b++;
    #pragma omp critical
    c++;
    #pragma omp barrier
    printf(&amp;quot;[%d] %d %d %d\n&amp;quot;, id, a, b, c);
  }
  printf(&amp;quot;END: _ %d %d\n&amp;quot;, b, c);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;a-quick-demo-of-perf&#34;&gt;A quick demo of &lt;code&gt;perf&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Linux &lt;code&gt;perf&lt;/code&gt; is a kernel interrupt-based profiling tool.  It uses
performance counters and interrupts to diagnose all sorts of
bottlenecks.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ perf stat ./dot -n 10000 &amp;gt; /dev/null

 Performance counter stats for &#39;./dot -n 10000&#39;:

              1.56 msec task-clock:u              #    1.201 CPUs utilized          
                 0      context-switches:u        #    0.000 K/sec                  
                 0      cpu-migrations:u          #    0.000 K/sec                  
               124      page-faults:u             #    0.079 M/sec                  
         3,041,706      cycles:u                  #    1.947 GHz                    
         2,272,231      instructions:u            #    0.75  insn per cycle         
           410,889      branches:u                #  262.962 M/sec                  
             7,911      branch-misses:u           #    1.93% of all branches        

       0.001301176 seconds time elapsed

       0.001970000 seconds user
       0.000000000 seconds sys
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ perf record -g ./dot -n 10000 -r 1000 &amp;gt; /dev/null
[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.075 MB perf.data (1098 samples) ]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ perf report -M intel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;perf-report.png&#34; alt=&#34;perf report&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note how GOMP overhead dominates the cost in this experiment.  We need
more work (longer arrays, etc.) to justify the overhead of
distributing and collecting the parallel work.&lt;/p&gt;

&lt;p&gt;We can drill down into particular functions (especially ours, which we
have hopefully compiled with &lt;code&gt;-g&lt;/code&gt; to include debugging information).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;perf-report-ann.png&#34; alt=&#34;perf report annotation&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From this, we see specific instructions, and their corresponding lines
of code, that are most frequently being processed when the kernel
interrupts to check.  In this experiment, we see &lt;code&gt;*sd&lt;/code&gt; &amp;ldquo;scalar double&amp;rdquo;
instructions, indicating lack of vectorization.&lt;/p&gt;

&lt;p&gt;In contrast, the following annotation shows use of &lt;code&gt;*pd&lt;/code&gt; &amp;ldquo;packed
double&amp;rdquo; instructions, indicating that the &amp;ldquo;hot&amp;rdquo; loop has been
vectorized.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;perf-report-ann-vec.png&#34; alt=&#34;perf report annotation with vectorization&#34; /&gt;&lt;/p&gt;

&lt;p&gt;(The reason for vectorization can sometimes be determined by
&lt;code&gt;-fopt-info -fopt-info-missed&lt;/code&gt;, and can be encouraged by techniques
like manually splitting accumulators, preventing aliasing by using
&lt;code&gt;restrict&lt;/code&gt;, directives like &lt;code&gt;#pragma omp simd&lt;/code&gt;, and global compiler
flags like &lt;code&gt;-ffast-math&lt;/code&gt;.)&lt;/p&gt;

&lt;p&gt;For more on &lt;code&gt;perf&lt;/code&gt;, see &lt;a href=&#34;http://www.brendangregg.com/linuxperf.html&#34; target=&#34;_blank&#34;&gt;Brendan Gregg&amp;rsquo;s Linux Performance site&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenMP Basics</title>
      <link>https://cucs-hpsc.github.io/fall2019/openmp/</link>
      <pubDate>Wed, 11 Sep 2019 06:49:25 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/openmp/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def render_c(filename):
    from IPython.display import Markdown
    with open(filename) as f:
        contents = f.read()
    return Markdown(&amp;quot;```c\n&amp;quot; + contents + &amp;quot;```\n&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;what-is-openmp-https-en-wikipedia-org-wiki-openmp&#34;&gt;What is &lt;a href=&#34;https://en.wikipedia.org/wiki/OpenMP&#34; target=&#34;_blank&#34;&gt;OpenMP&lt;/a&gt;?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/f/f1/Fork_join.svg&#34; alt=&#34;By Wikipedia user A1 - w:en:File:Fork_join.svg, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=32004077&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A community-developed standard Application Programming Interface (with &amp;ldquo;directives&amp;rdquo;) for
* multithreaded programming
* vectorization
* offload to coprocessors (such as GPUs)&lt;/p&gt;

&lt;p&gt;OpenMP is available for C, C++, and Fortran.&lt;/p&gt;

&lt;p&gt;Latest version: OpenMP-5.0, released November 2018.  Implementations are still incomplete!&lt;/p&gt;

&lt;h3 id=&#34;openmp-resources&#34;&gt;OpenMP Resources&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.openmp.org/resources/refguides/&#34; target=&#34;_blank&#34;&gt;OpenMP-5.0 Reference Cards&lt;/a&gt; (a few pages, printable)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.openmp.org/spec-html/5.0/openmp.html&#34; target=&#34;_blank&#34;&gt;OpenMP-5.0 Standard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.openmp.org/wp-content/uploads/openmp-examples-4.5.0.pdf&#34; target=&#34;_blank&#34;&gt;OpenMP-4.5 Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://computing.llnl.gov/tutorials/openMP/&#34; target=&#34;_blank&#34;&gt;LLNL Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://press3.mcs.anl.gov//atpesc/files/2019/07/ATPESC_2019_Track-2_2_7-31_830am_Mattson-The-OpenMP_Common_Core.pdf&#34; target=&#34;_blank&#34;&gt;Mattson: The OpenMP Common Core&lt;/a&gt; from &lt;a href=&#34;https://extremecomputingtraining.anl.gov/&#34; target=&#34;_blank&#34;&gt;ATPESC&lt;/a&gt; (&lt;a href=&#34;https://www.youtube.com/watch?v=T0csnAirv-U&amp;amp;list=PLGj2a3KTwhRa6Ux64xg5L5ga6Jg8QykoQ&amp;amp;index=2&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;pragma-omp-parallel&#34;&gt;&lt;code&gt;#pragma omp parallel&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;The standard is &lt;strong&gt;big&lt;/strong&gt;, but most applications only use a few constructs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;omp-hello.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;omp.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;

int main() {
  #pragma omp parallel
  {
    int num_threads = omp_get_num_threads();
    int my_thread_num = omp_get_thread_num();
    printf(&amp;quot;I am %d of %d\n&amp;quot;, my_thread_num, num_threads);
  }
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!make CFLAGS=&#39;-fopenmp -Wall&#39; -B omp-hello
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cc -fopenmp -Wall    omp-hello.c   -o omp-hello
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!./omp-hello
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;I am 1 of 4
I am 2 of 4
I am 0 of 4
I am 3 of 4
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!OMP_NUM_THREADS=8 ./omp-hello
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;I am 0 of 8
I am 7 of 8
I am 1 of 8
I am 3 of 8
I am 4 of 8
I am 6 of 8
I am 2 of 8
I am 5 of 8
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;parallelizing-triad&#34;&gt;Parallelizing &lt;code&gt;triad&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void triad(int N, double *a, const double *b, double scalar, const double *c) {
#pragma omp parallel
    {
        for (int i=0; i&amp;lt;N; i++)
            a[i] = b[i] + scalar * c[i];
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What does this code do?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void triad(int N, double *a, const double *b, double scalar, const double *c) {
#pragma omp parallel
    {
        int id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();
        for (int i=id; i&amp;lt;N; i+=num_threads)
            a[i] = b[i] + scalar * c[i];
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;parallelizing-dot&#34;&gt;Parallelizing &lt;code&gt;dot&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;static double dot_ref(size_t n, const double *a, const double *b) {
  double sum = 0;
  for (size_t i=0; i&amp;lt;n; i++)
    sum += a[i] * b[i];
  return sum;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!make CFLAGS=&#39;-O3 -march=native -fopenmp&#39; -B dot
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;cc -O3 -march=native -fopenmp    dot.c   -o dot
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!OMP_NUM_THREADS=2 ./dot -r 10 -n 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  Name      flops   ticks   flops/tick
 dot_ref    20000   40327       0.50    
 dot_ref    20000   35717       0.56    
 dot_ref    20000   36096       0.55    
 dot_ref    20000   36487       0.55    
 dot_ref    20000   37157       0.54    
 dot_ref    20000   36024       0.56    
 dot_ref    20000   35322       0.57    
 dot_ref    20000   36601       0.55    
 dot_ref    20000   72193       0.28    
 dot_ref    20000   37924       0.53    
dot_opt1    20000   51256384        0.00    
dot_opt1    20000   23343145        0.00    
dot_opt1    20000   4646174     0.00    
dot_opt1    20000   16710       1.20    
dot_opt1    20000   15512       1.29    
dot_opt1    20000   16016       1.25    
dot_opt1    20000   16982       1.18    
dot_opt1    20000   452064      0.04    
dot_opt1    20000   16278       1.23    
dot_opt1    20000   16311       1.23    
dot_opt2    20000   24616       0.81    
dot_opt2    20000   16095       1.24    
dot_opt2    20000   17561       1.14    
dot_opt2    20000   16270       1.23    
dot_opt2    20000   18130       1.10    
dot_opt2    20000   16831       1.19    
dot_opt2    20000   16968       1.18    
dot_opt2    20000   16391       1.22    
dot_opt2    20000   17063       1.17    
dot_opt2    20000   16315       1.23    
dot_opt3    20000   77013       0.26    
dot_opt3    20000   12419       1.61    
dot_opt3    20000   12124       1.65    
dot_opt3    20000   12193       1.64    
dot_opt3    20000   12051       1.66    
dot_opt3    20000   12009       1.67    
dot_opt3    20000   11944       1.67    
dot_opt3    20000   12032       1.66    
dot_opt3    20000   12687       1.58    
dot_opt3    20000   12188       1.64    
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;vectorization&#34;&gt;Vectorization&lt;/h3&gt;

&lt;p&gt;OpenMP-4.0 added the &lt;code&gt;omp simd&lt;/code&gt; construct, which is a portable way to request that the compiler vectorize code.
An example of a reason why a compiler might fail to vectorize code is aliasing, which we investigate below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;triad.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdlib.h&amp;gt;

void triad(size_t N, double *a, const double *b, double scalar, const double *c) {
  for (size_t i=0; i&amp;lt;N; i++)
    a[i] = b[i] + scalar * c[i];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!gcc -O2 -ftree-vectorize -fopt-info-all -c triad.c
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Unit growth for small function inlining: 15-&amp;gt;15 (0%)

Inlined 0 calls, eliminated 0 functions

triad.c:4:3: optimized: loop vectorized using 16 byte vectors
triad.c:4:3: optimized:  loop versioned for vectorization because of possible aliasing
triad.c:3:6: note: vectorized 1 loops in function.
triad.c:4:3: optimized: loop turned into non-loop; it never loops
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;gcc autovectorization starts at &lt;code&gt;-O3&lt;/code&gt; or if you use &lt;code&gt;-ftree-vectorize&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;options such as &lt;a href=&#34;https://gcc.gnu.org/onlinedocs/gcc/Developer-Options.html#index-fopt-info&#34; target=&#34;_blank&#34;&gt;-fopt-info&lt;/a&gt; give useful diagnostics, but are compiler-dependent and sometimes referring to assembly is useful&lt;/li&gt;
&lt;li&gt;&lt;code&gt;man gcc&lt;/code&gt; with search (&lt;code&gt;/&lt;/code&gt;) is your friend&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-is-aliasing&#34;&gt;What is aliasing?&lt;/h3&gt;

&lt;p&gt;Is this valid code?  What xs &lt;code&gt;x&lt;/code&gt; after this call?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;double x[5] = {1, 2, 3, 4, 5};
triad(2, x+1, x, 10., x);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;C allows memory to overlap arbitrarily.  You can inform the compiler of this using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Restrict&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;restrict&lt;/code&gt; qualifier&lt;/a&gt; (C99/C11; &lt;code&gt;__restrict&lt;/code&gt; or &lt;code&gt;__restrict__&lt;/code&gt; work with most C++ and &lt;a href=&#34;https://devblogs.nvidia.com/cuda-pro-tip-optimize-pointer-aliasing/&#34; target=&#34;_blank&#34;&gt;CUDA&lt;/a&gt; compilers).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;triad-restrict.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void triad(int N, double *restrict a, const double *restrict b, double scalar, const double *restrict c) {
  for (int i=0; i&amp;lt;N; i++)
    a[i] = b[i] + scalar * c[i];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!gcc -O2 -march=native -ftree-vectorize -fopt-info-all -c triad-restrict.c
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Unit growth for small function inlining: 15-&amp;gt;15 (0%)

Inlined 0 calls, eliminated 0 functions

triad-restrict.c:2:5: optimized: loop vectorized using 32 byte vectors
triad-restrict.c:1:6: note: vectorized 1 loops in function.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice how there is no more &lt;code&gt;loop versioned for vectorization because of possible aliasing&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The complexity of checking for aliasing can grow combinatorially in the number of arrays being processed, leading to many loop variants and/or preventing vectorization.&lt;/p&gt;

&lt;h4 id=&#34;aside-warnings&#34;&gt;Aside: Warnings&lt;/h4&gt;

&lt;p&gt;The &lt;code&gt;-Wrestrict&lt;/code&gt; flag (included in &lt;code&gt;-Wall&lt;/code&gt;) can catch some programming errors&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void foo(double *x) {
  triad(2, x, x, 10, x);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!gcc -O2 -Wall -c triad-foo.c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The powers of &lt;code&gt;-Wrestrict&lt;/code&gt; are limited, however, and (as of gcc-9) do not even catch&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void foo(double *x) {
  triad(2, x+1, x, 10, x);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;check-the-assembly&#34;&gt;Check the assembly&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!objdump -d --prefix-addresses -M intel triad-restrict.o
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;triad-restrict.o:     file format elf64-x86-64


Disassembly of section .text:
0000000000000000 &amp;lt;triad&amp;gt; test   edi,edi
0000000000000002 &amp;lt;triad+0x2&amp;gt; jle    0000000000000067 &amp;lt;triad+0x67&amp;gt;
0000000000000004 &amp;lt;triad+0x4&amp;gt; lea    eax,[rdi-0x1]
0000000000000007 &amp;lt;triad+0x7&amp;gt; cmp    eax,0x2
000000000000000a &amp;lt;triad+0xa&amp;gt; jbe    0000000000000074 &amp;lt;triad+0x74&amp;gt;
000000000000000c &amp;lt;triad+0xc&amp;gt; mov    r8d,edi
000000000000000f &amp;lt;triad+0xf&amp;gt; shr    r8d,0x2
0000000000000013 &amp;lt;triad+0x13&amp;gt; vbroadcastsd ymm2,xmm0
0000000000000018 &amp;lt;triad+0x18&amp;gt; shl    r8,0x5
000000000000001c &amp;lt;triad+0x1c&amp;gt; xor    eax,eax
000000000000001e &amp;lt;triad+0x1e&amp;gt; xchg   ax,ax
0000000000000020 &amp;lt;triad+0x20&amp;gt; vmovupd ymm1,YMMWORD PTR [rcx+rax*1]
0000000000000025 &amp;lt;triad+0x25&amp;gt; vfmadd213pd ymm1,ymm2,YMMWORD PTR [rdx+rax*1]
000000000000002b &amp;lt;triad+0x2b&amp;gt; vmovupd YMMWORD PTR [rsi+rax*1],ymm1
0000000000000030 &amp;lt;triad+0x30&amp;gt; add    rax,0x20
0000000000000034 &amp;lt;triad+0x34&amp;gt; cmp    rax,r8
0000000000000037 &amp;lt;triad+0x37&amp;gt; jne    0000000000000020 &amp;lt;triad+0x20&amp;gt;
0000000000000039 &amp;lt;triad+0x39&amp;gt; mov    eax,edi
000000000000003b &amp;lt;triad+0x3b&amp;gt; and    eax,0xfffffffc
000000000000003e &amp;lt;triad+0x3e&amp;gt; test   dil,0x3
0000000000000042 &amp;lt;triad+0x42&amp;gt; je     0000000000000070 &amp;lt;triad+0x70&amp;gt;
0000000000000044 &amp;lt;triad+0x44&amp;gt; vzeroupper 
0000000000000047 &amp;lt;triad+0x47&amp;gt; cdqe   
0000000000000049 &amp;lt;triad+0x49&amp;gt; nop    DWORD PTR [rax+0x0]
0000000000000050 &amp;lt;triad+0x50&amp;gt; vmovsd xmm1,QWORD PTR [rcx+rax*8]
0000000000000055 &amp;lt;triad+0x55&amp;gt; vfmadd213sd xmm1,xmm0,QWORD PTR [rdx+rax*8]
000000000000005b &amp;lt;triad+0x5b&amp;gt; vmovsd QWORD PTR [rsi+rax*8],xmm1
0000000000000060 &amp;lt;triad+0x60&amp;gt; inc    rax
0000000000000063 &amp;lt;triad+0x63&amp;gt; cmp    edi,eax
0000000000000065 &amp;lt;triad+0x65&amp;gt; jg     0000000000000050 &amp;lt;triad+0x50&amp;gt;
0000000000000067 &amp;lt;triad+0x67&amp;gt; ret    
0000000000000068 &amp;lt;triad+0x68&amp;gt; nop    DWORD PTR [rax+rax*1+0x0]
0000000000000070 &amp;lt;triad+0x70&amp;gt; vzeroupper 
0000000000000073 &amp;lt;triad+0x73&amp;gt; ret    
0000000000000074 &amp;lt;triad+0x74&amp;gt; xor    eax,eax
0000000000000076 &amp;lt;triad+0x76&amp;gt; jmp    0000000000000047 &amp;lt;triad+0x47&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;How do the results change if you go up and replace &lt;code&gt;-march=native&lt;/code&gt; with &lt;code&gt;-march=skylake-avx512 -mprefer-vector-width=512&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;Is the assembly qualitatively different without &lt;code&gt;restrict&lt;/code&gt; (in which case the compiler &amp;ldquo;versions&amp;rdquo; the loop).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;pragma-omp-simd&#34;&gt;Pragma &lt;code&gt;omp simd&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;An alternative (or supplement) to &lt;code&gt;restrict&lt;/code&gt; is &lt;code&gt;#pragma omp simd&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;render_c(&#39;triad-omp-simd.c&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void triad(int N, double *a, const double *b, double scalar, const double *c) {
#pragma omp simd
  for (int i=0; i&amp;lt;N; i++)
    a[i] = b[i] + scalar * c[i];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!gcc -O2 -march=native -ftree-vectorize -fopenmp -fopt-info-all -c triad-omp-simd.c
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Unit growth for small function inlining: 15-&amp;gt;15 (0%)

Inlined 0 calls, eliminated 0 functions

triad-omp-simd.c:4:17: optimized: loop vectorized using 32 byte vectors
triad-omp-simd.c:1:6: note: vectorized 1 loops in function.
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Intro to Parallel Scaling</title>
      <link>https://cucs-hpsc.github.io/fall2019/intro-parallel-scaling/</link>
      <pubDate>Fri, 06 Sep 2019 08:07:31 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/intro-parallel-scaling/</guid>
      <description>

&lt;h2 id=&#34;programs-with-more-than-one-part&#34;&gt;Programs with more than one part&lt;/h2&gt;

&lt;p&gt;So far, we&amp;rsquo;ve focused on simple programs with only one part, but real programs have several different parts, often with data dependencies.
Some parts will be amenable to optimization and/or parallelism and others will not.
&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Optimizing-different-parts.svg/2880px-Optimizing-different-parts.svg.png&#34; alt=&#34;Diminishing returns&#34; /&gt;
This principle is called &lt;a href=&#34;https://en.wikipedia.org/wiki/Amdahl%27s_law&#34; target=&#34;_blank&#34;&gt;Amdahl&amp;rsquo;s Law&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def exec_time(f, p, n=10, latency=1):
    # Suppose that a fraction f of the total work is amenable to optimization
    # We run a problem size n with parallelization factor p
    return latency + (1-f)*n + f*n/p
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
import matplotlib.pyplot as plt
import pandas
import numpy as np
plt.style.use(&#39;seaborn&#39;)

ps = np.geomspace(1, 1000)

plt.loglog(ps, exec_time(.99, ps, latency=0))
plt.loglog(ps, exec_time(1, ps, latency=0))
plt.title(&#39;Strong scaling&#39;)
plt.xlabel(&#39;p&#39;)
plt.ylabel(&#39;time&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_2_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;strong-scaling-fixed-total-problem-size&#34;&gt;Strong scaling: fixed total problem size&lt;/h2&gt;

&lt;h3 id=&#34;cost-time-p&#34;&gt;Cost = &lt;code&gt;time * p&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def exec_cost(f, p, **kwargs):
    return exec_time(f, p, **kwargs) * p

plt.loglog(ps, exec_cost(.99, ps))
plt.title(&#39;Strong scaling&#39;)
plt.xlabel(&#39;p&#39;)
plt.ylabel(&#39;cost&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_4_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;efficiency&#34;&gt;Efficiency&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.semilogx(ps, 1/exec_cost(.99, ps, latency=1))
plt.title(&#39;Strong scaling&#39;)
plt.xlabel(&#39;p&#39;)
plt.ylabel(&#39;efficiency&#39;)
plt.ylim(bottom=0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_6_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;speedup&#34;&gt;Speedup&lt;/h3&gt;

&lt;p&gt;$$ S(p) = \frac{T(1)}{T(p)} $$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(ps, exec_time(.99, 1, latency=1) / exec_time(.99, ps, latency=1))
plt.title(&#39;Strong scaling&#39;)
plt.xlabel(&#39;p&#39;)
plt.ylabel(&#39;speedup&#39;)
plt.ylim(bottom=0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_8_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;stunt-1-report-speedup-not-absolute-performance&#34;&gt;Stunt 1: Report speedup, not absolute performance!&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;stunt1.jpg&#34; alt=&#34;Hager&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;efficiency-time-spectrum-my-preference&#34;&gt;Efficiency-Time spectrum (my preference)&lt;/h2&gt;

&lt;p&gt;People care about two observable properties
* &lt;strong&gt;Time&lt;/strong&gt; until job completes
* &lt;strong&gt;Cost&lt;/strong&gt; in core-hours or dollars to do job&lt;/p&gt;

&lt;p&gt;Most HPC applications have access to large machines, so don&amp;rsquo;t really care how many processes they use for any given job.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(exec_time(.99, ps), 1/exec_cost(.99, ps), &#39;o-&#39;)
plt.title(&#39;Strong scaling&#39;)
plt.xlabel(&#39;time&#39;)
plt.ylabel(&#39;efficiency&#39;)
plt.ylim(bottom=0);
plt.xlim(left=0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_11_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;principles&#34;&gt;Principles&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blogs.fau.de/hager/archives/5835&#34; target=&#34;_blank&#34;&gt;No &amp;ldquo;soft&amp;rdquo; &lt;code&gt;log&lt;/code&gt; scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Both axes have tangible units&lt;/li&gt;
&lt;li&gt;Bigger is better on the $y$ axis&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;weak-scaling-fixed-work-per-processor&#34;&gt;Weak Scaling: Fixed work per processor&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve kept the problem size $n$ fixed thus far, but parallel computers are also used to solve large problems.  If we keep the amount of work per processor fixed, we are &lt;a href=&#34;https://en.wikipedia.org/wiki/Gustafson&#39;s_law&#34; target=&#34;_blank&#34;&gt;weak/Gustafson scaling&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ns = 10*ps
plt.semilogx(ps, ns/exec_cost(.99, ps, n=ns, latency=1), &#39;o-&#39;)
ns = 100*ps
plt.semilogx(ps, ns/exec_cost(.99, ps, n=ns, latency=1), &#39;s-&#39;)
plt.title(&#39;Weak scaling&#39;)
plt.xlabel(&#39;procs&#39;)
plt.ylabel(&#39;efficiency&#39;)
plt.ylim(bottom=0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_14_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for w in np.geomspace(0.1, 1e3, 20):
    ns = w*ps
    plt.semilogx(exec_time(.99, ps, n=ns, latency=1),
                 ns/exec_cost(.99, ps, n=ns, latency=1), &#39;o-&#39;)
plt.title(&#39;Weak scaling&#39;)
plt.xlabel(&#39;time&#39;)
plt.ylabel(&#39;efficiency&#39;)
plt.ylim(bottom=0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_15_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;fuhrer-et-al-2018-near-global-climate-simulation-at-1-km-resolution-https-www-geosci-model-dev-net-11-1665-2018-gmd-11-1665-2018-pdf&#34;&gt;&lt;a href=&#34;https://www.geosci-model-dev.net/11/1665/2018/gmd-11-1665-2018.pdf&#34; target=&#34;_blank&#34;&gt;Fuhrer et al (2018): Near-global climate simulation at 1 km resolution&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;fuhrer2018-fig4.png&#34; alt=&#34;Fuhrer (2018)&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I replotted these data for &lt;a href=&#34;https://jedbrown.org/files/20190822-Latsis.pdf&#34; target=&#34;_blank&#34;&gt;my talk&lt;/a&gt; at the &lt;a href=&#34;https://latsis2019.ethz.ch/&#34; target=&#34;_blank&#34;&gt;Latsis Symposium&lt;/a&gt; last month.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;fuhrer2018-scaling-time-ann4.png&#34; alt=&#34;Fuhrer (2018) replotted&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;further-resources&#34;&gt;Further resources&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blogs.fau.de/hager/archives/5260&#34; target=&#34;_blank&#34;&gt;Hager: Fooling the masses&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Learn by counter-examples&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://htor.inf.ethz.ch/publications/index.php?pub=222&#34; target=&#34;_blank&#34;&gt;Hoefler and Belli: Scientific Benchmarking of Parallel Computing Systems&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Recommended best practices, especially for dealing with performance variability&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please read/watch something from this list and be prepared to share on Monday.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Performance Modeling</title>
      <link>https://cucs-hpsc.github.io/fall2019/intro-modeling/</link>
      <pubDate>Wed, 04 Sep 2019 09:17:28 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/intro-modeling/</guid>
      <description>

&lt;h2 id=&#34;why-model-performance&#34;&gt;Why model performance?&lt;/h2&gt;

&lt;p&gt;Models give is a conceptual and roughly quantitative framework by which to answer the following types of questions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Why is an implementation exhibiting its observed performance?&lt;/li&gt;
&lt;li&gt;How will performance change if we:

&lt;ul&gt;
&lt;li&gt;optimize this component?&lt;/li&gt;
&lt;li&gt;buy new hardware? (Which new hardware?)&lt;/li&gt;
&lt;li&gt;run a different configuration?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;While conceptualizing a new algorithm, what performance can we expect and what will be bottlenecks?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Models are a guide for performance, but not an absolute.&lt;/p&gt;

&lt;h3 id=&#34;terms&#34;&gt;Terms&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Symbol&lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$n$&lt;/td&gt;
&lt;td&gt;Input parameter related to problem size&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$W$&lt;/td&gt;
&lt;td&gt;Amount of work to solve problem $n$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$T$&lt;/td&gt;
&lt;td&gt;Execution time&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$R$&lt;/td&gt;
&lt;td&gt;Rate at which work is done&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;stream-triad&#34;&gt;STREAM Triad&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;for (i=0; i&amp;lt;n; i++)
    a[i] = b[i] + scalar*c[i];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$n$ is the array size and
$$W = 3 \cdot \texttt{sizeof(double)} \cdot n$$
is the number of bytes transferred.  The rate $R = W/T$ is measured in bytes per second (or MB/s, etc.).&lt;/p&gt;

&lt;h4 id=&#34;dense-matrix-multiplication&#34;&gt;Dense matrix multiplication&lt;/h4&gt;

&lt;p&gt;To perform the operation $C \gets C + A B$ where $A,B,C$ are $n\times n$ matrices.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;for (i=0; i&amp;lt;n; i++)
    for (j=0; j&amp;lt;n; j++)
        for (k=0; k&amp;lt;n; k++)
            c[i*n+j] += a[i*n+k] * b[k*n+j];
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Can you identify two expressions for the total amount of work $W(n)$ and the associated units?&lt;/li&gt;
&lt;li&gt;Can you think of a context in which one is better than the other and vice-versa?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;estimating-time&#34;&gt;Estimating time&lt;/h3&gt;

&lt;p&gt;To estimate time, we need to know how fast hardware executes flops and moves bytes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
import matplotlib.pyplot as plt
import pandas
import numpy as np
plt.style.use(&#39;seaborn&#39;)

hardware = pandas.read_csv(&#39;data-intel.csv&#39;, index_col=&amp;quot;Name&amp;quot;)
hardware
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;GFLOPs-SP&lt;/th&gt;
      &lt;th&gt;GFLOPs-DP&lt;/th&gt;
      &lt;th&gt;Cores&lt;/th&gt;
      &lt;th&gt;Mem-GBps&lt;/th&gt;
      &lt;th&gt;TDP&lt;/th&gt;
      &lt;th&gt;Freq(MHz)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon X5482&lt;/th&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3200&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon X5492&lt;/th&gt;
      &lt;td&gt;2008&lt;/td&gt;
      &lt;td&gt;108&lt;/td&gt;
      &lt;td&gt;54&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon W5590&lt;/th&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;106&lt;/td&gt;
      &lt;td&gt;53&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon X5680&lt;/th&gt;
      &lt;td&gt;2010&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon X5690&lt;/th&gt;
      &lt;td&gt;2011&lt;/td&gt;
      &lt;td&gt;166&lt;/td&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;3470&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon E5-2690&lt;/th&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;372&lt;/td&gt;
      &lt;td&gt;186&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;135&lt;/td&gt;
      &lt;td&gt;2900&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon E5-2697 v2&lt;/th&gt;
      &lt;td&gt;2013&lt;/td&gt;
      &lt;td&gt;518&lt;/td&gt;
      &lt;td&gt;259&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;2700&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon E5-2699 v3&lt;/th&gt;
      &lt;td&gt;2014&lt;/td&gt;
      &lt;td&gt;1324&lt;/td&gt;
      &lt;td&gt;662&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;145&lt;/td&gt;
      &lt;td&gt;2300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon E5-2699 v3&lt;/th&gt;
      &lt;td&gt;2015&lt;/td&gt;
      &lt;td&gt;1324&lt;/td&gt;
      &lt;td&gt;662&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
      &lt;td&gt;145&lt;/td&gt;
      &lt;td&gt;2300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon E5-2699 v4&lt;/th&gt;
      &lt;td&gt;2016&lt;/td&gt;
      &lt;td&gt;1548&lt;/td&gt;
      &lt;td&gt;774&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;77&lt;/td&gt;
      &lt;td&gt;145&lt;/td&gt;
      &lt;td&gt;2200&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon Platinum 8180&lt;/th&gt;
      &lt;td&gt;2017&lt;/td&gt;
      &lt;td&gt;4480&lt;/td&gt;
      &lt;td&gt;2240&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;205&lt;/td&gt;
      &lt;td&gt;2500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Xeon Platinum 9282&lt;/th&gt;
      &lt;td&gt;2018&lt;/td&gt;
      &lt;td&gt;9320&lt;/td&gt;
      &lt;td&gt;4660&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt;175&lt;/td&gt;
      &lt;td&gt;400&lt;/td&gt;
      &lt;td&gt;2600&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig = hardware.plot(x=&#39;GFLOPs-DP&#39;, y=&#39;Mem-GBps&#39;, marker=&#39;o&#39;)
fig.set_xlim(left=0)
fig.set_ylim(bottom=0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_2_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So we have rates $R_f = 4660 \cdot 10^9$ flops/second and $R_m = 175 \cdot 10^9$ bytes/second.  Now we need to characterize some algorithms.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;algs = pandas.read_csv(&#39;algs.csv&#39;, index_col=&#39;Name&#39;)
algs[&#39;intensity&#39;] = algs[&#39;flops&#39;] / algs[&#39;bytes&#39;]
algs = algs.sort_values(&#39;intensity&#39;)
algs
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bytes&lt;/th&gt;
      &lt;th&gt;flops&lt;/th&gt;
      &lt;th&gt;intensity&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Triad&lt;/th&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.083333&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;SpMV&lt;/th&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.166667&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stencil27-cache&lt;/th&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;54&lt;/td&gt;
      &lt;td&gt;2.250000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MatFree-FEM&lt;/th&gt;
      &lt;td&gt;2376&lt;/td&gt;
      &lt;td&gt;15228&lt;/td&gt;
      &lt;td&gt;6.409091&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Stencil27-ideal&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;54&lt;/td&gt;
      &lt;td&gt;6.750000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def exec_time(machine, alg, n):
    bytes = n * alg.bytes
    flops = n * alg.flops
    T_mem = bytes / (machine[&#39;Mem-GBps&#39;] * 1e9)
    T_flops = flops / (machine[&#39;GFLOPs-DP&#39;] * 1e9)
    return max(T_mem, T_flops)
    
exec_time(hardware.loc[&#39;Xeon Platinum 9282&#39;], algs.loc[&#39;SpMV&#39;], 1e8)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.006857142857142857
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for _, machine in hardware.iterrows():
    for _, alg in algs.iterrows():
        ns = np.geomspace(1e4, 1e9, 10)
        times = np.array([exec_time(machine, alg, n) for n in ns])
        flops = np.array([alg.flops * n for n in ns])
        rates = flops/times
        plt.loglog(ns, rates, &#39;o-&#39;)
plt.xlabel(&#39;n&#39;)
plt.ylabel(&#39;rate&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_6_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It looks like performance does not depend on problem size.&lt;/p&gt;

&lt;p&gt;Well, yeah, we chose a model in which flops and bytes were both proportional to $n$, and our machine model has no sense of cache hierarchy or latency, so time is also proportional to $n$.  We can divide through by $n$ and yield a more illuminating plot.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for _, machine in hardware.iterrows():
    times = np.array([exec_time(machine, alg, 1) 
                      for _, alg in algs.iterrows()])
    rates = algs.flops/times
    intensities = algs.intensity
    plt.loglog(intensities, rates, &#39;o-&#39;, label=machine.name)
plt.xlabel(&#39;intensity&#39;)
plt.ylabel(&#39;rate&#39;)
plt.legend();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./lecture_8_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re seeing the &amp;ldquo;roofline&amp;rdquo; for the older processors while the newer models are memory bandwidth limited for all of these algorithms.&lt;/p&gt;

&lt;h3 id=&#34;recommended-reading-on-single-node-performance-modeling&#34;&gt;Recommended reading on single-node performance modeling&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://doi.org/10.1145/1498765.1498785&#34; target=&#34;_blank&#34;&gt;Williams, Waterman, Patterson (2009): &lt;strong&gt;Roofline: An insightful visual performance model for multicore architectures&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Vectorization and Instruction-Level Parallelism</title>
      <link>https://cucs-hpsc.github.io/fall2019/intro-vectorization/</link>
      <pubDate>Fri, 30 Aug 2019 11:00:00 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/intro-vectorization/</guid>
      <description>

&lt;p&gt;Remember how single-thread performance has increased significantly
since ~2004 when clock frequency stagnated?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.karlrupp.net/wp-content/uploads/2018/02/42-years-processor-trend.png&#34; alt=&#34;42 years of microprocessor data&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is a result of doing more per clock cycle.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.karlrupp.net/wp-content/uploads/2013/06/flops-per-cycle-sp.png&#34; alt=&#34;Flops per clock cycle&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s visit some slides:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://moodle.rrze.uni-erlangen.de/pluginfile.php/12916/mod_resource/content/6/01_IntroArchitecture.pdf&#34; target=&#34;_blank&#34;&gt;Georg Hager (2019): Modern Computer Architucture&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;further-resources&#34;&gt;Further resources&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://software.intel.com/sites/landingpage/IntrinsicsGuide/#&#34; target=&#34;_blank&#34;&gt;Intel Intrinsics Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wikichip

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikichip.org/wiki/intel/microarchitectures/cascade_lake&#34; target=&#34;_blank&#34;&gt;Intel Xeon: Cascade Lake&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikichip.org/wiki/amd/cores/rome&#34; target=&#34;_blank&#34;&gt;AMD EPYC gen2: Rome&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikichip.org/wiki/ibm/microarchitectures/power9&#34; target=&#34;_blank&#34;&gt;IBM POWER9&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.agner.org/optimize/&#34; target=&#34;_blank&#34;&gt;Agner Fog&amp;rsquo;s website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Intro to Architecture</title>
      <link>https://cucs-hpsc.github.io/fall2019/intro-architecture/</link>
      <pubDate>Wed, 28 Aug 2019 08:10:18 -0600</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/intro-architecture/</guid>
      <description>

&lt;h1 id=&#34;cores-caches-and-memory&#34;&gt;Cores, caches, and memory&lt;/h1&gt;

&lt;h3 id=&#34;a-von-neumann-architecture-https-en-wikipedia-org-wiki-von-neumann-architecture&#34;&gt;A &lt;a href=&#34;https://en.wikipedia.org/wiki/Von_Neumann_architecture&#34; target=&#34;_blank&#34;&gt;von Neumann Architecture&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/Von_Neumann_Architecture.svg/2880px-Von_Neumann_Architecture.svg.png&#34; alt=&#34;von Neumann architecture&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;a-contemporary-architecture&#34;&gt;A contemporary architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://static.duartes.org/img/blogPosts/physicalMemoryAccess.png&#34; alt=&#34;Core 2&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;my-laptop&#34;&gt;My laptop&lt;/h3&gt;

&lt;p&gt;We can get this kind of information for our machine using &lt;a href=&#34;https://www.open-mpi.org/projects/hwloc/&#34; target=&#34;_blank&#34;&gt;hwloc&lt;/a&gt;, which provides a library as well as the command-line tool &lt;code&gt;lstopo&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!lstopo --output-format svg &amp;gt; lstopo-local.svg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;lstopo-local.svg&#34; alt=&#34;lstopo&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;a-double-socket-compute-node-with-two-gpus&#34;&gt;A double-socket compute node with two GPUs&lt;/h3&gt;

&lt;p&gt;2x Xeon Ivy-Bridge-EP &lt;a href=&#34;https://ark.intel.com/content/www/us/en/ark/products/75277/intel-xeon-processor-e5-2680-v2-25m-cache-2-80-ghz.html&#34; target=&#34;_blank&#34;&gt;E5-2680v2&lt;/a&gt; + 2x NVIDIA GPUs (from 2013, with hwloc v1.11).
GPUs are reported as CUDA devices and X11 display :1.0: (from the &lt;a href=&#34;https://www-lb.open-mpi.org/projects/hwloc/lstopo/&#34; target=&#34;_blank&#34;&gt;hwloc gallery&lt;/a&gt;)
&lt;img src=&#34;https://www-lb.open-mpi.org/projects/hwloc/lstopo/images/2XeonE5v2+2cuda+1display_v1.11.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;block-diagrams&#34;&gt;Block diagrams&lt;/h3&gt;

&lt;p&gt;A block diagram from a vendor can include additional information about how cores are physically connected.&lt;/p&gt;

&lt;h4 id=&#34;ring-bus-xeon-e5-2600-family&#34;&gt;Ring bus (Xeon E5-2600 family)&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://software.intel.com/sites/default/files/managed/e3/a4/xeon-processor-scalable-family-tech-overview-fig04.png&#34; alt=&#34;Intel Xeon E5-2600&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;mesh-bus-xeon-scalable-family&#34;&gt;Mesh bus (Xeon Scalable family)&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://software.intel.com/sites/default/files/managed/5a/03/xeon-processor-scalable-family-tech-overview-fig05.png&#34; alt=&#34;Intel Xeon Scalable&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;multi-socket-configurations&#34;&gt;Multi-socket configurations&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://software.intel.com/sites/default/files/managed/77/f2/xeon-processor-scalable-family-tech-overview-fig07.png&#34; alt=&#34;4-socket ring&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview&#34; target=&#34;_blank&#34;&gt;https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;multiple-nodes-go-into-racks-or-cabinets&#34;&gt;Multiple nodes go into &lt;strong&gt;racks&lt;/strong&gt; or &lt;strong&gt;cabinets&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;BlueGenePRacks.png&#34; alt=&#34;Blue Gene/P Racks&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.olcf.ornl.gov/wp-content/uploads/2018/06/summit-1.jpg&#34; alt=&#34;OLCF Summit&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;terminology&#34;&gt;Terminology&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Core&lt;/strong&gt; (virtual and physical): has a single program counter (logically sequential processing of instructions)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory channel&lt;/strong&gt;: e.g., DDR4-2400: transfers 64 bits (8 bytes) at a rate of 2400 MHz = 15.36 GB/s&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Socket&lt;/strong&gt; or &lt;strong&gt;CPU&lt;/strong&gt;: contains multiple cores in a single piece* of silicon&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-Uniform Memory Access (NUMA)&lt;/strong&gt;: different channels may be different distances from a core&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compute node&lt;/strong&gt;: one or more sockets, plus memory, network card, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-expensive-is-it-to-access-memory&#34;&gt;How expensive is it to access memory?&lt;/h3&gt;

&lt;p&gt;What does that mean?  How would we measure?&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.rdrop.com/~paulmck/RCU/RCU.2013.01.22d.PLMW.pdf&#34; target=&#34;_blank&#34;&gt;McKenney (2013): Laws of Physics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html&#34; target=&#34;_blank&#34;&gt;Interactive&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.anandtech.com/show/14694/amd-rome-epyc-2nd-gen/7&#34; target=&#34;_blank&#34;&gt;Variation by vendor&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;how-your-program-accesses-memory&#34;&gt;How your program accesses memory&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;double a[1000];

void foo() {
    for (int i=0; i&amp;lt;1000; i++)
        a[i] = 1.234 * i;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The compiler turns the loop body into instructions, which we can examine using &lt;a href=&#34;https://gcc.godbolt.org/z/gbhuZR&#34; target=&#34;_blank&#34;&gt;Godbolt&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pxor xmm0, xmm0                  ; zero the xmm0 register
cvtsi2sd xmm0, eax               ; convert the integer i to double
mulsd xmm0, xmm1                 ; multiply by 1.234 (held in xmm1)
movsd QWORD PTR a[0+rax*8], xmm0 ; store to memory address a[i]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Only one instruction here accesses memory, and the performance will be affected greatly by where that memory resides (which level of cache, where in DRAM).&lt;/p&gt;

&lt;p&gt;Most architectures today have &lt;strong&gt;64-byte cache lines&lt;/strong&gt;: all transfers from main memory (DRAM) to and from cache operate in units of 64 bytes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://static.duartes.org/img/blogPosts/L1CacheExample.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;let-s-compare-three-code-samples&#34;&gt;Let&amp;rsquo;s compare three code samples&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;for (int i=0; i&amp;lt;N; i++)
    a[i] = b[i];
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;for (int i=0; i&amp;lt;N; i++)
    a[i] = b[(i*8) % N];
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;for (int i=0; i&amp;lt;N; i++)
    a[i] = b[random() % N];
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;what-happens-when-you-request-a-cache-line&#34;&gt;What happens when you request a cache line?&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://static.duartes.org/img/blogPosts/memoryRead.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;operating-system-effects&#34;&gt;Operating system effects&lt;/h2&gt;

&lt;p&gt;Most systems today use virtual addressing, so every address in your program needs to be translated to a physical address before looking for it (in cache or memory).  Fortunately, there is hardware to assist with this: the Translation Lookaside Buffer (TLB).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://drawings.jvns.ca/drawings/pagetable.svg&#34; alt=&#34;Virtual memory and the page table&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;further-resources&#34;&gt;Further resources&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jvns.ca/blog/2016/12/03/how-much-memory-is-my-process-using-/&#34; target=&#34;_blank&#34;&gt;Julia Evans (2016): How much memory is my process using?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://manybutfinite.com/post/intel-cpu-caches/&#34; target=&#34;_blank&#34;&gt;Gustavo Duarte (2009): Cache: a place for concealment and safekeeping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://manybutfinite.com/post/getting-physical-with-memory/&#34; target=&#34;_blank&#34;&gt;Gustavo Duarte (2009): Getting Physical With Memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.akkadia.org/drepper/cpumemory.pdf&#34; target=&#34;_blank&#34;&gt;Ulrich Drepper (2007): What Every Programmer Should Know About Memory&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Trends</title>
      <link>https://cucs-hpsc.github.io/fall2019/trends/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://cucs-hpsc.github.io/fall2019/trends/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/&#34; target=&#34;_blank&#34;&gt;https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.karlrupp.net/2013/06/cpu-gpu-and-mic-hardware-characteristics-over-time/&#34; target=&#34;_blank&#34;&gt;https://www.karlrupp.net/2013/06/cpu-gpu-and-mic-hardware-characteristics-over-time/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Fischer2015-Latency.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
