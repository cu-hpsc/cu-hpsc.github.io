[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1566849863,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://cucs-hpsc.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"","tags":null,"title":"Jed Brown","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1566796546,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://cucs-hpsc.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" Where and When  CSCI 5576 / 4576: High Performance Scientific Computing Lectures: Mon/Wed/Fri 11-11:50 in ECCR 150 Labs: Fri 9-10:40 in KOBL 300 Website: https://cucs-hpsc.github.io/fall2019 Issues/Questions/etc.: Browse or Create  Zoom  Meeting ID: 214-104-523 Join via web browser: https://cuboulder.zoom.us/j/214104523 Join via Zoom app (using meeting ID) Join via One tap mobile: +16699006833,,214104523# or +16465588656,,214104523# Join via telephone: 1-669-900-6833 or 1-646-558-8656  Calendar  Instructor: Jed Brown  GitHub: @jedbrown Office hours: See calendar in ECOT 824 (usually Tue 14:30-15:30 and Thu 9:00-10:00)  Teaching Assistant: Camden Elliott-Williams  GitHub: @CamdenCU Office hours: Wed 9:30-10:30 and 13:30-14:30 or by appointment in ECCR 1B \u0026ldquo;Systems Lab\u0026rdquo; (see map)  Homework For each assignment, click the link below to accept via GitHub Classroom. This creates a private repository for you to work in. Then git clone the repository to whatever machine you\u0026rsquo;ll work on and follow instructions in the README. Usually you will be asked to read and edit code, run a range of experiments, and interpret/plot data in a Report.ipynb.\n   Assigned Due Description     2019-09-06 2019-09-16 (part by 2019-09-13) Experiments in vectorization   2019-09-20 2019-09-30 Sorting    Videos Videos appear automatically on Canvas and linked below.\n   Date Topic     Aug 26 Course introduction and preview of architectural trends   Aug 28 Intro to architecture   Aug 30 Intro to vectorization and ILP   Sep 4 Intro to performance modeling (roofline)   Sep 6 Intro to parallel scaling   Sep 9 Joel Frahm on CU Research Computing (slides)   Sep 11 OpenMP Basics   Sep 13 OpenMP memory semantics, synchronization, and perf demo   Sep 16 OpenMP tasking and computational depth/critical path   Sep 18 Low-level optimization, parallel reductions and scans   Sep 20 Searching and sorting methods (based on parts of slides and slides)   Sep 23 Bitonic sort recap/demo; intro to graph independence   Sep 25 Recorded lecture: Introduction to MPI   Sep 27 Library interfaces with MPI: Conway\u0026rsquo;s Game of Life   Sep 30 Dense linear algebra and networks   Oct 2 Dense linear algebra and orthogonality   Oct 4 Orthogonality and conditioning   Oct 7 Parallel QR and Elemental for distributed memory   Oct 9 Sparse and iterative linear algebra   Oct 11 Intro to preconditioning   Oct 14 Intro to domain decomposition preconditioning   Oct 16 Domain decomposition preconditioning and scaling   Oct 18 Multilevel preconditioning and predictive modeling   Oct 21 Nonlinear solvers   Oct 23 Transient problems   Oct 25 libCEED: spectral elements and matrix-free methods   Oct 28 Coprocessor architectures   Oct 30 CUDA (by Camden Elliott-Williams)   Nov 1 Practical CUDA   Nov 4 ISPC, OpenMP target, OpenACC   Nov 6 HPC I/O   Nov 8 MPI-IO    ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1573242553,"objectID":"51928ad4c101e2a8b04b4a7a8650b816","permalink":"https://cucs-hpsc.github.io/fall2019/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/fall2019/","section":"fall2019","summary":"CSCI 5576/4576: Fall 2019: High Performance Scientific Computing","tags":null,"title":"Logistics","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://cucs-hpsc.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Overview This course will develop the skills necessary to reason about performance of applications and modern architectures, to identify opportunities and side-effects of changes, to develop high-performance software, to transfer algorithmic patterns and lessons learned from different domains, and to communicate such analyses with diverse stakeholders. These skills are important for research and development of numerical methods and performance-sensitive science and engineering applications, obtaining allocations via NSF\u0026rsquo;s XSEDE and DOE ASCR facilities, as well as in jobs affiliated with computing facilities at national labs, industry, and academia.\nWe will introduce widely-used parallel programming models such as OpenMP, MPI, and CUDA, as well as ubiquitous parallel libraries, but the purpose of the course is not to teach interfaces, but to develop skills that will be durable and transferrable.\nPreparation This course does not assume prior experience with parallel programming. It will use Linux command-line tools, and some activities will involve batch computing environments (SLURM). Most exercises will use the C programming language, though you can use any appropriate language for projects. Some of the exercises will involve techniques from numerical computing (e.g., CSCI-3656). I will do my best to avoid assuming prior knowledge of these topics, and to provide resources for you to learn or refresh your memory as we use them.\nEveryone here is capable of succeeding in the course, but the effort level will be higher if most of the topics above are new to you. Regardless of your preparation, it is normal to feel lost sometimes. A big part of pragmatic HPC is learning to efficiently answer your questions through documentation, online resources, and even consulting the code or running experiments. (Most of our software stack is open source.) That said, it\u0026rsquo;s easy to lose lots of time in a rabbit hole. My hope is that you will have the courage to dive into that rabbit hole occasionally, but also to ask questions when stuck and to budget your time for such excursions so that you can complete assignments on-time without compromising your work/life balance.\nApproximate timeline    Week Topics     Aug 26 Introduction and modern computer architecture (vectorization and memory hierarchy)   Sep 2 Performance modeling, analysis, and scaling; profiling   Sep 9 Intro to OpenMP and non-numerical algorithms (sorting and searching)   Sep 16 Parallel algorithmic patterns   Sep 23 Dense linear algebra   Sep 30 Intro to MPI and distributed memory parallelism   Oct 7 Sparse and iterative linear algebra   Oct 14 Domain decomposition   Oct 21 Graph algorithms   Oct 28 GPU programming via OpenMP-5 and CUDA   Nov 4 Parallel file systems and IO   Nov 11 Data analysis/machine learning algorithms and dynamic cloud environments   Nov 18 Particles and N-body systems   Nov 25 Fall Break   Dec 2 Multigrid, FFT, and FMM   Dec 9 Special topics    Evaluation    Activity Percentage     Participation 10%   Labs and homework assignments 40%   Community contribution 15%   Community analysis 15%   Final project (written + presentation) 20%    Git and GitHub Homework assignments and in-class activities will be submitted via Git. This class will use GitHub classroom. Homeworks will be completed by cloning GitHub repositories, completing coding and analysis activities, and pushing completed assignments back to GitHub.\nAssignments may be completed using Coding CSEL Hub and/or RMACC Summit (request an account). Assignments will typically have written analysis, for which I recommend Jupyter.\nIt is notoriously difficult to predict the time required to develop quality code and understand its performance, so please start early to give yourself plenty of time. You are welcome to work together on all assignments, but must acknowledge collaborators. You should ensure that your written work is entirely your own.\nCommunity contributions and analysis Over the course of the semester, you will follow the development activities of an active open source project of your choosing. This should be a project with an active developer community from multiple institutions that discuss their rationale in public, such as a mailing list and/or GitHub/GitLab issues and pull requests. You will write and present about the performance and capability needs of key stakeholders, the way project resources are allocated, their metrics for success, and any notable achievements made over the course of the semester.\nYou will also make a contribution to be merged by the project. Adding new examples and/or improving documentation are extremely valuable contributions, but you may also add features or improve implementations. Please respect the time of project maintainers and reviewers by learning about the project and its expectations and process, communicating in advance if appropriate, and leaving plenty of time for multiple rounds of review and revision.\nDistance sections and labs The lectures for this class can be joined synchronously via Zoom (see instructions); they are also recorded and will be posted here (and automatically on Canvas). Some labs will be activities that can be completed within the time period (with group discussion and compare/contrast) while others will be a jump start for homeworks. I envision that distance students will form groups and set a time for virtual discussion in lieu of synchronous discussion during the lab period. In both settings, there will be a peer evaluation component during which each participant credits one or more peers with some specific contributions to the conversation.\nMoodle Moodle will be used to maintain grades. Please enroll yourself at https://moodle.cs.colorado.edu.\nResources This course will use a variety of online resources and papers. There is no required textbook, but the following resources may be helpful.\n Hager and Wellein (2010), Introduction to High Performance Computing for Scientists and Engineers van de Geijn, Myers, Parikh (2019): LAFF on Programming for High Performance (free online) Eijkhout, Chow, van de Geijn (2017), Introduction to High-Performance Scientific Computing (free PDF) Grama, Gupta, Karypis, Kumar (2003), Introduction to Parallel Computing  Additional resources  Greenbaum and Chartier (2012), Numerical Methods Design, Analysis, and Computer Implementation of Algorithms \u0026ndash; an excellent, comprehensive book. Boyd and Vandenberghe (2018), Introduction to Applied Linear Algebra \u0026ndash; practical introduction to linear algebra for computer scientists; free PDF Trefethen and Bau (1997), Numerical Linear Algebra \u0026ndash; fantastic, but limited to numerical linear algebra and covers more advanced topics. Scopatz and Huff (2015), Effective Computation in Physics \u0026ndash; Python language, data science workflow, and computation.  A SIAM Membership is free for CU students and provides a 30% discount on SIAM books.\nDisability Accommodations If you qualify for accommodations because of a disability, please submit to your professor a letter from Disability Services in a timely manner (for exam accommodations provide your letter at least one week prior to the exam) so that your needs can be addressed. Disability Services determines accommodations based on documented disabilities. Contact Disability Services at 303-492-8671 or by e-mail at dsinfo@colorado.edu. If you have a temporary medical condition or injury, see the Temporary Injuries guidelines under the Quick Links at the Disability Services website and discuss your needs with your professor.\nReligious Observances Campus policy regarding religious observances requires that faculty make every effort to deal reasonably and fairly with all students who, because of religious obligations, have conflicts with scheduled exams, assignments or required assignments/attendance. If this applies to you, please speak with me directly as soon as possible at the beginning of the term. See the campus policy regarding religious observances for full details.\nClassroom Behavior Students and faculty each have responsibility for maintaining an appropriate learning environment. Those who fail to adhere to such behavioral standards may be subject to discipline. Professional courtesy and sensitivity are especially important with respect to individuals and topics dealing with differences of race, color, culture, religion, creed, politics, veteran\u0026rsquo;s status, sexual orientation, gender, gender identity and gender expression, age, disability,and nationalities. Class rosters are provided to the instructor with the student\u0026rsquo;s legal name. I will gladly honor your request to address you by an alternate name or gender pronoun. Please advise me of this preference early in the semester so that I may make appropriate changes to my records. For more information, see the policies on classroom behavior and the student code.\nDiscrimination and Harassment The University of Colorado Boulder (CU Boulder) is committed to maintaining a positive learning, working, and living environment. CU Boulder will not tolerate acts of sexual misconduct, discrimination, harassment or related retaliation against or by any employee or student. CU\u0026rsquo;s Sexual Misconduct Policy prohibits sexual assault, sexual exploitation, sexual harassment,intimate partner abuse (dating or domestic violence), stalking or related retaliation. CU Boulder\u0026rsquo;s Discrimination and Harassment Policy prohibits discrimination, harassment or related retaliation based on race, color,national origin, sex, pregnancy, age, disability, creed, religion, sexual orientation, gender identity, gender expression, veteran status, political affiliation or political philosophy. Individuals who believe they have been subject to misconduct under either policy should contact the Office of Institutional Equity and Compliance (OIEC) at 303-492-2127. Information about the OIEC, the above referenced policies, and the campus resources available to assist individuals regarding sexual misconduct, discrimination, harassment or related retaliation can be found at the OIEC website.\nHonor Code All students enrolled in a University of Colorado Boulder course are responsible for knowing and adhering to the academic integrity policy of the institution. Violations of the policy may include: plagiarism, cheating,fabrication, lying, bribery, threat, unauthorized access, clicker fraud,resubmission, and aiding academic dishonesty. All incidents of academic misconduct will be reported to the Honor Code Council (honor@colorado.edu; 303-735-2273). Students who are found responsible for violating the academic integrity policy will be subject to nonacademic sanctions from the Honor Code Council as well as academic sanctions from the faculty member. Additional information regarding the academic integrity policy can be found at http://honorcode.colorado.edu.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567206505,"objectID":"e9c15258b33f23fe5212de16261d2bc2","permalink":"https://cucs-hpsc.github.io/fall2019/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/fall2019/syllabus/","section":"fall2019","summary":"Overview This course will develop the skills necessary to reason about performance of applications and modern architectures, to identify opportunities and side-effects of changes, to develop high-performance software, to transfer algorithmic patterns and lessons learned from different domains, and to communicate such analyses with diverse stakeholders. These skills are important for research and development of numerical methods and performance-sensitive science and engineering applications, obtaining allocations via NSF\u0026rsquo;s XSEDE and DOE ASCR facilities, as well as in jobs affiliated with computing facilities at national labs, industry, and academia.","tags":null,"title":"Syllabus","type":"docs"},{"authors":null,"categories":null,"content":" MPI-IO  ATPESC 2019: Introduction to MPI-IO [ATPESC 2019:  Unstructured mesh load (from Václav Hapla) DAOS: Distributed Asynchronous Object Storage CF conventions  CF = Climate and Forecast; widely used in earth sciences Based on NetCDF, which builds on top of HDF5 Documented standard, v1.7 Standard Names Table Compliance checker  Unstructured mesh formats  ExodusII (uses NetCDF uses HDF5) CGNS (uses HDF5) MED (uses HDF5) \u0026hellip;  !ncdump -h squaremotor-30.exo  netcdf squaremotor-30 { dimensions: len_string = 33 ; len_line = 81 ; four = 4 ; time_step = UNLIMITED ; // (0 currently) num_dim = 2 ; num_nodes = 719 ; num_elem = 659 ; num_el_blk = 2 ; num_qa_rec = 1 ; num_side_sets = 2 ; num_side_ss1 = 108 ; num_df_ss1 = 216 ; num_side_ss2 = 12 ; num_df_ss2 = 24 ; num_el_in_blk1 = 117 ; num_nod_per_el1 = 4 ; num_att_in_blk1 = 1 ; num_el_in_blk2 = 542 ; num_nod_per_el2 = 4 ; num_att_in_blk2 = 1 ; variables: double time_whole(time_step) ; char qa_records(num_qa_rec, four, len_string) ; char coor_names(num_dim, len_string) ; char eb_names(num_el_blk, len_string) ; int ss_status(num_side_sets) ; int ss_prop1(num_side_sets) ; ss_prop1:name = \u0026quot;ID\u0026quot; ; char ss_names(num_side_sets, len_string) ; int elem_ss1(num_side_ss1) ; int side_ss1(num_side_ss1) ; double dist_fact_ss1(num_df_ss1) ; int elem_ss2(num_side_ss2) ; int side_ss2(num_side_ss2) ; double dist_fact_ss2(num_df_ss2) ; int elem_map(num_elem) ; int eb_status(num_el_blk) ; int eb_prop1(num_el_blk) ; eb_prop1:name = \u0026quot;ID\u0026quot; ; double attrib1(num_el_in_blk1, num_att_in_blk1) ; int connect1(num_el_in_blk1, num_nod_per_el1) ; connect1:elem_type = \u0026quot;SHELL4\u0026quot; ; double attrib2(num_el_in_blk2, num_att_in_blk2) ; int connect2(num_el_in_blk2, num_nod_per_el2) ; connect2:elem_type = \u0026quot;SHELL4\u0026quot; ; double coordx(num_nodes) ; double coordy(num_nodes) ; // global attributes: :api_version = 4.98f ; :version = 4.98f ; :floating_point_word_size = 8 ; :file_size = 1 ; :title = \u0026quot;cubit(squaremotor-30.exo): 11/20/2012: 15:12:45\u0026quot; ; }  !h5dump -H cylinder.med  HDF5 \u0026quot;cylinder.med\u0026quot; { GROUP \u0026quot;/\u0026quot; { ATTRIBUTE \u0026quot;descripteur de fichier\u0026quot; { DATATYPE H5T_STRING { STRSIZE 27; STRPAD H5T_STR_NULLTERM; CSET H5T_CSET_ASCII; CTYPE H5T_C_S1; } DATASPACE SCALAR } GROUP \u0026quot;ENS_MAA\u0026quot; { GROUP \u0026quot;box_3d_1\u0026quot; { ATTRIBUTE \u0026quot;DES\u0026quot; { DATATYPE H5T_STRING { STRSIZE 23; STRPAD H5T_STR_NULLTERM; CSET H5T_CSET_ASCII; CTYPE H5T_C_S1; } DATASPACE SCALAR } ATTRIBUTE \u0026quot;DIM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;ESP\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NOM\u0026quot; { DATATYPE H5T_STRING { STRSIZE 1; STRPAD H5T_STR_NULLTERM; CSET H5T_CSET_ASCII; CTYPE H5T_C_S1; } DATASPACE SCALAR } ATTRIBUTE \u0026quot;NXI\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NXT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;REP\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;SRT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;TYP\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;UNI\u0026quot; { DATATYPE H5T_STRING { STRSIZE 1; STRPAD H5T_STR_NULLTERM; CSET H5T_CSET_ASCII; CTYPE H5T_C_S1; } DATASPACE SCALAR } ATTRIBUTE \u0026quot;UNT\u0026quot; { DATATYPE H5T_STRING { STRSIZE 1; STRPAD H5T_STR_NULLTERM; CSET H5T_CSET_ASCII; CTYPE H5T_C_S1; } DATASPACE SCALAR } GROUP \u0026quot;-0000000000000000001-0000000000000000001\u0026quot; { ATTRIBUTE \u0026quot;CGT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NDT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NOR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NXI\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NXT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;PDT\u0026quot; { DATATYPE H5T_IEEE_F64LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;PVI\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;PVT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } GROUP \u0026quot;MAI\u0026quot; { ATTRIBUTE \u0026quot;CGT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } GROUP \u0026quot;TE4\u0026quot; { ATTRIBUTE \u0026quot;CGS\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;CGT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;GEO\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;PFL\u0026quot; { DATATYPE H5T_STRING { STRSIZE 24; STRPAD H5T_STR_NULLTERM; CSET H5T_CSET_ASCII; CTYPE H5T_C_S1; } DATASPACE SCALAR } DATASET \u0026quot;FAM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SIMPLE { ( 161 ) / ( 161 ) } ATTRIBUTE \u0026quot;CGT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } } DATASET \u0026quot;NOD\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SIMPLE { ( 644 ) / ( 644 ) } ATTRIBUTE \u0026quot;CGT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } } } GROUP \u0026quot;TR3\u0026quot; { ATTRIBUTE \u0026quot;CGS\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;CGT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;GEO\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;PFL\u0026quot; { DATATYPE H5T_STRING { STRSIZE 24; STRPAD H5T_STR_NULLTERM; CSET H5T_CSET_ASCII; CTYPE H5T_C_S1; } DATASPACE SCALAR } DATASET \u0026quot;FAM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SIMPLE { ( 56 ) / ( 56 ) } ATTRIBUTE \u0026quot;CGT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } } DATASET \u0026quot;NOD\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SIMPLE { ( 168 ) / ( 168 ) } ATTRIBUTE \u0026quot;CGT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } } } } GROUP \u0026quot;NOE\u0026quot; { ATTRIBUTE \u0026quot;CGS\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;CGT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;PFL\u0026quot; { DATATYPE H5T_STRING { STRSIZE 24; STRPAD H5T_STR_NULLTERM; CSET H5T_CSET_ASCII; CTYPE H5T_C_S1; } DATASPACE SCALAR } DATASET \u0026quot;COO\u0026quot; { DATATYPE H5T_IEEE_F64LE DATASPACE SIMPLE { ( 168 ) / ( 168 ) } ATTRIBUTE \u0026quot;CGT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } } DATASET \u0026quot;FAM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SIMPLE { ( 56 ) / ( 56 ) } ATTRIBUTE \u0026quot;CGT\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } } } } } } GROUP \u0026quot;FAS\u0026quot; { GROUP \u0026quot;box_3d_1\u0026quot; { GROUP \u0026quot;ELEME\u0026quot; { GROUP \u0026quot;F_2D_1\u0026quot; { ATTRIBUTE \u0026quot;NUM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } GROUP \u0026quot;GRO\u0026quot; { ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } DATASET \u0026quot;NOM\u0026quot; { DATATYPE H5T_ARRAY { [80] H5T_STD_I8LE } DATASPACE SIMPLE { ( 1 ) / ( 1 ) } } } } GROUP \u0026quot;F_2D_126\u0026quot; { ATTRIBUTE \u0026quot;NUM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } GROUP \u0026quot;GRO\u0026quot; { ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } DATASET \u0026quot;NOM\u0026quot; { DATATYPE H5T_ARRAY { [80] H5T_STD_I8LE } DATASPACE SIMPLE { ( 1 ) / ( 1 ) } } } } GROUP \u0026quot;F_2D_128\u0026quot; { ATTRIBUTE \u0026quot;NUM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } GROUP \u0026quot;GRO\u0026quot; { ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } DATASET \u0026quot;NOM\u0026quot; { DATATYPE H5T_ARRAY { [80] H5T_STD_I8LE } DATASPACE SIMPLE { ( 1 ) / ( 1 ) } } } } GROUP \u0026quot;F_2D_130\u0026quot; { ATTRIBUTE \u0026quot;NUM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } GROUP \u0026quot;GRO\u0026quot; { ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } DATASET \u0026quot;NOM\u0026quot; { DATATYPE H5T_ARRAY { [80] H5T_STD_I8LE } DATASPACE SIMPLE { ( 1 ) / ( 1 ) } } } } GROUP \u0026quot;F_2D_132\u0026quot; { ATTRIBUTE \u0026quot;NUM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } GROUP \u0026quot;GRO\u0026quot; { ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } DATASET \u0026quot;NOM\u0026quot; { DATATYPE H5T_ARRAY { [80] H5T_STD_I8LE } DATASPACE SIMPLE { ( 1 ) / ( 1 ) } } } } GROUP \u0026quot;F_2D_134\u0026quot; { ATTRIBUTE \u0026quot;NUM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } GROUP \u0026quot;GRO\u0026quot; { ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } DATASET \u0026quot;NOM\u0026quot; { DATATYPE H5T_ARRAY { [80] H5T_STD_I8LE } DATASPACE SIMPLE { ( 1 ) / ( 1 ) } } } } GROUP \u0026quot;F_2D_136\u0026quot; { ATTRIBUTE \u0026quot;NUM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } GROUP \u0026quot;GRO\u0026quot; { ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } DATASET \u0026quot;NOM\u0026quot; { DATATYPE H5T_ARRAY { [80] H5T_STD_I8LE } DATASPACE SIMPLE { ( 1 ) / ( 1 ) } } } } GROUP \u0026quot;F_2D_2\u0026quot; { ATTRIBUTE \u0026quot;NUM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } GROUP \u0026quot;GRO\u0026quot; { ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } DATASET \u0026quot;NOM\u0026quot; { DATATYPE H5T_ARRAY { [80] H5T_STD_I8LE } DATASPACE SIMPLE { ( 1 ) / ( 1 ) } } } } GROUP \u0026quot;F_3D_1\u0026quot; { ATTRIBUTE \u0026quot;NUM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } GROUP \u0026quot;GRO\u0026quot; { ATTRIBUTE \u0026quot;NBR\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } DATASET \u0026quot;NOM\u0026quot; { DATATYPE H5T_ARRAY { [80] H5T_STD_I8LE } DATASPACE SIMPLE { ( 1 ) / ( 1 ) } } } } } GROUP \u0026quot;FAMILLE_ZERO\u0026quot; { ATTRIBUTE \u0026quot;NUM\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } } } } GROUP \u0026quot;INFOS_GENERALES\u0026quot; { ATTRIBUTE \u0026quot;MAJ\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;MIN\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } ATTRIBUTE \u0026quot;REL\u0026quot; { DATATYPE H5T_STD_I32LE DATASPACE SCALAR } } } }  ","date":1573217365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573242553,"objectID":"4765d6435543cfdbb8bc0d72962912cb","permalink":"https://cucs-hpsc.github.io/fall2019/mpi-io/","publishdate":"2019-11-08T06:49:25-06:00","relpermalink":"/fall2019/mpi-io/","section":"fall2019","summary":"MPI-IO  ATPESC 2019: Introduction to MPI-IO [ATPESC 2019:  Unstructured mesh load (from Václav Hapla) DAOS: Distributed Asynchronous Object Storage CF conventions  CF = Climate and Forecast; widely used in earth sciences Based on NetCDF, which builds on top of HDF5 Documented standard, v1.7 Standard Names Table Compliance checker  Unstructured mesh formats  ExodusII (uses NetCDF uses HDF5) CGNS (uses HDF5) MED (uses HDF5) \u0026hellip;  !","tags":null,"title":"MPI-IO and HDF5","type":"docs"},{"authors":null,"categories":null,"content":" Hiding latency Throughout this course, we\u0026rsquo;ve discussed ways in which computer architecture and algorithms hide latency. * instruction-level parallelism * SMT/SIMT multi-threading * memory prefetch * organizing data structures for streaming access and cache reuse * tiling, etc.\nFile latency is vastly higher than memory.\nfrom IPython.display import IFrame IFrame('https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html', width=1200, height=700)      Device Bandwidth (GB/s) Cost (\\$/TB) Seek Latency ($\\mu$s)     7200 RPM disk 0.3 50 \u0026gt;3000   SSD (SATA 3.0) 0.6 100-200 15-50   SSD/NVMe (PCIe-3 x4) 3 200-1000 10-20   DRAM DDR4 25 5000 0.1    slide credit\nHow much storage bandwidth does a similation need? wall_clock_per_timestep = 0.6 dofs_per_node = 1800 * 1e4 MBps_per_node = dofs_per_node * 8 / wall_clock_per_timestep / 1e6 dofs = 95e6 MBps = dofs * 8 / wall_clock_per_timestep / 1e6 MBps_per_node, MBps  (240.0, 1266.6666666666667)  seconds_per_day = 24 * 60 * 60 TB_per_day = MBps * seconds_per_day / 1e6 # TB TB_per_day  109.44  What if all nodes used storage at this rate?\nMBps_per_node * 4600 / 1e6 # TB/s  1.104  TB_per_day * 4600 / 8 / 1e3 # PB/day  62.928  Summit File Systems  Capacity: 250 PB Theoretical Bandwidth: 2.5 TB/s  Storage architecture Hadoop File System: Medium/long-term storage collocated with compute nodes HPC-style parallel storage https://www.olcf.ornl.gov/for-users/system-user-guides/summit/summit-user-guide/#file-systems\nRecommended slide decks  ATPESC 2019: Principles of HPC I/O:Everything you always wanted to know about HPC I/O but were afraid to ask ATPESC 2019: I/O Architectures and Technology  Burst buffers ! strace python -c 'import numpy' |\u0026amp; grep open  openat(AT_FDCWD, \u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/tls/haswell/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) stat(\u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/tls/haswell/x86_64\u0026quot;, 0x7ffc51a1a890) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/tls/haswell/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) stat(\u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/tls/haswell\u0026quot;, 0x7ffc51a1a890) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/tls/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) stat(\u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/tls/x86_64\u0026quot;, 0x7ffc51a1a890) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/tls/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) stat(\u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/tls\u0026quot;, 0x7ffc51a1a890) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/haswell/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) stat(\u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/haswell/x86_64\u0026quot;, 0x7ffc51a1a890) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/haswell/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) stat(\u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/haswell\u0026quot;, 0x7ffc51a1a890) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) stat(\u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/x86_64\u0026quot;, 0x7ffc51a1a890) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) stat(\u0026quot;/opt/pgi/linux86-64/19.4/mpi/openmpi/lib\u0026quot;, 0x7ffc51a1a890) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/home/jed/usr/paraview/lib/tls/haswell/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/home/jed/usr/paraview/lib/tls/haswell/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/home/jed/usr/paraview/lib/tls/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/home/jed/usr/paraview/lib/tls/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/home/jed/usr/paraview/lib/haswell/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/home/jed/usr/paraview/lib/haswell/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/home/jed/usr/paraview/lib/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/home/jed/usr/paraview/lib/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/tls/haswell/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/tls/haswell/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/tls/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/tls/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/haswell/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/haswell/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/x86_64/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libpython3.7m.so.1.0\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/libpython3.7m.so.1.0\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libpthread.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/libpthread.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libdl.so.2\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/libdl.so.2\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libutil.so.1\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/libutil.so.1\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libm.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/libm.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/locale/locale-archive\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/gconv/gconv-modules.cache\u0026quot;, O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/gconv/gconv-modules\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/bin/pyvenv.cfg\u0026quot;, O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;pyvenv.cfg\u0026quot;, O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/home/jed/usr/lib/python3.7/site-packages\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/petsc/lib/petsc/bin\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/encodings/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/encodings/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/codecs.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/codecs.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/encodings\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/encodings/__pycache__/aliases.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/encodings/aliases.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/encodings/__pycache__/utf_8.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/encodings/utf_8.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/encodings/__pycache__/latin_1.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/encodings/latin_1.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/io.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/io.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/abc.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/abc.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/_bootlocale.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/_bootlocale.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/usr/lib/python3.7/site-packages/__pycache__/site.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/os.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/os.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/stat.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/stat.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/posixpath.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/posixpath.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/genericpath.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/genericpath.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/_collections_abc.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/_collections_abc.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/imp.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/imp.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/importlib/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/importlib/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/types.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/types.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/warnings.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/warnings.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/importlib\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/importlib/__pycache__/machinery.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/importlib/machinery.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/importlib/__pycache__/util.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/importlib/util.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/importlib/__pycache__/abc.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/importlib/abc.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/contextlib.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/contextlib.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/collections/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/collections/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/operator.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/operator.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/keyword.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/keyword.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/heapq.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/heapq.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_heapq.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/reprlib.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/reprlib.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/functools.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/functools.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/tokenize.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/tokenize.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/re.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/re.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/enum.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/enum.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/sre_compile.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/sre_compile.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/sre_parse.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/sre_parse.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/sre_constants.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/sre_constants.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/copyreg.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/copyreg.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/token.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/token.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/site.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/_sitebuiltins.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/_sitebuiltins.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/.local/lib/python3.7/site-packages\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/.local/lib/python3.7/site-packages/easy-install.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/.local/lib/python3.7/site-packages/python_graph_core-1.8.2-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/.local/lib/python3.7/site-packages\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 4 openat(AT_FDCWD, \u0026quot;/home/jed/.local/lib/python3.7/site-packages/sphinxcontrib_spelling-4.2.0-py3.6-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/matplotlib-3.1.1-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 4 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/protobuf-3.10.0-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/pytest-cov.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/ruamel.yaml-0.16.5-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/sphinxcontrib_applehelp-1.0.1-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/src/nbgrader\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 4 openat(AT_FDCWD, \u0026quot;/home/jed/.local/lib/python3.7/site-packages/jupyter-1.0.0-py3.7.egg\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 4 openat(AT_FDCWD, \u0026quot;/home/jed/src/meshio\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 4 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/sphinxcontrib_devhelp-1.0.1-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/sphinxcontrib_htmlhelp-1.0.2-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/sphinxcontrib_jsmath-1.0.1-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/sphinxcontrib_qthelp-1.0.2-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/sphinxcontrib_serializinghtml-1.1.3-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/zope.component-4.5-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/zope.deferredimport-4.3.1-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/zope.deprecation-4.4.0-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/zope.event-4.4-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/zope.hookable-4.2.0-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/zope.interface-4.6.0-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/zope.proxy-4.3.2-py3.7-nspkg.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/usr/lib/python3.7/site-packages\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/usr/lib/python3.7/site-packages/easy-install.pth\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/petsc/lib/petsc/bin\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/cu/hpsc/hpsc-class/content/fall2019/io\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/src/python-vote-core\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/home/jed/src/academic-admin\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/__future__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__future__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/__pycache__/_globals.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/_globals.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/__pycache__/__config__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/__config__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/__pycache__/version.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/version.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/__pycache__/_distributor_init.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/_distributor_init.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/info.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/info.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/multiarray.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/multiarray.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/overrides.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/overrides.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/textwrap.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/textwrap.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libcblas.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/libcblas.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libblas.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/tls/haswell/x86_64/libblas.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/tls/haswell/libblas.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/tls/x86_64/libblas.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/tls/libblas.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/haswell/x86_64/libblas.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/haswell/libblas.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/x86_64/libblas.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/libblas.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libgomp.so.1\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/libgomp.so.1\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/sys/devices/system/cpu\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/sys/devices/system/cpu\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/datetime.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/datetime.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/etc/localtime\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/math.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_datetime.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/compat/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/compat/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/compat\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/compat/__pycache__/_inspect.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/compat/_inspect.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/compat/__pycache__/py3k.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/compat/py3k.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/pathlib.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/pathlib.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/fnmatch.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/fnmatch.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/ntpath.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/ntpath.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/urllib/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/urllib/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/urllib\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/urllib/__pycache__/parse.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/urllib/parse.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/pickle.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/pickle.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/struct.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/struct.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_struct.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/_compat_pickle.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/_compat_pickle.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_pickle.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/umath.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/umath.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/numerictypes.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/numerictypes.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/numbers.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/numbers.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/_string_helpers.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_string_helpers.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/_type_aliases.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_type_aliases.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/_dtype.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_dtype.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/numeric.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/numeric.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/_exceptions.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_exceptions.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/_asarray.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_asarray.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/_ufunc_config.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_ufunc_config.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/collections\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/collections/__pycache__/abc.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/collections/abc.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/fromnumeric.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/_methods.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_methods.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/arrayprint.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/arrayprint.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/defchararray.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/defchararray.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/records.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/records.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/memmap.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/memmap.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/function_base.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/function_base.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/machar.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/machar.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/getlimits.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/getlimits.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/shape_base.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/shape_base.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/einsumfunc.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/einsumfunc.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/_add_newdocs.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_add_newdocs.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_multiarray_tests.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/_dtype_ctypes.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_dtype_ctypes.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libffi.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/libffi.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/ctypes/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/ctypes/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/ctypes\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/ctypes/__pycache__/_endian.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/ctypes/_endian.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/proc/self/status\u0026quot;, O_RDONLY) = 3 openat(AT_FDCWD, \u0026quot;/proc/mounts\u0026quot;, O_RDONLY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/__pycache__/_internal.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/core/_internal.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/platform.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/platform.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/subprocess.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/subprocess.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/signal.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/signal.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_posixsubprocess.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/select.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/selectors.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/selectors.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/threading.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/threading.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/traceback.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/traceback.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/linecache.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/linecache.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/_weakrefset.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/_weakrefset.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/__pycache__/_pytesttester.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/_pytesttester.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/info.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/info.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/type_check.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/type_check.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/ufunclike.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/ufunclike.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/index_tricks.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/index_tricks.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/matrixlib/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/matrixlib/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/matrixlib\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/matrixlib/__pycache__/defmatrix.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/matrixlib/defmatrix.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/ast.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/ast.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/linalg/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/linalg/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/linalg\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/linalg/__pycache__/info.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/linalg/info.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/linalg/__pycache__/linalg.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/linalg/linalg.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/twodim_base.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/twodim_base.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/linalg/lapack_lite.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/liblapack.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/liblapack.so.3\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libgfortran.so.5\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/libgfortran.so.5\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libgcc_s.so.1\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/libgcc_s.so.1\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/../lib/tls/haswell/x86_64/libquadmath.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/../lib/tls/haswell/libquadmath.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/../lib/tls/x86_64/libquadmath.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/../lib/tls/libquadmath.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/../lib/haswell/x86_64/libquadmath.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/../lib/haswell/libquadmath.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/../lib/x86_64/libquadmath.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/lib/../lib/libquadmath.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/linalg/_umath_linalg.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/function_base.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/function_base.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/histograms.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/histograms.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/stride_tricks.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/stride_tricks.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/mixins.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/mixins.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/nanfunctions.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/nanfunctions.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/shape_base.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/shape_base.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/scimath.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/scimath.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/polynomial.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/polynomial.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/utils.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/utils.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/arraysetops.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/npyio.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/npyio.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/weakref.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/weakref.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/format.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/format.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/_datasource.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/_datasource.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/shutil.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/shutil.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/zlib.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libz.so.1\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/libz.so.1\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/bz2.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/bz2.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/_compression.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/_compression.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_bz2.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libbz2.so.1.0\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/libbz2.so.1.0\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/lzma.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lzma.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_lzma.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/liblzma.so.5\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/liblzma.so.5\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/grp.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/_iotools.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/_iotools.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/financial.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/financial.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/decimal.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/decimal.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_decimal.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libmpdec.so.2\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/libmpdec.so.2\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/arrayterator.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/arrayterator.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/arraypad.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/arraypad.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/__pycache__/_version.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/lib/_version.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/fft/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/fft/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/fft\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/fft/__pycache__/_pocketfft.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/fft/_pocketfft.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/fft/_pocketfft_internal.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/fft/__pycache__/helper.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/fft/helper.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/__pycache__/polynomial.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/polynomial.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/__pycache__/polyutils.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/polyutils.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/__pycache__/_polybase.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/_polybase.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/__pycache__/chebyshev.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/chebyshev.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/__pycache__/legendre.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/legendre.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/__pycache__/hermite.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/hermite.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/__pycache__/hermite_e.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/hermite_e.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/__pycache__/laguerre.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/polynomial/laguerre.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/__pycache__/_pickle.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/_pickle.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/mtrand.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/common.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/bounded_integers.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/mt19937.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/bit_generator.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/secrets.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/secrets.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/base64.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/base64.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/binascii.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/hmac.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/hmac.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_hashlib.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/R/lib/libcrypto.so.1.1\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/libcrypto.so.1.1\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/hashlib.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/hashlib.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_blake2.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_sha3.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/random.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/random.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/bisect.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/bisect.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_bisect.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/lib-dynload/_random.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/philox.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/pcg64.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/sfc64.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/random/generator.cpython-37m-x86_64-linux-gnu.so\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/__pycache__/ctypeslib.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/ctypeslib.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/ma/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/ma/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/ma\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/ma/__pycache__/core.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/ma/core.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/ma/__pycache__/extras.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/ma/extras.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/__pycache__/result.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/result.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/__pycache__/util.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/util.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/__pycache__/case.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/case.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/difflib.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/difflib.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/logging/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/logging/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/string.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/string.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/pprint.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/pprint.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/__pycache__/suite.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/suite.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/__pycache__/loader.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/loader.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/__pycache__/main.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/main.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/argparse.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/argparse.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/gettext.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/gettext.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/locale.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/locale.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/__pycache__/runner.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/runner.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/__pycache__/signals.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/unittest/signals.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing/_private/__pycache__/__init__.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing/_private/__init__.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing/_private\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing/_private/__pycache__/utils.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing/_private/utils.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/__pycache__/tempfile.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/tempfile.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing/_private/__pycache__/decorators.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing/_private/decorators.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing/_private/__pycache__/nosetester.cpython-37.pyc\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/usr/lib/python3.7/site-packages/numpy/testing/_private/nosetester.py\u0026quot;, O_RDONLY|O_CLOEXEC) = 3  figure\nFurther reading  ATPESC 2019 Presentations (see August 2 slides) ATPESC 2019 Videos: Data Intensive Computing and I/O Hadoop\u0026rsquo;s Uncomfortable Fit in HPC  ","date":1573044565,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573067738,"objectID":"081b303d83d06e38e369ac32b1e5bc8f","permalink":"https://cucs-hpsc.github.io/fall2019/io/","publishdate":"2019-11-06T06:49:25-06:00","relpermalink":"/fall2019/io/","section":"fall2019","summary":"Hiding latency Throughout this course, we\u0026rsquo;ve discussed ways in which computer architecture and algorithms hide latency. * instruction-level parallelism * SMT/SIMT multi-threading * memory prefetch * organizing data structures for streaming access and cache reuse * tiling, etc.\nFile latency is vastly higher than memory.\nfrom IPython.display import IFrame IFrame('https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html', width=1200, height=700)      Device Bandwidth (GB/s) Cost (\\$/TB) Seek Latency ($\\mu$s)     7200 RPM disk 0.","tags":null,"title":"HPC I/O","type":"docs"},{"authors":null,"categories":null,"content":" def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)     Architecture Directives SIMD SPMD     Intel AVX+ (SIMD) #pragma omp simd intrinsics ISPC   CUDA (SIMT) #pragma omp target C++ templates and other high-level APIs CUDA    ISPC: Intel SPMD Program Compiler We can program SIMT (e.g., CUDA) devices using directives, but we can also program SIMD (e.g., Intel CPUs) using a SPMD (CUDA-like, acronym comes from \u0026ldquo;single program\u0026rdquo; versus \u0026ldquo;single instruction\u0026rdquo;) programming model.\nrender_c('simple-ispc.ispc')  export void simple_ispc(uniform double vin[], uniform double vout[], uniform int count) { foreach (index = 0 ... count) { double v = vin[index]; if (v \u0026lt; 3.) v = v * v; else v = sqrt(v); vout[index] = v; } }  This function is callable from native C code.\nrender_c('simple.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;math.h\u0026gt; void simple_ispc(double vin[], double vout[], int count); void simple_c(double vin[], double vout[], int count) { for (int index=0; index\u0026lt;count; index++) { double v = vin[index]; if (v \u0026lt; 3.) v = v * v; else v = sqrt(v); vout[index] = v; } } int main() { double vin[16], vout[16]; for (int i = 0; i \u0026lt; 16; ++i) vin[i] = i; simple_ispc(vin, vout, 16); for (int i = 0; i \u0026lt; 16; ++i) printf(\u0026quot;%d: simple_ispc(%f) = %f\\n\u0026quot;, i, vin[i], vout[i]); simple_c(vin, vout, 16); for (int i = 0; i \u0026lt; 16; ++i) printf(\u0026quot;%d: simple_c(%f) = %f\\n\u0026quot;, i, vin[i], vout[i]); return 0; }  ! make -B simple \u0026amp;\u0026amp; ./simple  cc -O3 -march=native -c -o simple.o simple.c ispc -O3 --target=avx2-i32x8 simple-ispc.ispc -o simple-ispc.o cc simple.o simple-ispc.o -lm -o simple 0: simple_ispc(0.000000) = 0.000000 1: simple_ispc(1.000000) = 1.000000 2: simple_ispc(2.000000) = 4.000000 3: simple_ispc(3.000000) = 1.732051 4: simple_ispc(4.000000) = 2.000000 5: simple_ispc(5.000000) = 2.236068 6: simple_ispc(6.000000) = 2.449490 7: simple_ispc(7.000000) = 2.645751 8: simple_ispc(8.000000) = 2.828427 9: simple_ispc(9.000000) = 3.000000 10: simple_ispc(10.000000) = 3.162278 11: simple_ispc(11.000000) = 3.316625 12: simple_ispc(12.000000) = 3.464102 13: simple_ispc(13.000000) = 3.605551 14: simple_ispc(14.000000) = 3.741657 15: simple_ispc(15.000000) = 3.872983 0: simple_c(0.000000) = 0.000000 1: simple_c(1.000000) = 1.000000 2: simple_c(2.000000) = 4.000000 3: simple_c(3.000000) = 1.732051 4: simple_c(4.000000) = 2.000000 5: simple_c(5.000000) = 2.236068 6: simple_c(6.000000) = 2.449490 7: simple_c(7.000000) = 2.645751 8: simple_c(8.000000) = 2.828427 9: simple_c(9.000000) = 3.000000 10: simple_c(10.000000) = 3.162278 11: simple_c(11.000000) = 3.316625 12: simple_c(12.000000) = 3.464102 13: simple_c(13.000000) = 3.605551 14: simple_c(14.000000) = 3.741657 15: simple_c(15.000000) = 3.872983  ! objdump -d --prefix-addresses -M intel simple | grep sqrt  0000000000001050 \u0026lt;sqrt@plt\u0026gt; jmp QWORD PTR [rip+0x2fd2] # 0000000000004028 \u0026lt;sqrt@GLIBC_2.2.5\u0026gt; 0000000000001056 \u0026lt;sqrt@plt+0x6\u0026gt; push 0x2 000000000000105b \u0026lt;sqrt@plt+0xb\u0026gt; jmp 0000000000001020 \u0026lt;.plt\u0026gt; 00000000000012ec \u0026lt;simple_c+0x4c\u0026gt; vsqrtsd xmm1,xmm0,xmm0 0000000000001302 \u0026lt;simple_c+0x62\u0026gt; call 0000000000001050 \u0026lt;sqrt@plt\u0026gt; 000000000000142d \u0026lt;simple_ispc___un_3C_und_3E_un_3C_und_3E_uni+0xdd\u0026gt; vsqrtpd ymm1,ymm4 0000000000001431 \u0026lt;simple_ispc___un_3C_und_3E_un_3C_und_3E_uni+0xe1\u0026gt; vsqrtpd ymm7,ymm5 000000000000156e \u0026lt;simple_ispc___un_3C_und_3E_un_3C_und_3E_uni+0x21e\u0026gt; vsqrtpd ymm2,ymm6 0000000000001577 \u0026lt;simple_ispc___un_3C_und_3E_un_3C_und_3E_uni+0x227\u0026gt; vsqrtpd ymm3,ymm7 000000000000168d \u0026lt;simple_ispc+0xdd\u0026gt; vsqrtpd ymm1,ymm4 0000000000001691 \u0026lt;simple_ispc+0xe1\u0026gt; vsqrtpd ymm7,ymm5 00000000000017ce \u0026lt;simple_ispc+0x21e\u0026gt; vsqrtpd ymm2,ymm6 00000000000017d7 \u0026lt;simple_ispc+0x227\u0026gt; vsqrtpd ymm3,ymm7  ISPC is a good option for code with cross-lane dependencies or vector lane divergence (branches that affect some lanes differently than others). Writing such code with intrinsics is laborious and compilers often do a poor job of inferring good vectorization strategies (despite #pragma omp simd and the like). An example of successful use of ISPC is Intel\u0026rsquo;s Embree ray tracing engine.\n(As with most vendor-reported performance numbers, we can probably take this with a grain of salt. But it indicates that CPUs remain highly competitive for ray tracing.)\nOpenMP target offload and OpenACC CUDA is relatively hard to maintain and logic/tuning is spread out (between the kernel launch and the device code). OpenMP target offload and OpenACC attempt to provide a more friendly story for maintenance and incremental migration of legacy code.\nTerminology    CUDA Concept CUDA keyword OpenACC OpenMP target     Thread block blockIdx gang teams   Warp (implicit) worker thread   Thread threadIdx vector simd    Incremental porting with unified memory Example OpenACC example from a Lattice-Boltzman miniapp\nvoid LBM::stream(Real* const __restrict a_f, const Real* const __restrict a_f_post, const int* a_loStr, const int* a_hiStr, const int* a_loAll, const int* a_hiAll, const int a_numPts) const { const int* const __restrict latI = \u0026amp;m_lattice[0][0]; const int* const __restrict latJ = \u0026amp;m_lattice[1][0]; const int* const __restrict latK = \u0026amp;m_lattice[2][0]; const int klo = a_loStr[2], khi = a_hiStr[2], jlo = a_loStr[1], jhi = a_hiStr[1], ilo = a_loStr[0], ihi = a_hiStr[0]; #pragma acc parallel loop independent collapse(3) \\ copyin(a_loAll[SPACEDIM],a_hiAll[SPACEDIM],a_f_post[a_numPts*m_numVels]) \\ copyout(a_f[a_numPts*m_numVels]) vector_length(256) for (int k = klo; k \u0026lt;= khi; ++k) { for (int j = jlo; j \u0026lt;= jhi; ++j) { for (int i = ilo; i \u0026lt;= ihi; ++i) { #pragma acc loop seq independent for (int m = 0; m \u0026lt; NUMV; ++m) { const long int offset = m * a_numPts; const long int index0 = INDEX(i , j, k, a_loAll, a_hiAll); const long int index2 = INDEX(i - latI[m], j - latJ[m], k - latK[m], a_loAll, a_hiAll); a_f[index0 + offset] = a_f_post[index2 + offset]; // new f comes from upwind } } } } }  Resources  Getting started with OpenACC Advanced OpenACC SC18 OpenMP Presentations (with videos) OpenMP 5.0 for Accelerators and What Comes Next Hackathon series  ","date":1572871765,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572896119,"objectID":"6aa91eedf6361cf0d2a2d1dbdf980bbd","permalink":"https://cucs-hpsc.github.io/fall2019/openmp-target/","publishdate":"2019-11-04T06:49:25-06:00","relpermalink":"/fall2019/openmp-target/","section":"fall2019","summary":"def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)     Architecture Directives SIMD SPMD     Intel AVX+ (SIMD) #pragma omp simd intrinsics ISPC   CUDA (SIMT) #pragma omp target C++ templates and other high-level APIs CUDA    ISPC: Intel SPMD Program Compiler We can program SIMT (e.g., CUDA) devices using directives, but we can also program SIMD (e.","tags":null,"title":"ISPC, OpenMP target, OpenACC, and all that","type":"docs"},{"authors":null,"categories":null,"content":"  GPUs have 2-4x greater floating point and bandwidth peak for the watts  also for the $ if you buy enterprise gear better for the $ if you buy gaming gear  Step 1 is to assess workload and latency requirements   Don\u0026rsquo;t waste time with GPUs if  your problem size or time to solution requirements don\u0026rsquo;t align if the work you\u0026rsquo;d like to move to the GPU is not a bottleneck if the computation cost will be dwarfed by moving data to/from the GPU often you need to restructure so that caller passes in data already on the device can require nonlocal refactoring  Almost never: pick one kernel at a time and move it to the GPU  DOE ACME/E3SM (to pick on one high-profile application) has basically done this for five years and it still doesn\u0026rsquo;t help their production workloads so they bought a non-GPU machine    Okay, okay, okay. What if I have the right workload?\nTerminology/intro  An even easier introduction to CUDA CUDA Programming Model\n On the CPU, we have a thread with vector registers/instructions\n In CUDA, we write code inside a single vector lane (\u0026ldquo;confusingly\u0026rdquo; called a CUDA thread)\n To get inside the lane, we launch a kernel from the CPU using special syntax\nadd\u0026lt;\u0026lt;\u0026lt;numBlocks, blockSize\u0026gt;\u0026gt;\u0026gt;(N, x, y);   needs to be compiled using nvcc Logically 1D/2D/3D rectangular tiled iteration space    There are many constraints and limitations to the iteration \u0026ldquo;grid\u0026rdquo;   Control flow for CUDA threads is nominally independent, but performance will be poor if you don\u0026rsquo;t coordinate threads within each block.  Implicit coordination Memory coalescing Organize your algorithm to limit \u0026ldquo;divergence\u0026rdquo; Explicit coordination Shared memory __syncthreads() Warp shuffles  We implement the kernel by using the __global__ attribute  Visible from the CPU Special built-in variables are defined gridDim blockIdx blockDim threadIdx There is also __device__, which is callable from other device functions Can use __host__ __device__ to compile two versions   How does this relate to the hardware?  Each thread block is assigned to one streaming multiprocessor (SM) Executed in warps (number of hardware lanes) Multiple warps (from the same or different thread blocks) execute like hyperthreads  Practical CUDA CUDA Best Practices Guide Occupancy  Thread instructions are executed sequentially in CUDA, and, as a result, executing other warps when one warp is paused or stalled is the only way to hide latencies and keep the hardware busy. Some metric related to the number of active warps on a multiprocessor is therefore important in determining how effectively the hardware is kept busy [emphasis added]. This metric is occupancy.\n  Reality: occupancy is just one aspect, and often inversely correlated with keeping the hardware busy (and with performance).   Occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps.\n  If your kernel uses fewer registers/less shared memory, more warps can be scheduled. Register/shared memory usage is determined by the compiler.\ndef render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;) render_c('add.cu')  __global__ void add(int n, float *x, float *y) { int index = blockIdx.x * blockDim.x + threadIdx.x; int stride = blockDim.x * gridDim.x; for (int i = index; i \u0026lt; n; i += stride) y[i] += x[i]; }  ! nvcc -arch sm_75 --resource-usage -c add.cu  ptxas info : 0 bytes gmem ptxas info : Compiling entry function \u0026lsquo;Z3addiPfS\u0026rsquo; for \u0026lsquo;sm_75\u0026rsquo; ptxas info : Function properties for Z3addiPfS 0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads ptxas info : Used 12 registers, 376 bytes cmem[0]\nrender_c('copy.cu')  __global__ void copy(float *dst, float *src) { int iblock = blockIdx.x + blockIdx.y * gridDim.x; int index = threadIdx.x + TILE_SIZE * iblock * blockDim.x; float a[TILE_SIZE]; // allocated in registers for (int i=0; i\u0026lt;TILE_SIZE; i++) a[i] = src[index + i * blockDim.x]; for (int i=0; i\u0026lt;TILE_SIZE; i++) dst[index + i * blockDim.x] = a[i]; }  ! nvcc -arch sm_75 --resource-usage -DTILE_SIZE=16 -c copy.cu  ptxas info : 0 bytes gmem ptxas info : Compiling entry function \u0026lsquo;Z4copyPfS\u0026rsquo; for \u0026lsquo;sm_75\u0026rsquo; ptxas info : Function properties for Z4copyPfS 0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads ptxas info : Used 64 registers, 368 bytes cmem[0]\n The Occupancy Calculator can compute occupancy based on the register and shared memory usage.\n You can tell the compiler to reduce register usage, sometimes at the expense of spills.\n! nvcc -arch sm_75 --resource-usage -DTILE_SIZE=16 --maxrregcount 24 -c copy.cu  ptxas info : 0 bytes gmem ptxas info : Compiling entry function \u0026lsquo;Z4copyPfS\u0026rsquo; for \u0026lsquo;sm_75\u0026rsquo; ptxas info : Function properties for Z4copyPfS 80 bytes stack frame, 76 bytes spill stores, 76 bytes spill loads ptxas info : Used 24 registers, 368 bytes cmem[0]\n  Further reading  Vasily Volkov (2010) Better Performance at Lower Occupancy (slides) Vasily Volkov (2016) Understanding Latency Hiding on GPUs (very in-depth) Kasia Swirydowicz (2018) Finite Element Stiffness Matrix Action: monolithic kernel optimization on Titan V  Memory  GPU memory is not CPU memory  Duh, so why does NVIDIA publish this?\nGetting your memory into position is often the hardest part of CUDA programming.  Allocate memory on the GPU\ncudaMalloc(\u0026amp;xdevice, N*sizeof(double));  Populate it from the host\ncudaMemcpy(xdevice, xhost, N*sizeof(double), cudaMemcpyHostToDevice);  Repeat for all data, including control parameters\n Easy to forget, ongoing maintenance/complexity cost\n  Unified/managed memory  Allocate \u0026ldquo;managed\u0026rdquo; memory, accessible from CPU and GPU\ncudaMallocManaged(\u0026amp;x, N*sizeof(float));  How?\n   With OpenACC, make all dynamic allocations in managed memory: pgcc -ta=tesla:managed  The GPU probably has a lot less memory than you have DRAM Really convenient for incremental work in legacy code Performance isn\u0026rsquo;t great without cudaMemPrefetchAsync    Further reading: Maximizing Unified Memory Performance in CUDA\nOn memory coalescing and strided access __global__ void strideCopy(float *odata, float* idata, int stride) { int xid = (blockIdx.x*blockDim.x + threadIdx.x)*stride; odata[xid] = idata[xid]; }  Lose half your bandwidth for stride=2.\n","date":1572612565,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572632837,"objectID":"a824445d346ccbbf8183e050ad63efea","permalink":"https://cucs-hpsc.github.io/fall2019/cuda-practical/","publishdate":"2019-11-01T06:49:25-06:00","relpermalink":"/fall2019/cuda-practical/","section":"fall2019","summary":"GPUs have 2-4x greater floating point and bandwidth peak for the watts  also for the $ if you buy enterprise gear better for the $ if you buy gaming gear  Step 1 is to assess workload and latency requirements   Don\u0026rsquo;t waste time with GPUs if  your problem size or time to solution requirements don\u0026rsquo;t align if the work you\u0026rsquo;d like to move to the GPU is not a bottleneck if the computation cost will be dwarfed by moving data to/from the GPU often you need to restructure so that caller passes in data already on the device can require nonlocal refactoring  Almost never: pick one kernel at a time and move it to the GPU  DOE ACME/E3SM (to pick on one high-profile application) has basically done this for five years and it still doesn\u0026rsquo;t help their production workloads so they bought a non-GPU machine    Okay, okay, okay.","tags":null,"title":"Practical CUDA","type":"docs"},{"authors":null,"categories":null,"content":"  GPU vs CPU characterization CUDA preview Execution heirarchy Memory managerie Optimizations  Graphics Processing Units Graphics Processing Units (GPUs) evolved from commercial demand for high-definition graphics. HPC general purpose computing with GPUs picked up after programmable shaders were added in early 2000s.\nGPU compute performance relative to CPU is not magic, rather it is based on difference in goals; GPUs were unpolluted by CPU demands for user-adaptability.\nNvidia.com: real-estate difference\nGPUs have no* branch prediction and no speculative execution. (In the early days, computational uses even needed to implement their own error correction in software!) Longer memory access latencies from tiny cache size is meant to be hidden behind co-resident compute. The difference in mentality allowed GPUs to far surpass CPU compute efficiency.\n*: recent devices use branch prediction to group divergent threads\nkarlrupp.net: compute efficiency\nPower can dominant the cost in HPC. Consider the Summit supercomputer:\n #2 GREEN500 (was #3, but #1 was decomissioned) cost \\$200 million to build 13 MW to run compute+interconnect+file systems =\u0026gt; roughly \\$7 million/year in raw electricity to power (does not count facilities/infrastructure cost to actually supply this power, nor cooling)  The drawbacks: GPU efficiency needs the problem to fit well into SIMD and have a relatively high computation intensity.\nCUDA Early general purpose computing GPU efforts required formulating problems in terms of graphics primatives (e.g. DirectX).\nNVIDIA publicly launched CUDA in 2006, allowing programming in C (and Fortran).\nFlash forward to 2019: AMD has its own language and there are also several vendor-independent languages (dominant: OpenCL), but CUDA still dominates overall.\nNvidia maintains good documentation to ease adoption, like its https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html.\nKernel syntax example // Add two matrices A and B of size NxN and stores the result into matrix C: // Kernel definition __global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N]) { int i = blockIdx.x * blockDim.x + threadIdx.x; int j = blockIdx.y * blockDim.y + threadIdx.y; if (i \u0026lt; N \u0026amp;\u0026amp; j \u0026lt; N) C[i][j] = A[i][j] + B[i][j]; } int main() { ... // Kernel invocation dim3 threadsPerBlock(16, 16); dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y); MatAdd\u0026lt;\u0026lt;\u0026lt;numBlocks, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(A, B, C); ... }  CUDA-specific additions: * Kernels are defined with a __global__ specifier (when called by the host). * \u0026lt;\u0026lt;\u0026lt;numBlocks, threadsPerBlock\u0026gt;\u0026gt;\u0026gt; gives the execution configuration. * Ways for threads to query their location: threadIdx, blockIdx.\nThread Heirarchy Threads each have their own register allocation. They are always executed in \u0026ldquo;SIMT\u0026rdquo; warps of up to 32 (could change, but hasn\u0026rsquo;t yet). This means: * any divergence of instructions between threads within a warp causes some of the threads to no-op (relaxed recently); * product(threadsPerBlock) should be a multiple of 32 (maximum 1024) where possible.\nBlocks each have their own shared memory allocation. All threads in a block are resident on the same processing core. Thread layout can be up to three dimensions. * can perform a lightweight synchronization within a block; * co-resident blocks can be helpful at masking latency, but this is limited by block memory and register use.\nBlocks themselves are layed out on a grid of up to three dimensions (on recent compute capabilities). They must be logically executable in parallel or any serial order. * no synchronization across blocks within a kernel; * embarassingly parallel only, although caches can be reused.\nNvidia.com: 2d grid and threads\nMemory Nvidia.com: model of memory connections\nGlobal, constant, and texture memories persist across kernel calls, and each has its own cache per SM (L2 cache shared by SMs). By default, host and device are assumed to maintain separate memory: * explicit device allocation and deallocation; * explicit transfer between host and device.\nAlternatively, there is a \u0026ldquo;Unified Memory\u0026rdquo; configuration that automates these on an as-needed basis, pretending there is one common address space.\nEach block has shared memory which tends to be fast (equivalent to a user-managed L1 cache).\nEach thread has \u0026ldquo;local\u0026rdquo; memory (that is actually no more local than global memory!), which is mostly used for register spilling. (Register and shared memory usage are reported by the compiler when compiling with the -ptxas-options=-v option.)\nMemory transfer example __global__ void VecAdd(float* A, float* B, float* C, int N) { int i = blockDim.x * blockIdx.x + threadIdx.x; if (i \u0026lt; N) C[i] = A[i] + B[i]; } int main() { int N = ...; size_t size = N * sizeof(float); // Allocate input vectors h_A and h_B in host memory float* h_A = (float*)malloc(size); float* h_B = (float*)malloc(size); float* h_C = (float*)malloc(size); // Initialize input vectors ... // Allocate vectors in device memory float* d_A; cudaMalloc(\u0026amp;d_A, size); float* d_B; cudaMalloc(\u0026amp;d_B, size); float* d_C; cudaMalloc(\u0026amp;d_C, size); // Copy vectors from host memory to device memory cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice); cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice); // Invoke kernel int threadsPerBlock = 256; int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock; VecAdd\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, d_C, N); // Copy result from device memory to host memory // h_C contains the result in host memory cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost); // Free device memory cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); // Free host memory ... }  \n Optimization Details Often details depend on the particular \u0026ldquo;compute capability\u0026rdquo; of the device.\nIntrinsic function instructions  similar tradeoffs to \u0026ndash;ffast-math  Memory Hiding Memory Transfers: Memory transfers between host and device generally have the greatest latency. Modern capabilities can hide data transfer between host and device by giving the device other tasks to work on and having the host use asynchronous versions of the transfer functions.\nThis is managed through streams on the host, where cuda calls within a stream are guaranteed to execute on the device in order, but those between streams may be out of order or overlap depending on the compute capability.\nTo minimize waiting with the following code, the compute capability needs to allow concurrent data transfers, concurrent kernel execution, and overlap of data transfer and kernel execution.\nfor (int i = 0; i \u0026lt; 2; ++i) { cudaMemcpyAsync(inputDevPtr + i * size, hostPtr + i * size, size, cudaMemcpyHostToDevice, stream[i]); MyKernel \u0026lt;\u0026lt;\u0026lt;100, 512, 0, stream[i]\u0026gt;\u0026gt;\u0026gt; (outputDevPtr + i * size, inputDevPtr + i * size, size); cudaMemcpyAsync(hostPtr + i * size, outputDevPtr + i * size, size, cudaMemcpyDeviceToHost, stream[i]); }  Global memory access size and alignment: Example: an array of this struct would have elements that aren\u0026rsquo;t aligned if not for the __align__(16):\nstruct __align__(16) { float x; float y; float z; };  This usually crops up with 2d arrays, which are more efficient if width-padded to a multiple of the warp size.\nCoalescence: Global (and local*) memory requests must be coalesced\u0026mdash;falling into the same 128-byte wide+aligned window (for all modern capabilities)\u0026mdash;or they will require multiple instructions.\n*: the compiler will generally ensure that local memory use is coalesced\nBank Distribution: Similar, but different from global-memory coalescence. Shared memory is divided into banks (typically 32), where each bank can be accessed simultaneously.\nNvidia.com: A) conflict-free, B) conflict depth 2, C) conflict-free, D) conflict-free, E) conflict-free, F) conflict-free\nMy impression is that most programmers rely on the compiler to sensibly structure bank accesses for temporary variables, but occasionally breaking into the \u0026ldquo;CUDA assembly\u0026rdquo; language PTX will yeild significant performance improvements.\nThe combination of coalescence and shared banks can cause an interesting interplay for certain problems. Consider: * Mx31 array made of structs of 2 32-bit floats. * Coalescence would suggest padding the array to 32 wide when reading from global memory, but then once it resides in a shared memory with 32-bit strided banks, a warp of threads accessing the first of the pair of floats will cause bank conflicts of depth 2. * Shared memory would be better served by padding the array width to 31.5.\n(the better solution might be to pull the struct apart\u0026hellip;)\nTexture-specific memory features:  Optimized for 2d locality; can be faster than non-coalesced global/constant memory requests. Ability to automatically cast 8\u0026frasl;16-bit integers into [0,1] 32-bit floats.\n  ","date":1572439765,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572464488,"objectID":"7b93af7dcbc27c07f05e7dc23d888cff","permalink":"https://cucs-hpsc.github.io/fall2019/cuda/","publishdate":"2019-10-30T06:49:25-06:00","relpermalink":"/fall2019/cuda/","section":"fall2019","summary":"GPU vs CPU characterization CUDA preview Execution heirarchy Memory managerie Optimizations  Graphics Processing Units Graphics Processing Units (GPUs) evolved from commercial demand for high-definition graphics. HPC general purpose computing with GPUs picked up after programmable shaders were added in early 2000s.\nGPU compute performance relative to CPU is not magic, rather it is based on difference in goals; GPUs were unpolluted by CPU demands for user-adaptability.\nNvidia.com: real-estate difference","tags":null,"title":"GPUs and CUDA","type":"docs"},{"authors":null,"categories":null,"content":"Coprocessor architectures \n CUDA devices (NVIDIA)  Programmable via CUDA, OpenACC, OpenMP-5, OpenCL, HIP-\u0026gt;CUDA, SYCL-\u0026gt;CUDA Example machine: OLCF Summit (details from user guide)  ROCm devices (AMD)  Programmable via HIP, OpenMP-5, OpenCL, SYCL-\u0026gt;HIP Example machine: OLCF Frontier  Intel X GPUs  Programmable via SYCL, OpenMP-5, OpenCL? Example machine: ALCF Aurora/A21  Upcoming non-coprocessor Supercomputers  RIKEN Fugaku (Post-K) TACC Frontera   Fundamental capabilities from io import StringIO import pandas import numpy as np import matplotlib.pyplot as plt plt.style.use('ggplot') data = StringIO(\u0026quot;\u0026quot;\u0026quot; package,cores,lanes/core,clock (MHz),peak (GF),bandwidth (GB/s),TDP (W),MSRP Xeon 8280,28,8,2700,2400,141,205,10000 NVIDIA V100,80,64,1455,7800,900,300,10664 AMD MI60,64,64,1800,7362,1024,300, AMD Rome,64,4,2000,2048,205,200,6450 \u0026quot;\u0026quot;\u0026quot;) df = pandas.read_csv(data, index_col='package') df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    cores lanes/core clock (MHz) peak (GF) bandwidth (GB/s) TDP (W) MSRP   package            Xeon 8280 28 8 2700 2400 141 205 10000.0   NVIDIA V100 80 64 1455 7800 900 300 10664.0   AMD MI60 64 64 1800 7362 1024 300 NaN   AMD Rome 64 4 2000 2048 205 200 6450.0     Amdahl's Law for energy efficiency df['efficiency (GF/W)'] = df['peak (GF)'] / df['TDP (W)'] df['efficiency (GF/W)']  package Xeon 8280 11.707317 NVIDIA V100 26.000000 AMD MI60 24.540000 AMD Rome 10.240000 Name: efficiency (GF/W), dtype: float64  ngpu = np.arange(0, 9) overhead = 100 # Power supply, DRAM, disk, etc. peak = (ngpu == 0)*df.loc['Xeon 8280']['peak (GF)'] + ngpu*df.loc['NVIDIA V100']['peak (GF)'] tdp = overhead + df.loc['Xeon 8280']['TDP (W)'] + ngpu*df.loc['NVIDIA V100']['TDP (W)'] plt.plot(ngpu, peak / tdp) plt.xlabel('number of GPUs per CPU') plt.title('DP Peak efficiency (GF/W)');  \nCompare to Green 500 list  #1 system is 15.1 GF/W (2x Xeon E5-2698v4, 8x V100) #2 system (Summit) is 14.7 GF/W (2x Power9, 6x V100) #27 system is 5.8 GF/W on Xeon 6248 (no GPU)  Amdahl for cost efficiency df['cost (GF/$)'] = df['peak (GF)'] / df['MSRP'] df['cost (GF/$)']  package Xeon 8280 0.240000 NVIDIA V100 0.731433 AMD MI60 NaN AMD Rome 0.317519 Name: cost (GF/$), dtype: float64  overhead = 3000 + 2000*ngpu # power supply, memory, cooling, maintenance cost = overhead + df.loc['Xeon 8280']['MSRP'] + ngpu*df.loc['NVIDIA V100']['MSRP'] plt.plot(ngpu, peak / cost) plt.xlabel('number of GPUs per CPU') plt.title('DP cost efficiency (GF/$)');  \nWhat fraction of datacenter cost goes to the power bill?  OLCF Summit is reportedly a \\$200M machine. What if we just buy the GPUs at retail?  256 racks 18 nodes per rack 6 GPUs per node V100 MSRP of about $10k   256 * 18 * 6 * 10e3 / 1e6 # millions  276.48   Rule of thumb: $ \\lesssim \\$1M $ per MW-year We know Summit is a 13 MW facility Check industrial electricity rates   .0638 * 24 * 365  558.8879999999999  \nProgramming models  Directives  OpenMP-5 OpenACC   #pragma acc data copy(A) create(Anew) while ( error \u0026gt; tol \u0026amp;\u0026amp; iter \u0026lt; iter_max ) { error = 0.0; #pragma acc kernels { #pragma acc loop independent collapse(2) for ( int j = 1; j \u0026lt; n-1; j++ ) { for ( int i = 1; i \u0026lt; m-1; i++ ) { Anew [j] [i] = 0.25 * ( A [j] [i+1] + A [j] [i-1] + A [j-1] [i] + A [j+1] [i]); error = max ( error, fabs (Anew [j] [i] - A [j] [i])); } } } }  Comparison slides: Is OpenMP 4.5 Target Off-load Ready for Real Life? A Case Study of Three Benchmark Kernels (2018)\n Thread \u0026quot;kernel\u0026quot; and control  CUDA HIP (video)  C++ templated  SYCL Kokkos Raja   ","date":1572266965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572292819,"objectID":"bbe9a9411405f1e5e942164de486ebcd","permalink":"https://cucs-hpsc.github.io/fall2019/coprocessor/","publishdate":"2019-10-28T06:49:25-06:00","relpermalink":"/fall2019/coprocessor/","section":"fall2019","summary":"Coprocessor architectures \n CUDA devices (NVIDIA)  Programmable via CUDA, OpenACC, OpenMP-5, OpenCL, HIP-\u0026gt;CUDA, SYCL-\u0026gt;CUDA Example machine: OLCF Summit (details from user guide)  ROCm devices (AMD)  Programmable via HIP, OpenMP-5, OpenCL, SYCL-\u0026gt;HIP Example machine: OLCF Frontier  Intel X GPUs  Programmable via SYCL, OpenMP-5, OpenCL? Example machine: ALCF Aurora/A21  Upcoming non-coprocessor Supercomputers  RIKEN Fugaku (Post-K) TACC Frontera   Fundamental capabilities from io import StringIO import pandas import numpy as np import matplotlib.","tags":null,"title":"Coprocessor architectures","type":"docs"},{"authors":null,"categories":null,"content":"Guest lecture on libCEED from Dr. Valeria Barra: slides.\nHPSC Lab 9 2019-10-25\nWe are going to run some PETSc examples and consider them as \u0026quot;baseline\u0026quot; for the libCEED examples that will follow.\n%%bash # You may need to change these for your machine PETSC_DIR=$HOME/petsc-3.12.0 PETSC_ARCH=mpich-dbg # Build the examples make -C $PETSC_DIR -f gmakefile $PETSC_ARCH/tests/ksp/ksp/examples/tutorials/ex34 make -C $PETSC_DIR -f gmakefile $PETSC_ARCH/tests/ksp/ksp/examples/tutorials/ex45 # Link them from the current directory to make it easy to run below cp -sf $PETSC_DIR/$PETSC_ARCH/tests/ksp/ksp/examples/tutorials/ex34 . cp -sf $PETSC_DIR/$PETSC_ARCH/tests/ksp/ksp/examples/tutorials/ex45 . ./ex34 -pc_type none -da_grid_x 50 -da_grid_y 50 -da_grid_z 50 -ksp_monitor #run with -ksp_view if you want to see details about the solver [preconditioning]  make: Entering directory '/home/jovyan/petsc-3.12.0' make: 'mpich-dbg/tests/ksp/ksp/examples/tutorials/ex34' is up to date. make: Leaving directory '/home/jovyan/petsc-3.12.0' make: Entering directory '/home/jovyan/petsc-3.12.0' make: 'mpich-dbg/tests/ksp/ksp/examples/tutorials/ex45' is up to date. make: Leaving directory '/home/jovyan/petsc-3.12.0' 0 KSP Residual norm 1.184352528131e-01 1 KSP Residual norm 4.514009350561e-15 Residual norm 4.63793e-15 Error norm 0.00130921 Error norm 0.000338459 Error norm 1.31699e-06  # Another variant with blocked jacobi as smoother: ! ./ex34 -da_grid_x 50 -da_grid_y 50 -da_grid_z 50 -pc_type ksp -ksp_ksp_type cg -ksp_pc_type bjacobi -ksp_monitor   0 KSP Residual norm 1.251646233668e+02 1 KSP Residual norm 1.480869591053e-03 2 KSP Residual norm 5.120833590957e-09 Residual norm 1.91909e-08 Error norm 0.00130992 Error norm 0.000338481 Error norm 1.31699e-06  # Another variant full multigrid preconditioning ! ./ex34 -pc_type mg -pc_mg_type full -ksp_type fgmres -ksp_monitor_short -pc_mg_levels 3 -mg_coarse_pc_factor_shift_type nonzero   0 KSP Residual norm 1.00731 1 KSP Residual norm 0.0510812 2 KSP Residual norm 0.00248709 3 KSP Residual norm 0.000165921 4 KSP Residual norm 1.1586e-05 5 KSP Residual norm 8.71845e-07 Residual norm 8.71845e-07 Error norm 0.0208751 Error norm 0.00618516 Error norm 0.000197005  # For ex45, compare the number of iterations without precoditioning: ! ./ex45 -pc_type none -da_grid_x 21 -da_grid_y 21 -da_grid_z 21 -ksp_monitor   0 KSP Residual norm 1.470306455035e+01 1 KSP Residual norm 2.526523006237e+00 2 KSP Residual norm 1.199024543393e+00 3 KSP Residual norm 8.017624157084e-01 4 KSP Residual norm 5.850738300493e-01 5 KSP Residual norm 4.643372450285e-01 6 KSP Residual norm 3.794775861442e-01 7 KSP Residual norm 3.182229782482e-01 8 KSP Residual norm 2.707869730107e-01 9 KSP Residual norm 2.342221169435e-01 10 KSP Residual norm 2.044268946887e-01 11 KSP Residual norm 1.799290014681e-01 12 KSP Residual norm 1.597128452355e-01 13 KSP Residual norm 1.424463131478e-01 14 KSP Residual norm 1.286048456000e-01 15 KSP Residual norm 1.180539437186e-01 16 KSP Residual norm 1.097826197330e-01 17 KSP Residual norm 1.006546027975e-01 18 KSP Residual norm 8.528703754785e-02 19 KSP Residual norm 6.502594142087e-02 20 KSP Residual norm 5.023918850795e-02 21 KSP Residual norm 4.014387264317e-02 22 KSP Residual norm 2.976949998851e-02 23 KSP Residual norm 2.038487027792e-02 24 KSP Residual norm 1.483308034344e-02 25 KSP Residual norm 1.094830085637e-02 26 KSP Residual norm 7.449788171631e-03 27 KSP Residual norm 5.269131329764e-03 28 KSP Residual norm 3.594369080540e-03 29 KSP Residual norm 2.262888004918e-03 30 KSP Residual norm 1.493039224295e-03 31 KSP Residual norm 1.107124599084e-03 32 KSP Residual norm 7.286598548293e-04 33 KSP Residual norm 4.716912759260e-04 34 KSP Residual norm 3.214892159593e-04 35 KSP Residual norm 2.214075669479e-04 36 KSP Residual norm 1.645224575275e-04 37 KSP Residual norm 1.190806015370e-04 Residual norm 0.000119081  # With the ones with preconditioning: !./ex45 -da_grid_x 21 -da_grid_y 21 -da_grid_z 21 -pc_type mg -pc_mg_levels 3 -mg_levels_ksp_type richardson -mg_levels_ksp_max_it 1 -mg_levels_pc_type bjacobi -ksp_monitor   0 KSP Residual norm 9.713869141172e+01 1 KSP Residual norm 1.457128977402e+00 2 KSP Residual norm 7.197915243881e-02 3 KSP Residual norm 6.946697263348e-04 Residual norm 6.67463e-05  Introduction to libCEED libCEED is a low-level API library for the efficient high-order discretization methods developed by the ECP co-design Center for Efficient Exascale Discretizations (CEED).\nWhile our focus is on high-order finite elements, the approach is mostly algebraic and thus applicable to other discretizations in factored form, as explained in the API documentation portion of the Doxygen documentation.\nClone or download libCEED by running\n! git clone https://github.com/CEED/libCEED.git  # then compile it by running ! make -C libCEED -B  make: Entering directory '/home/jovyan/libCEED' make: 'lib' with optional backends: /cpu/self/ref/memcheck /cpu/self/avx/serial /cpu/self/avx/blocked CC \u001b[38;5;177;1mbuild/interface\u001b[m/ceed-fortran.o CC \u001b[38;5;177;1mbuild/interface\u001b[m/ceed-basis.o CC \u001b[38;5;177;1mbuild/interface\u001b[m/ceed-elemrestriction.o CC \u001b[38;5;177;1mbuild/interface\u001b[m/ceed-operator.o CC \u001b[38;5;177;1mbuild/interface\u001b[m/ceed-vec.o CC \u001b[38;5;177;1mbuild/interface\u001b[m/ceed.o CC \u001b[38;5;177;1mbuild/interface\u001b[m/ceed-tensor.o CC \u001b[38;5;177;1mbuild/interface\u001b[m/ceed-qfunction.o CC \u001b[38;5;85;1mbuild/gallery/identity\u001b[m/ceed-identity.o CC \u001b[38;5;93;1mbuild/gallery/poisson3d\u001b[m/ceed-poisson3dapply.o CC \u001b[38;5;93;1mbuild/gallery/poisson3d\u001b[m/ceed-poisson3dbuild.o CC \u001b[38;5;155;1mbuild/gallery/mass1d\u001b[m/ceed-massapply.o CC \u001b[38;5;155;1mbuild/gallery/mass1d\u001b[m/ceed-mass1dbuild.o CC \u001b[38;5;41;1mbuild/gallery/mass2d\u001b[m/ceed-mass2dbuild.o CC \u001b[38;5;47;1mbuild/gallery/poisson1d\u001b[m/ceed-poisson1dapply.o CC \u001b[38;5;47;1mbuild/gallery/poisson1d\u001b[m/ceed-poisson1dbuild.o CC \u001b[38;5;67;1mbuild/gallery/mass3d\u001b[m/ceed-mass3dbuild.o CC \u001b[38;5;211;1mbuild/gallery/poisson2d\u001b[m/ceed-poisson2dapply.o CC \u001b[38;5;211;1mbuild/gallery/poisson2d\u001b[m/ceed-poisson2dbuild.o CC \u001b[38;5;39;1mbuild/backends/ref\u001b[m/ceed-ref-basis.o CC \u001b[38;5;39;1mbuild/backends/ref\u001b[m/ceed-ref-operator.o CC \u001b[38;5;39;1mbuild/backends/ref\u001b[m/ceed-ref-qfunction.o CC \u001b[38;5;39;1mbuild/backends/ref\u001b[m/ceed-ref-restriction.o CC \u001b[38;5;39;1mbuild/backends/ref\u001b[m/ceed-ref-tensor.o CC \u001b[38;5;39;1mbuild/backends/ref\u001b[m/ceed-ref-vec.o CC \u001b[38;5;39;1mbuild/backends/ref\u001b[m/ceed-ref.o CC \u001b[38;5;63;1mbuild/backends/blocked\u001b[m/ceed-blocked-operator.o CC \u001b[38;5;63;1mbuild/backends/blocked\u001b[m/ceed-blocked.o CC \u001b[38;5;93;1mbuild/backends/opt\u001b[m/ceed-opt-blocked.o CC \u001b[38;5;93;1mbuild/backends/opt\u001b[m/ceed-opt-operator.o CC \u001b[38;5;93;1mbuild/backends/opt\u001b[m/ceed-opt-serial.o CC \u001b[38;5;55;1mbuild/backends/memcheck\u001b[m/ceed-memcheck-qfunction.o CC \u001b[38;5;55;1mbuild/backends/memcheck\u001b[m/ceed-memcheck.o CC \u001b[38;5;89;1mbuild/backends/avx\u001b[m/ceed-avx-blocked.o CC \u001b[38;5;89;1mbuild/backends/avx\u001b[m/ceed-avx-serial.o CC \u001b[38;5;89;1mbuild/backends/avx\u001b[m/ceed-avx-tensor.o LINK \u001b[38;5;97;1mlib\u001b[m/libceed.so make: Leaving directory '/home/jovyan/libCEED'  We are going to look at some libCEED's examples that use some PETSc's capabilities (e.g., process partitioning and geometry handling).\nCheck out my branch for the demo where I made a couple of changes to print more info for the tutorial.\n%%bash cd libCEED # checkout my branch for the demo git checkout valeria/CUHPSC-demo cd ~/ # And compile the examples by running make -C libCEED/examples/petsc PETSC_DIR=$HOME/petsc-3.12.0 PETSC_ARCH=mpich-dbg -B # Link them from the current directory to make it easy to run below cp -sf libCEED/examples/petsc/bpsraw . cp -sf libCEED/examples/petsc/multigrid .   To run the example solving the Poisson's equation on a structured grid, use\n! ./bpsraw -ceed /cpu/self/ref/serial -problem bp3 -degree 1 -local 10000  -- CEED Benchmark Problem 3 -- libCEED + PETSc -- libCEED: libCEED Backend : /cpu/self/ref/serial Mesh: Number of 1D Basis Nodes (p) : 2 Number of 1D Quadrature Points (q) : 3 Global nodes : 11466 Process Decomposition : 1 1 1 Local Elements : 10000 = 20 20 25 Owned nodes : 11466 = 21 21 26 KSP: KSP Type : cg KSP Convergence : CONVERGED_RTOL Total KSP Iterations : 2 Final rnorm : 9.710169e-15 Performance: CG Solve Time : 0.144075 sec DoFs/Sec in CG : 0.159167 million Pointwise Error (max) : 2.079708e-02  See what happens when you run this in parallel, let's say with 2 processes\n! mpiexec -n 2 ./bpsraw -ceed /cpu/self/ref/serial -problem bp3 -degree 1 -local 10000  -- CEED Benchmark Problem 3 -- libCEED + PETSc -- libCEED: libCEED Backend : /cpu/self/ref/serial Mesh: Number of 1D Basis Nodes (p) : 2 Number of 1D Quadrature Points (q) : 3 Global nodes : 22386 Process Decomposition : 2 1 1 Local Elements : 10000 = 20 20 25 Owned nodes : 10920 = 20 21 26 KSP: KSP Type : cg KSP Convergence : CONVERGED_RTOL Total KSP Iterations : 2 Final rnorm : 1.327753e-14 Performance: CG Solve Time : 0.229866 sec DoFs/Sec in CG : 0.194774 million Pointwise Error (max) : 1.999130e-02  Instead, you can keep the total amount of work roughly constant, when you request more processes, but divide the local size of the problem so that each process works roughly the same. For instance, compare\n! ./bpsraw -ceed /cpu/self/ref/serial -problem bp3 -degree 1 -local 10000  -- CEED Benchmark Problem 3 -- libCEED + PETSc -- libCEED: libCEED Backend : /cpu/self/ref/serial Mesh: Number of 1D Basis Nodes (p) : 2 Number of 1D Quadrature Points (q) : 3 Global nodes : 11466 Process Decomposition : 1 1 1 Local Elements : 10000 = 20 20 25 Owned nodes : 11466 = 21 21 26 KSP: KSP Type : cg KSP Convergence : CONVERGED_RTOL Total KSP Iterations : 2 Final rnorm : 9.710169e-15 Performance: CG Solve Time : 0.148991 sec DoFs/Sec in CG : 0.153915 million Pointwise Error (max) : 2.079708e-02  with\n! mpiexec -n 4 ./bpsraw -ceed /cpu/self/ref/serial -problem bp3 -degree 1 -local 2500  -- CEED Benchmark Problem 3 -- libCEED + PETSc -- libCEED: libCEED Backend : /cpu/self/ref/serial Mesh: Number of 1D Basis Nodes (p) : 2 Number of 1D Quadrature Points (q) : 3 Global nodes : 11500 Process Decomposition : 2 2 1 Local Elements : 2508 = 11 12 19 Owned nodes : 2640 = 11 12 20 KSP: KSP Type : cg KSP Convergence : CONVERGED_RTOL Total KSP Iterations : 2 Final rnorm : 9.925692e-15 Performance: CG Solve Time : 0.041265 sec DoFs/Sec in CG : 0.557373 million Pointwise Error (max) : 2.854310e-02   The multigrid example This example solves the same problem, but by using a preconditioning strategy. We use Chebchev as the smoother (solver) with Jacobi as the preconditioner for the smoother.\nRun\n! ./multigrid -ceed /cpu/self/ref/serial -problem bp3 -cells 1000  -- CEED Benchmark Problem 3 -- libCEED + PETSc + PCMG -- libCEED: libCEED Backend : /cpu/self/ref/serial Mesh: Number of 1D Basis Nodes (p) : 3 Number of 1D Quadrature Points (q) : 4 Global Nodes : 49975 Owned Nodes : 49975 Multigrid: Number of Levels : 2 Level 0 (coarse): Number of 1D Basis Nodes (p) : 2 Global Nodes : 3996 Owned Nodes : 3996 Level 1 (fine): Number of 1D Basis Nodes (p) : 3 Global Nodes : 49975 Owned Nodes : 49975 KSP: KSP Type : cg KSP Convergence : CONVERGED_RTOL Total KSP Iterations : 26 Final rnorm : 3.394703e-12 PCMG: PCMG Type : MULTIPLICATIVE PCMG Cycle Type : v Performance: Pointwise Error (max) : 4.270543e-02 CG Solve Time : 19.3821 sec DoFs/Sec in CG : 0.0670388 million  and\n! mpiexec -n 4 ./multigrid -ceed /cpu/self/ref/serial -problem bp3 -cells 1000  -- CEED Benchmark Problem 3 -- libCEED + PETSc + PCMG -- libCEED: libCEED Backend : /cpu/self/ref/serial Mesh: Number of 1D Basis Nodes (p) : 3 Number of 1D Quadrature Points (q) : 4 Global Nodes : 49975 Owned Nodes : 6995 Multigrid: Number of Levels : 2 Level 0 (coarse): Number of 1D Basis Nodes (p) : 2 Global Nodes : 3996 Owned Nodes : 0 Level 1 (fine): Number of 1D Basis Nodes (p) : 3 Global Nodes : 49975 Owned Nodes : 6995 KSP: KSP Type : cg KSP Convergence : CONVERGED_RTOL Total KSP Iterations : 26 Final rnorm : 3.387644e-12 PCMG: PCMG Type : MULTIPLICATIVE PCMG Cycle Type : v Performance: Pointwise Error (max) : 4.270543e-02 CG Solve Time : 33.2205 sec DoFs/Sec in CG : 0.0391129 million  What do you see?\nNow let's raise the degree (accuracy of solution). This will also increase the number of neighboring points we need information from, i.e., the number of nodes.\nCompare\n! ./multigrid -ceed /cpu/self/ref/serial -problem bp3 -degree 8  -- CEED Benchmark Problem 3 -- libCEED + PETSc + PCMG -- libCEED: libCEED Backend : /cpu/self/ref/serial Mesh: Number of 1D Basis Nodes (p) : 9 Number of 1D Quadrature Points (q) : 10 Global Nodes : 12167 Owned Nodes : 12167 Multigrid: Number of Levels : 8 Level 0 (coarse): Number of 1D Basis Nodes (p) : 2 Global Nodes : 8 Owned Nodes : 8 Level 7 (fine): Number of 1D Basis Nodes (p) : 9 Global Nodes : 12167 Owned Nodes : 12167 KSP: KSP Type : cg KSP Convergence : CONVERGED_RTOL Total KSP Iterations : 6 Final rnorm : 3.132892e-11 PCMG: PCMG Type : MULTIPLICATIVE PCMG Cycle Type : v Performance: Pointwise Error (max) : 4.525195e-08 CG Solve Time : 1.60019 sec DoFs/Sec in CG : 0.0456207 million  With\n! ./multigrid -ceed /cpu/self/ref/serial -problem bp3 -degree 8 -coarsen logarithmic  -- CEED Benchmark Problem 3 -- libCEED + PETSc + PCMG -- libCEED: libCEED Backend : /cpu/self/ref/serial Mesh: Number of 1D Basis Nodes (p) : 9 Number of 1D Quadrature Points (q) : 10 Global Nodes : 12167 Owned Nodes : 12167 Multigrid: Number of Levels : 4 Level 0 (coarse): Number of 1D Basis Nodes (p) : 2 Global Nodes : 8 Owned Nodes : 8 Level 3 (fine): Number of 1D Basis Nodes (p) : 9 Global Nodes : 12167 Owned Nodes : 12167 KSP: KSP Type : cg KSP Convergence : CONVERGED_RTOL Total KSP Iterations : 7 Final rnorm : 1.970523e-11 PCMG: PCMG Type : MULTIPLICATIVE PCMG Cycle Type : v Performance: Pointwise Error (max) : 4.525156e-08 CG Solve Time : 0.912013 sec DoFs/Sec in CG : 0.0933857 million  Without specifying a coarsening strategy, it defaults to -coarsen uniform. This way, the domain is partitioned from finest grid to coarsest grid in a linear fashion, i.e., for -degree 8, we run all intermediate levels given by\n8-\u0026gt;7-\u0026gt;6-\u0026gt;5-\u0026gt;...-\u0026gt;2-\u0026gt;1\nInstead, when we use -coarsen logarithmic we have fewer subdivisions, using only powers of 2 as intermediate levels\n8-\u0026gt;4-\u0026gt;2-\u0026gt;1\n Collect your experiments data and try to plot the accuracy gained (given by the error, when the actual solution is available, otherwise by the digits of precision gained) vs time to solve\n ","date":1572007765,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572010989,"objectID":"6f0e85eee93327a1361d23c3e0f7861f","permalink":"https://cucs-hpsc.github.io/fall2019/libceed/","publishdate":"2019-10-25T06:49:25-06:00","relpermalink":"/fall2019/libceed/","section":"fall2019","summary":"Guest lecture on libCEED from Dr. Valeria Barra: slides.\nHPSC Lab 9 2019-10-25\nWe are going to run some PETSc examples and consider them as \u0026quot;baseline\u0026quot; for the libCEED examples that will follow.\n%%bash # You may need to change these for your machine PETSC_DIR=$HOME/petsc-3.12.0 PETSC_ARCH=mpich-dbg # Build the examples make -C $PETSC_DIR -f gmakefile $PETSC_ARCH/tests/ksp/ksp/examples/tutorials/ex34 make -C $PETSC_DIR -f gmakefile $PETSC_ARCH/tests/ksp/ksp/examples/tutorials/ex45 # Link them from the current directory to make it easy to run below cp -sf $PETSC_DIR/$PETSC_ARCH/tests/ksp/ksp/examples/tutorials/ex34 .","tags":null,"title":"libCEED","type":"docs"},{"authors":null,"categories":null,"content":"%matplotlib inline import pandas import seaborn import matplotlib.pyplot as plt import numpy as np plt.style.use('ggplot') plt.rc('figure', figsize=(12,8))  Recap: Nonlinear solver costs  for each nonlinear iteration  compute residual $F(u)$ assemble matrix $J(u) = F'(u)$ setup preconditioner $M^{-1}(J, \\dotsc)$ for each linear iteration apply preconditioner $M^{-1} v$ apply Jacobian $J v$ Krylov vector work/inner products   Shifting costs  Approximate Jacobian Less expensive preconditioner -pc_type jacobi Lag the preconditioner -snes_lag_preconditioner Lag the Jacobian -snes_lag_jacobian Matrix-free finite differencing to skip assembly of Jacobian -snes_mf or -snes_mf_operator  A good PETSc example to test with: src/snes/examples/tutorials/ex15.c\nTransient problems When solving time-dependent problems such as\n\\[ \\dot u := \\frac{\\partial u(x,t)}{\\partial t} = f\\big(u(x,t), t\\big) \\]\nwe have a choice between explicit methods and implicit methods. Explicit methods only require that we evaluate $f(u,t)$ on each time step while implicit methods require solves (linear or nonlinear). To get a handle on this choice, we have to understand stability and stiffness. First, we consider an explicit method for a scalar model problem,\n\\[ \\dot u = -k (u - \\cos t) \\]\nwhere $k$ is a parameter controlling the rate at which the solution $u(t)$ is pulled toward the curve $\\cos t$.\ndef ode_euler(f, u0, tfinal=1, h=0.1): u = np.array(u0) t = 0 thist = [t] uhist = [u0] while t \u0026lt; tfinal: h = min(h, tfinal - t) u += h * f(t, u) t += h thist.append(t) uhist.append(u.copy()) return np.array(thist), np.array(uhist) tests = [] class fcos: def __init__(self, k=5): self.k = k def __repr__(self): return 'fcos(k={:d})'.format(self.k) def f(self, t, u): return -self.k * (u - np.cos(t)) def u(self, t, u0): k2p1 = self.k**2+1 return (u0 - self.k**2/k2p1) * np.exp(np.array(-self.k*t)) + self.k*(np.sin(t) + self.k*np.cos(t))/k2p1 tests.append(fcos(k=2)) tests.append(fcos(k=20)) u0 = np.array([.2]) for test in tests: thist, uhist = ode_euler(test.f, u0, h=.021, tfinal=6) plt.plot(thist, uhist, '.', label=repr(test)+' Forward Euler') plt.plot(thist, test.u(thist, u0), label=repr(test)+' exact') plt.plot(thist, np.cos(thist), label='cos') plt.legend(loc='upper right');  \nAs we increase the parameter $k$, we observe a greater separation of scales between the fast relaxation (characteristic time $1/k$) and the slow dynamics dictated by cos(t), which has characteristic time 1. After the initial transient, we could approximate the solution accurately with large time steps, but the Forward Euler method that we're using here is unstable. We'll now investigate why.\nLinear Stability Analysis Why did forward Euler diverge and how can we characterize methods that enable long time steps? We can answer this by considering the test equation \\( \\dot u = \\lambda u \\) and applying each method to construct \\( \\tilde u(h) = R(h\\lambda) u(0) \\) where $R(z)$ is called the stability function.\n\\[\\begin{align} \\tilde u(h) \u0026= u(0) + h \\lambda u(0) \u0026 R(z) \u0026= 1+z \u0026 \\text{Forward Euler} \\\\ \\tilde u(h) \u0026= u(0) + h \\lambda \\tilde u(h) \u0026 R(z) \u0026= \\frac{1}{1-z} \u0026 \\text{Backward Euler} \\\\ \\tilde u(h) \u0026= u(0) + h \\lambda \\frac{u(0) + \\tilde u(h)}{2} \u0026 R(z) \u0026= \\frac{1+z/2}{1-z/2} \u0026 \\text{Midpoint} \\end{align}\\]\n We can take $n$ steps via  \\[\\tilde u(hn) = \\big( R(h \\lambda) \\big)^n u(0) \\]\n Under what condition does this cause the solution to blow up?  def plot_stability(x, y, Rz, label): plt.figure() C = plt.contourf(xx, yy, np.abs(Rz), np.arange(0,2,.1), cmap=plt.cm.coolwarm) plt.colorbar(C, ticks=np.linspace(0, 2, 5)) plt.axvline(x=0, linewidth=1, color='grey') plt.axhline(y=0, linewidth=1, color='grey') plt.contour(xx, yy, np.abs(Rz), np.arange(0,2,.5), colors='k') plt.title(label) x = np.linspace(-2,2) xx, yy = np.meshgrid(x, x) zz = xx + 1j*yy Rlist = [('Forward Euler', 1+zz), ('Backward Euler', 1/(1-zz)), ('Midpoint', (1+zz/2)/(1-zz/2))] for Rlabel, R in Rlist: plot_stability(xx, yy, R, Rlabel)  \n\n\nWe can capture all of these methods in a family known as the $\\theta$ method.\nWe'll model our cosine decay equation as\n\\[ \\dot u = \\underbrace{A}_{-k} u + \\underbrace{s(t)}_{k \\cos t} \\]\ndef ode_theta_linear(A, u0, source, bcs=[], tfinal=1, h=0.1, theta=.5): u = u0.copy() t = 0. thist = [t] uhist = [u0] A = np.array(A, ndmin=2) I = np.eye(*A.shape) while t \u0026lt; tfinal: if tfinal - t \u0026lt; 1.01*h: h = tfinal - t tnext = tfinal else: tnext = t + h h = min(h, tfinal - t) rhs = (I + (1-theta)*h*A) @ u + h*source(t+theta*h) for i, f in bcs: rhs[i] = theta*h*f(t+theta*h, x[i]) u = np.linalg.solve(I - theta*h*A, rhs) t = tnext thist.append(t) uhist.append(u.copy()) return np.array(thist), np.array(uhist) test = fcos(k=5000) u0 = np.array([.2]) hist = ode_theta_linear(-test.k, u0, lambda t: test.k*np.cos(t), h=.1, tfinal=6, theta=.5) plt.plot(hist[0], hist[1], 'o') tt = np.linspace(0, 6, 200) plt.plot(tt, test.u(tt,u0));  \nObservations  $\\theta=1$ is robust $\\theta=1/2$ gets correct long-term behavior, but has oscillations at early times $\\theta \u0026lt; 1/2$ allows oscillations to grow  We say that a problem is stiff when it has multiple time scales and the slower time scales can be approximated using larger steps than would be needed to resolve the fast scales. This usually means that explicit methods are limited by stability rather than accuracy, and we should choose implicit methods to efficiently approximate the slow/long-term dynamics. Examples of stiff problems:\n Diffusion Elastic waves in quasi-static simulation Chemical reactions Low-Mach flows*  class flinear: def __init__(self, A): self.A = A.copy() def f(self, t, u): return self.A @ u def u(self, t, u0): from scipy.linalg import expm return expm(self.A*t) @ u0 def mms_error(h, theta): test = flinear(np.array([[0, 5], [-5, 0]])) u0 = np.array([1, 0]) thist, uhist = ode_theta_linear(test.A, u0, lambda t: 0*u0, h=h, tfinal=3, theta=theta) return np.linalg.norm(uhist[-1] - test.u(thist[-1], u0), np.inf) hs = np.geomspace(.002, 1, 20) for theta in [0, .5, 0.6, 1]: errors = [mms_error(h, theta) for h in hs] plt.loglog(hs, errors, 'o', label=f'numerical $\\\\theta = {theta}$') for p in range(1,4): plt.loglog(hs, hs**p, label='$h^{%d}$'%p) plt.loglog(hs, 0*hs+1, 'k') plt.xlabel('h') plt.ylabel('error') plt.legend(loc='lower right') plt.ylim(top=10);  \n Only $\\theta = 0.5$ converges at second order (slope matches the $h^2$ line). $\\theta=0$ becomes unstable at a step size where $\\theta=1$ already has very poor accuracy, but $\\theta=0.5$ is less than 10% error. All stable methods have $O(1)$ error for sufficiently large steps. High order explicit methods (beyond the scope of this class, but widely available) could be much more accurate for sufficiently small steps, but would still not be stable with arbitrarily large time steps.  Automatic step size control Some dynamical systems have rapid transitions that need short time steps, but long periods of slow dynamics in which large steps are possible. Adaptive controllers can automatically choose step sizes for efficiency gains. An example from chemical kinetics, using petsc4py:\n\nEvaluating accuracy and performance Each method has an order of accuracy\n error is $O(h^p)$  larger $p$ is important if you need lots of accuracy you might not need extreme accuracy accuracy might be limited by other approximations (collisions, spatial discretization, ...)  multi-stage vs multi-step  Runge-Kutta methods take larger steps, but need to do work on each \u0026quot;stage\u0026quot; Rosenbrock methods can amortize costs over the stages  Newton and Krylov methods might not converge (or converge more slowly) when taking steps that are too long  maybe need better preconditioners, multigrid, etc. or choose smaller time steps revisit formulation  adaptive error control  mostly heuristic, assuming asymptotic range controller stability   \nFurther reading  Hairer, Nørsett, Solving Ordinary Differential Equations I: Nonstiff Problems Hairer and Wanner, Solving Ordinary Differential Equations II: Stiff and Differential-Algebraic Problems  ","date":1571834965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571856849,"objectID":"f77f6aeda69ab68a7efd2736c251e60d","permalink":"https://cucs-hpsc.github.io/fall2019/transient/","publishdate":"2019-10-23T06:49:25-06:00","relpermalink":"/fall2019/transient/","section":"fall2019","summary":"%matplotlib inline import pandas import seaborn import matplotlib.pyplot as plt import numpy as np plt.style.use('ggplot') plt.rc('figure', figsize=(12,8))  Recap: Nonlinear solver costs  for each nonlinear iteration  compute residual $F(u)$ assemble matrix $J(u) = F'(u)$ setup preconditioner $M^{-1}(J, \\dotsc)$ for each linear iteration apply preconditioner $M^{-1} v$ apply Jacobian $J v$ Krylov vector work/inner products   Shifting costs  Approximate Jacobian Less expensive preconditioner -pc_type jacobi Lag the preconditioner -snes_lag_preconditioner Lag the Jacobian -snes_lag_jacobian Matrix-free finite differencing to skip assembly of Jacobian -snes_mf or -snes_mf_operator  A good PETSc example to test with: src/snes/examples/tutorials/ex15.","tags":null,"title":"Transient Problems","type":"docs"},{"authors":null,"categories":null,"content":"%matplotlib inline import pandas import seaborn import matplotlib.pyplot as plt import numpy as np plt.style.use('ggplot')  Nonlinear problems Up to now, we have been solving linear problems. The preferred way to leverage fast linear solves for (potentially ill-conditioned) nonlinear problems is via defect correction, usually Newton methods.\nThe Newton-Raphson method for scalar problems Much of numerical analysis reduces to Taylor series, the approximation \\( f(x) = f(x_0) + f'(x_0) (x-x_0) + \\underbrace{f''(x_0) (x - x_0)^2 / 2 + \\dotsb}_{O((x-x_0)^2)} \\) centered on some reference point $x_0$.\nIn numerical computation, it is exceedingly rare to look beyond the first-order approximation \\( \\tilde f_{x_0}(x) = f(x_0) + f'(x_0)(x - x_0) . \\) Since $\\tilde f_{x0}(x)$ is a linear function, we can explicitly compute the unique solution of $\\tilde f{x_0}(x) = 0$ as \\( x = x_0 - f(x_0) / f'(x_0) . \\) This is Newton's Method (aka Newton-Raphson or Newton-Raphson-Simpson) for finding the roots of differentiable functions.\ndef newton(func, x, verbose=False): \u0026quot;\u0026quot;\u0026quot;Solve f(x) = 0 using initial guess x. The provided function func must return a pair of values, f(x) and its derivative f'(x). For example, to solve the equation x^2 - 3 starting from initial guess x=1, one would write def func(x): return x**2 - 3, 2*x newton(func, 1) \u0026quot;\u0026quot;\u0026quot; for i in range(100): fx, dfx = func(x) if verbose: print(func.__name__, i, x, fx) if np.abs(fx) \u0026lt; 1e-12: return x, fx, i try: x -= fx / dfx except ZeroDivisionError: return x, np.NaN, i def test_func(x): f = np.exp(x) - np.cos(x) - 1 dfdx = np.exp(x) + np.sin(x) return f, dfdx x0 = -2 root, _, _ = newton(test_func, x0, verbose=1) x = np.linspace(min(x0,root)-1, max(x0,root+1)) plt.plot(x, test_func(x)[0]) plt.plot([x0,root], test_func([x0,root])[0], 'ok');  test_func 0 -2 -0.44851788021624484 test_func 1 -2.579508809224632 -0.07804240445653787 test_func 2 -2.7502278755485423 -0.01169737888867406 test_func 3 -2.787065713793899 -0.0005874789197288788 test_func 4 -2.7891231086081634 -1.8550336560174685e-06 test_func 5 -2.7891296463678903 -1.874356225783913e-11 test_func 6 -2.7891296464339503 0.0  \n We say this method converges quadratically since the number of correct digits doubles each iteration. The initial guess matters; a bad initial guess can take us to the wrong solution or cause divergence.  Systems of equations We've been solving linear systems, such as those resulting from discretizing linear PDE. To address nonlinear problems (be they from PDE or otherwise), we'll express our problem as \\( F(u) = 0 \\) where $u$ is a vector of state variables and $F(u)$ is a vector of residuals of the same length. Note that linear problems can be written in this form, $F(u) = A u - b$. We will primarily be interested in defect correction methods of the form \\begin{gather} A \\delta u = - F(u) u \\gets u + \\gamma \\delta u \\end{gather} where $A$ is a matrix and $\\gamma$ is a scalar parameter that may need to be found using an iteration.\n If $A = I$, this is a Richardson iteration, which is related to gradient descent. Such methods are usually quite slow unless $F(u)$ is especially \u0026quot;nice\u0026quot;. If $A = \\partial F/\\partial u$, this is a Newton method and $\\gamma=1$ can often be used.  Newton-Raphson methods for systems The Jacobian of $F$ is \\( J(u) = \\frac{\\partial F}{\\partial u}(u) = \\begin{bmatrix} \\frac{\\partial F_0}{\\partial u_0} \u0026 \\frac{\\partial F_0}{\\partial u_1} \u0026 \\dotsb \\\\ \\frac{\\partial F_1}{\\partial u_0} \u0026 \\frac{\\partial F_1}{\\partial u_1} \u0026 \\\\ \\vdots \u0026 \u0026 \\ddots \\end{bmatrix}(u) . \\) The method can be derived by taking the Taylor expansion of $F(u)$ at $u$, \\( F(u + \\delta u) = F(u) + \\frac{\\partial F}{\\partial u}(u) (\\delta u) + \\frac{\\partial^2 F}{\\partial u^2}(u) (\\delta u \\otimes \\delta u) / 2 + \\dotsb \\) Note that each higher term is a higher rank tensor, thus computationally unweildy. If we truncate the series with the linear term and set equal to zero, we have a linear equation for $\\delta u$ \\( \\frac{\\partial F}{\\partial u}(u) \\delta u = - F(u) \\) which will hopefully make $F(u + \\partial u) \\approx 0$. This is Newton's method.\n Each iteration requires evaluating $F(u)$ -- almost any method will have this property. Each iteration requires evaluating the Jacobian matrix $J(u)$ -- this either requires custom code, algorithmic differentiation, or a finite difference approximation (we'll revisit this later). Each iteration requires solving a linear system with the matrix $J(u)$. This may be expensive.  def fsolve_newton(F, J, u0, rtol=1e-10, maxit=50, verbose=False): u = u0.copy() Fu = F(u) norm0 = np.linalg.norm(Fu) enorm_last = np.linalg.norm(u - np.array([1,1])) for i in range(maxit): du = -np.linalg.solve(J(u), Fu) u += du Fu = F(u) norm = np.linalg.norm(Fu) if verbose: enorm = np.linalg.norm(u - np.array([1,1])) print('Newton {:d} anorm {:6.2e} rnorm {:6.2e} eratio {:6.2f}'. format(i+1, norm, norm/norm0, enorm/enorm_last**2)) enorm_last = enorm if norm \u0026lt; rtol * norm0: break return u, i def rostest(a,b): def F(u): x = u[0]; y = u[1] return np.array([-2*(a-x) + 4*b*x**3 - 4*b*x*y, 2*b*(y-x**2)]) def J(u): x = u[0]; y = u[1] return np.array([[2 + 12*b*x**2 - 4*b*y, -4*b*x], [-4*b*x, 2*b]]) return F, J F, J = rostest(1,3) fsolve_newton(F, J, np.array([0, 1.]), verbose=True)  Newton 1 anorm 2.51e+00 rnorm 3.96e-01 eratio 1.56 Newton 2 anorm 9.91e+00 rnorm 1.57e+00 eratio 0.56 Newton 3 anorm 3.83e-01 rnorm 6.05e-02 eratio 0.22 Newton 4 anorm 5.11e-01 rnorm 8.08e-02 eratio 0.25 Newton 5 anorm 5.24e-04 rnorm 8.28e-05 eratio 0.36 Newton 6 anorm 9.76e-07 rnorm 1.54e-07 eratio 0.21 Newton 7 anorm 3.61e-15 rnorm 5.72e-16 eratio 0.31 (array([1., 1.]), 6)   Can the iteration break down? How? How does the method depend on the initial guess? It turns out that Newton's method has locally quadratic convergence to simple roots, \\(\\lim_{i \\to \\infty} |e_{i+1}|/|e_i^2|  \u0026quot;The number of correct digits doubles each iteration.\u0026quot; Now that we know how to make a good guess accurate, the effort lies in getting a good guess.  Matrix-free Jacobian via finite differencing It can be error-prone and complicated to implement the Jacobian function J(u). In such cases, we can use the approximation\n\\[ J(u) v \\approx \\frac{F(u+\\epsilon v) - F(u)}{\\epsilon} \\]\nwhere $\\epsilon$ is some \u0026quot;small\u0026quot; number. Now can't access individual entries of $J$, but we can apply its action to an arbitrary vector $u$.\nWe know that this approximation is first order accurate in $\\epsilon$, \\( \\left\\lVert J(u) v - \\frac{F(u+\\epsilon v) - F(u)}{\\epsilon} \\right\\rVert \\in O(\\epsilon) . \\) But if $\\epsilon$ is too small, we will lose accuracy due to rounding error. If $F$ has been scaled such that its norm is of order 1, then $\\epsilon = \\sqrt{\\epsilon_{\\text{machine}}}$ is a good default choice.\nimport scipy.sparse.linalg as splinalg def fsolve_newtonkrylov(F, u0, epsilon=1e-8, rtol=1e-10, maxit=50, verbose=False): u = u0.copy() Fu = F(u) norm0 = np.linalg.norm(Fu) for i in range(maxit): def Ju_fd(v): return (F(u + epsilon*v) - Fu) / epsilon Ju = splinalg.LinearOperator((len(Fu),len(u)), matvec=Ju_fd) du, info = splinalg.gmres(Ju, Fu, atol=1.e-6) if info != 0: print(np.linalg.norm(Ju @ du - Fu), norm) raise RuntimeError('GMRES failed to converge: {:d}'.format(info)) u -= du Fu = F(u) norm = np.linalg.norm(Fu) if verbose: print('Newton {:d} anorm {:6.2e} rnorm {:6.2e}' .format(i, norm, norm/norm0)) if norm \u0026lt; rtol * norm0: break return u, i fsolve_newtonkrylov(F, np.array([0.,1]), rtol=1e-6, verbose=True)  Newton 0 anorm 2.51e+00 rnorm 3.96e-01 Newton 1 anorm 9.91e+00 rnorm 1.57e+00 Newton 2 anorm 3.83e-01 rnorm 6.05e-02 Newton 3 anorm 5.11e-01 rnorm 8.08e-02 Newton 4 anorm 5.24e-04 rnorm 8.28e-05 Newton 5 anorm 9.76e-07 rnorm 1.54e-07 (array([1. , 0.99999992]), 5)  Jacobian-Free Newton Krylov (JFNK) and Preconditioning While matrix-free finite differencing can save us the need to assemble the Jacobian (and write code to do that), there is no free lunch: Krylov convergence will be slow unless we have a good preconditioner. But sometimes there are short-cuts: we can assemble a cheaper approximation for preconditioning, or develop multigrid methods that don't involve any assembled matrices.\nFurther reading  Knoll and Keyes (2004) Jacobian-free Newton–Krylov methods: a survey of approaches and applications  Case study: Newton-Krylov Multigrid methods for hydrostatic ice flow This is a strongly nonlinear problem for which convergence of a high-resolution model stagnates if the initial guess isn't good. It is, however, amenable to a method called grid sequencing where we solve coarse problems to generate initial guesses on the fine grid. At each grid level, we solve nonlinear problems using Newton-Krylov methods preconditioned by multigrid.\n\n\n\nFurther reading  Brown, Smith, and Ahmadia (2013) Textbook multigrid efficiency for hydrostatic ice flow  Configuring an efficient nonlinear solver (see snes/examples/tutorials/ex48.c in PETSc repository)   Costs  Residual evaluation $F(u)$ Jacobian assembly $J(u) = F'(u)$ Preconditioner setup $M^{-1}$ Jacobian application $J(u) v$ Preconditioner application $M^{-1} v$ Krylov vector work: inner products and vector axpy  Shifting costs  Approximate Jacobian Less expensive preconditioner -pc_type jacobi Lag the preconditioner -snes_lag_preconditioner Lag the Jacobian -snes_lag_jacobian Matrix-free finite differencing to skip assembly of Jacobian -snes_mf or -snes_mf_operator  A good PETSc example to test with: src/snes/examples/tutorials/ex15.c\n","date":1571662165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571690699,"objectID":"a05e5ff71e9e74b2a82e1517e8866837","permalink":"https://cucs-hpsc.github.io/fall2019/nonlinear/","publishdate":"2019-10-21T06:49:25-06:00","relpermalink":"/fall2019/nonlinear/","section":"fall2019","summary":"%matplotlib inline import pandas import seaborn import matplotlib.pyplot as plt import numpy as np plt.style.use('ggplot')  Nonlinear problems Up to now, we have been solving linear problems. The preferred way to leverage fast linear solves for (potentially ill-conditioned) nonlinear problems is via defect correction, usually Newton methods.\nThe Newton-Raphson method for scalar problems Much of numerical analysis reduces to Taylor series, the approximation \\( f(x) = f(x_0) + f'(x_0) (x-x_0) + \\underbrace{f''(x_0) (x - x_0)^2 / 2 + \\dotsb}_{O((x-x_0)^2)} \\) centered on some reference point $x_0$.","tags":null,"title":"Nonlinear Solvers","type":"docs"},{"authors":null,"categories":null,"content":"Recap: Domain Decomposition convergence theory The formal convergence is beyond the scope of this course, but the following estimates are useful. We let $h$ be the element diameter, $H$ be the subdomain diameter, and $\\delta$ be the overlap, each normalized such that the global domain diameter is 1. We express the convergence in terms of the condition number $\\kappa$ for the preconditioned operator.\n (Block) Jacobi: $\\delta=0$, $\\kappa \\sim H^{-2} H/h = (Hh)^{-1}$ Overlapping Schwarz: $\\kappa \\sim H^{-2} H/\\delta = (H \\delta)^{-1}$ 2-level overlapping Schwarz: $\\kappa \\sim H/\\delta$  Hands-on with PETSc: demonstrate these estimates  Linear Poisson with geometric multigrid: src/ksp/ksp/examples/tutorials/ex29.c Nonlinear problems  Symmetric scalar problem: src/snes/examples/tutorials/ex5.c Nonsymmetric system (lid/thermal-driven cavity): src/snes/examples/tutorials/ex19.c  Compare preconditioned versus unpreconditioned norms. Compare BiCG versus GMRES Compare domain decomposition and multigrid preconditioning  -pc_type asm (Additive Schwarz) -pc_asm_type basic (symmetric, versus restrict) -pc_asm_overlap 2 (increase overlap) Effect of direct subdomain solver: -sub_pc_type lu -pc_type mg (Geometric Multigrid)  Use monitors:  -ksp_monitor_true_residual -ksp_monitor_singular_value -ksp_converged_reason  Explain methods: -snes_view Performance info: -log_view  Example: Inhomogeneous Poisson \\[ -\\nabla\\cdot \\Big( \\rho(x,y) \\nabla u(x,y) \\Big) = e^{-10 (x^2 + y^2)} \\]\nin $\\Omega = [0,1]^2$ with variable conductivity\n\\[ \\rho(x,y) = \\begin{cases} \\rho_0 \u0026 (x,y) \\in [1/3, 2/3]^2 \\\\ 1 \u0026 \\text{otherwise} \\end{cases} \\]\nwhere $\\rho_0 \u0026gt; 0$ is a parameter (with default $\\rho_0 = 1$).\n%%bash # You may need to change these for your machine PETSC_DIR=$HOME/petsc PETSC_ARCH=ompi-optg # Build the example make -C $PETSC_DIR -f gmakefile $PETSC_ARCH/tests/ksp/ksp/examples/tutorials/ex29 # Link it from the current directory to make it easy to run below cp -sf $PETSC_DIR/$PETSC_ARCH/tests/ksp/ksp/examples/tutorials/ex29 .  make: Entering directory '/home/jed/petsc' make: 'ompi-optg/tests/ksp/ksp/examples/tutorials/ex29' is up to date. make: Leaving directory '/home/jed/petsc'  # Prints solution DM and then a coordinate DM ! mpiexec -n 2 ./ex29 -da_refine 2 -dm_view  DM Object: 2 MPI processes type: da Processor [0] M 9 N 9 m 1 n 2 w 1 s 1 X range of indices: 0 9, Y range of indices: 0 5 Processor [1] M 9 N 9 m 1 n 2 w 1 s 1 X range of indices: 0 9, Y range of indices: 5 9 DM Object: 2 MPI processes type: da Processor [0] M 9 N 9 m 1 n 2 w 2 s 1 X range of indices: 0 9, Y range of indices: 0 5 Processor [1] M 9 N 9 m 1 n 2 w 2 s 1 X range of indices: 0 9, Y range of indices: 5 9  ! mpiexec -n 2 ./ex29 -rho 1e-1 -da_refine 3 -ksp_view_solution draw -draw_pause 5 -draw_cmap plasma  This problem is nonsymmetric due to boundary conditions, though symmetric solvers like CG and MINRES may still converge\n! mpiexec -n 2 ./ex29 -rho 1e-1 -da_refine 3 -ksp_monitor_true_residual -ksp_view -ksp_type gmres   0 KSP preconditioned resid norm 1.338744788815e-02 true resid norm 1.433852280437e-02 ||r(i)||/||b|| 1.000000000000e+00 1 KSP preconditioned resid norm 6.105013156491e-03 true resid norm 8.819020609674e-03 ||r(i)||/||b|| 6.150578222039e-01 2 KSP preconditioned resid norm 3.380566739974e-03 true resid norm 3.966597605983e-03 ||r(i)||/||b|| 2.766392089410e-01 3 KSP preconditioned resid norm 2.248884854426e-03 true resid norm 1.950654466953e-03 ||r(i)||/||b|| 1.360429169426e-01 4 KSP preconditioned resid norm 1.603958727893e-03 true resid norm 1.729343487982e-03 ||r(i)||/||b|| 1.206082043163e-01 5 KSP preconditioned resid norm 1.017005335066e-03 true resid norm 1.108652090238e-03 ||r(i)||/||b|| 7.731982613301e-02 6 KSP preconditioned resid norm 5.817999897588e-04 true resid norm 7.954596575686e-04 ||r(i)||/||b|| 5.547709958842e-02 7 KSP preconditioned resid norm 3.102671011646e-04 true resid norm 4.651546500795e-04 ||r(i)||/||b|| 3.244090457755e-02 8 KSP preconditioned resid norm 1.547863442961e-04 true resid norm 2.154582266646e-04 ||r(i)||/||b|| 1.502652885547e-02 9 KSP preconditioned resid norm 7.772941255716e-05 true resid norm 1.166482147907e-04 ||r(i)||/||b|| 8.135302107631e-03 10 KSP preconditioned resid norm 3.800559054824e-05 true resid norm 5.777187067722e-05 ||r(i)||/||b|| 4.029136854992e-03 11 KSP preconditioned resid norm 1.694315416916e-05 true resid norm 3.229096611633e-05 ||r(i)||/||b|| 2.252042735288e-03 12 KSP preconditioned resid norm 6.705763692270e-06 true resid norm 1.252406213904e-05 ||r(i)||/||b|| 8.734555372208e-04 13 KSP preconditioned resid norm 2.308568861148e-06 true resid norm 4.636253434420e-06 ||r(i)||/||b|| 3.233424738152e-04 14 KSP preconditioned resid norm 8.946501825242e-07 true resid norm 1.703002880989e-06 ||r(i)||/||b|| 1.187711526650e-04 15 KSP preconditioned resid norm 2.744515348301e-07 true resid norm 5.751960627589e-07 ||r(i)||/||b|| 4.011543382863e-05 16 KSP preconditioned resid norm 1.137618031844e-07 true resid norm 2.081989399152e-07 ||r(i)||/||b|| 1.452025029048e-05 KSP Object: 2 MPI processes type: gmres restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement happy breakdown tolerance 1e-30 maximum iterations=10000, initial guess is zero tolerances: relative=1e-05, absolute=1e-50, divergence=10000. left preconditioning using PRECONDITIONED norm type for convergence test PC Object: 2 MPI processes type: bjacobi number of blocks = 2 Local solve is same for all blocks, in the following KSP and PC objects: KSP Object: (sub_) 1 MPI processes type: preonly maximum iterations=10000, initial guess is zero tolerances: relative=1e-05, absolute=1e-50, divergence=10000. left preconditioning using NONE norm type for convergence test PC Object: (sub_) 1 MPI processes type: ilu out-of-place factorization 0 levels of fill tolerance for zero pivot 2.22045e-14 matrix ordering: natural factor fill ratio given 1., needed 1. Factored matrix follows: Mat Object: 1 MPI processes type: seqaij rows=153, cols=153 package used to perform factorization: petsc total: nonzeros=713, allocated nonzeros=713 total number of mallocs used during MatSetValues calls =0 not using I-node routines linear system matrix = precond matrix: Mat Object: 1 MPI processes type: seqaij rows=153, cols=153 total: nonzeros=713, allocated nonzeros=713 total number of mallocs used during MatSetValues calls =0 not using I-node routines linear system matrix = precond matrix: Mat Object: 2 MPI processes type: mpiaij rows=289, cols=289 total: nonzeros=1377, allocated nonzeros=1377 total number of mallocs used during MatSetValues calls =0  Default parallel solver  Krylov method: GMRES  restart length of 30 to bound memory requirement and orthogonalization cost classical Gram-Schmidt (compare -ksp_gmres_modifiedgramschmidt) left preconditioning, uses preconditioned norm \\( P^{-1} A x = P^{-1} b \\) -ksp_norm_type unpreconditioned \\( A P^{-1} (P x) = b \\) Can estimate condition number using Hessenberg matrix -ksp_monitor_singular_value -ksp_view_singularvalues Contaminated by restarts, so turn off restart -ksp_gmres_restart 1000 for accurate results  Preconditioner: block Jacobi  Expect condition number to scale with $1/(H h)$ where $H$ is the subdomain diameter and $h$ is the element size One block per MPI process No extra memory to create subdomain problems Create two blocks per process: -pc_bjacobi_local_blocks 2 Each subdomain solver can be configured/monitored using the -sub_ prefix -sub_ksp_type preonly (default) means just apply the preconditioner Incomplete LU factorization with zero fill $O(n)$ cost to compute and apply; same memory as matrix $A$ gets weaker as $n$ increases can fail unpredictably at the worst possible time Allow \u0026quot;levels\u0026quot; of fill: -sub_pc_factor_levels 2 Try -sub_pc_type lu   ! mpiexec -n 2 ./ex29 -rho 1e-1 -da_refine 3 -ksp_monitor -ksp_view -sub_pc_factor_levels 3   0 KSP Residual norm 3.321621226957e-02 1 KSP Residual norm 6.488371997792e-03 2 KSP Residual norm 3.872608843511e-03 3 KSP Residual norm 2.258796172567e-03 4 KSP Residual norm 6.146527388370e-04 5 KSP Residual norm 4.540373464970e-04 6 KSP Residual norm 1.994013489521e-04 7 KSP Residual norm 2.170446909144e-05 8 KSP Residual norm 7.079429242940e-06 9 KSP Residual norm 2.372198219605e-06 10 KSP Residual norm 9.203675161062e-07 11 KSP Residual norm 2.924907588760e-07 KSP Object: 2 MPI processes type: gmres restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement happy breakdown tolerance 1e-30 maximum iterations=10000, initial guess is zero tolerances: relative=1e-05, absolute=1e-50, divergence=10000. left preconditioning using PRECONDITIONED norm type for convergence test PC Object: 2 MPI processes type: bjacobi number of blocks = 2 Local solve is same for all blocks, in the following KSP and PC objects: KSP Object: (sub_) 1 MPI processes type: preonly maximum iterations=10000, initial guess is zero tolerances: relative=1e-05, absolute=1e-50, divergence=10000. left preconditioning using NONE norm type for convergence test PC Object: (sub_) 1 MPI processes type: ilu out-of-place factorization 3 levels of fill tolerance for zero pivot 2.22045e-14 matrix ordering: natural factor fill ratio given 1., needed 2.34642 Factored matrix follows: Mat Object: 1 MPI processes type: seqaij rows=153, cols=153 package used to perform factorization: petsc total: nonzeros=1673, allocated nonzeros=1673 total number of mallocs used during MatSetValues calls =0 not using I-node routines linear system matrix = precond matrix: Mat Object: 1 MPI processes type: seqaij rows=153, cols=153 total: nonzeros=713, allocated nonzeros=713 total number of mallocs used during MatSetValues calls =0 not using I-node routines linear system matrix = precond matrix: Mat Object: 2 MPI processes type: mpiaij rows=289, cols=289 total: nonzeros=1377, allocated nonzeros=1377 total number of mallocs used during MatSetValues calls =0  Scaling estimates Dependence on $h$ ! mpiexec -n 16 --oversubscribe ./ex29 -da_refine 3 -sub_pc_type lu -ksp_gmres_restart 1000 -ksp_converged_reason -ksp_view_singularvalues  Linear solve converged due to CONVERGED_RTOL iterations 20 Iteratively computed extreme singular values: max 1.9384 min 0.0694711 max/min 27.9023  %%bash for refine in {4..8}; do mpiexec -n 16 --oversubscribe ./ex29 -da_refine $refine -sub_pc_type lu -ksp_gmres_restart 1000 -ksp_converged_reason -ksp_view_singularvalues done  Linear solve converged due to CONVERGED_RTOL iterations 27 Iteratively computed extreme singular values: max 1.98356 min 0.0338842 max/min 58.5395 Linear solve converged due to CONVERGED_RTOL iterations 36 Iteratively computed extreme singular values: max 2.04703 min 0.0167502 max/min 122.209 Linear solve converged due to CONVERGED_RTOL iterations 47 Iteratively computed extreme singular values: max 2.12834 min 0.00830794 max/min 256.182 Linear solve converged due to CONVERGED_RTOL iterations 62 Iteratively computed extreme singular values: max 2.1865 min 0.00412757 max/min 529.731 Linear solve converged due to CONVERGED_RTOL iterations 82 Iteratively computed extreme singular values: max 2.22724 min 0.00206119 max/min 1080.56  %%bash for refine in {3..8}; do mpiexec -n 16 --oversubscribe ./ex29 -da_refine $refine -pc_type asm -sub_pc_type lu -ksp_gmres_restart 1000 -ksp_converged_reason -ksp_view_singularvalues done  Linear solve converged due to CONVERGED_RTOL iterations 12 Iteratively computed extreme singular values: max 1.39648 min 0.183011 max/min 7.63057 Linear solve converged due to CONVERGED_RTOL iterations 16 Iteratively computed extreme singular values: max 1.68852 min 0.0984075 max/min 17.1584 Linear solve converged due to CONVERGED_RTOL iterations 23 Iteratively computed extreme singular values: max 1.8569 min 0.0494302 max/min 37.5661 Linear solve converged due to CONVERGED_RTOL iterations 31 Iteratively computed extreme singular values: max 1.9503 min 0.0247646 max/min 78.7537 Linear solve converged due to CONVERGED_RTOL iterations 41 Iteratively computed extreme singular values: max 2.03979 min 0.0123563 max/min 165.081 Linear solve converged due to CONVERGED_RTOL iterations 54 Iteratively computed extreme singular values: max 2.12275 min 0.00615712 max/min 344.764  %%bash cat \u0026gt; results.csv \u0026lt;\u0026lt;EOF method,refine,its,cond bjacobi,3,20,27.90 bjacobi,4,27,58.54 bjacobi,5,36,122.2 bjacobi,6,47,256.2 bjacobi,7,62,529.7 bjacobi,8,82,1080.6 asm,3,12,7.63 asm,4,16,17.15 asm,5,23,37.57 asm,6,31,78.75 asm,7,41,165.1 asm,8,54,344.8 EOF  %matplotlib inline import pandas import seaborn import matplotlib.pyplot as plt plt.style.use('seaborn') df = pandas.read_csv('results.csv') n1 = 2**(df.refine + 1) # number of points per dimension df['P'] = 16 # number of processes df['N'] = n1**2 # number of dofs in global problem df['h'] = 1/n1 df['H'] = 0.25 # 16 procs = 4x4 process grid df['1/Hh'] = 1/(df.H * df.h)  seaborn.lmplot(x='1/Hh', y='cond', hue='method', data=df) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    method refine its cond P N h H 1/Hh     0 bjacobi 3 20 27.90 16 256 0.062500 0.25 64.0   1 bjacobi 4 27 58.54 16 1024 0.031250 0.25 128.0   2 bjacobi 5 36 122.20 16 4096 0.015625 0.25 256.0   3 bjacobi 6 47 256.20 16 16384 0.007812 0.25 512.0   4 bjacobi 7 62 529.70 16 65536 0.003906 0.25 1024.0   5 bjacobi 8 82 1080.60 16 262144 0.001953 0.25 2048.0   6 asm 3 12 7.63 16 256 0.062500 0.25 64.0   7 asm 4 16 17.15 16 1024 0.031250 0.25 128.0   8 asm 5 23 37.57 16 4096 0.015625 0.25 256.0   9 asm 6 31 78.75 16 16384 0.007812 0.25 512.0   10 asm 7 41 165.10 16 65536 0.003906 0.25 1024.0   11 asm 8 54 344.80 16 262144 0.001953 0.25 2048.0     \nimport numpy as np df['1/sqrt(Hh)'] = np.sqrt(df['1/Hh']) plt.rc('figure', figsize=(16, 9)) g = seaborn.lmplot(x='1/sqrt(Hh)', y='its', hue='method', data=df);  \nfrom scipy.stats import linregress bjacobi = df[df.method == 'bjacobi'] asm = df[df.method == 'asm'] bjacobi_its = linregress(bjacobi['1/sqrt(Hh)'], bjacobi['its']) asm_its = linregress(asm['1/sqrt(Hh)'], asm['its']) bjacobi_its, asm_its  (LinregressResult(slope=1.649535193425483, intercept=8.498251134591065, rvalue=0.998633733566655, pvalue=2.7987507565052237e-06, stderr=0.043157836341681514), LinregressResult(slope=1.130138246216531, intercept=4.034979240523391, rvalue=0.9973956316037952, pvalue=1.0165269744723222e-05, stderr=0.04086178847687418))  Cost Let $n = N/P$ be the subdomain size and suppose $k$ iterations are needed.\n Matrix assembly scales like $O(n)$ (perfect parallelism) 2D factorization in each subdomain scales as $O(n^{3/2})$ Preconditioner application scales like $O(n \\log n)$ Matrix multiplication scales like $O(n)$ GMRES scales like $O(k^2 n) + O(k^2 \\log P)$  With restart length $r \\ll k$, GMRES scales with $O(krn) + O(kr\\log P)$   ! mpiexec -n 2 --oversubscribe ./ex29 -da_refine 8 -pc_type asm -sub_pc_type lu -ksp_converged_reason -log_view  Linear solve converged due to CONVERGED_RTOL iterations 25 ************************************************************************************************************************ *** WIDEN YOUR WINDOW TO 120 CHARACTERS. Use 'enscript -r -fCourier9' to print this document *** ************************************************************************************************************************ ---------------------------------------------- PETSc Performance Summary: ---------------------------------------------- ./ex29 on a ompi-optg named joule.int.colorado.edu with 2 processors, by jed Wed Oct 16 10:57:30 2019 Using Petsc Development GIT revision: v3.12-32-g78b8d9f084 GIT Date: 2019-10-03 10:45:44 -0500 Max Max/Min Avg Total Time (sec): 1.484e+00 1.000 1.484e+00 Objects: 1.040e+02 1.000 1.040e+02 Flop: 1.432e+09 1.004 1.429e+09 2.857e+09 Flop/sec: 9.647e+08 1.004 9.628e+08 1.926e+09 MPI Messages: 6.200e+01 1.000 6.200e+01 1.240e+02 MPI Message Lengths: 2.524e+05 1.000 4.071e+03 5.048e+05 MPI Reductions: 1.710e+02 1.000 Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract) e.g., VecAXPY() for real vectors of length N --\u0026gt; 2N flop and VecAXPY() for complex vectors of length N --\u0026gt; 8N flop Summary of Stages: ----- Time ------ ----- Flop ------ --- Messages --- -- Message Lengths -- -- Reductions -- Avg %Total Avg %Total Count %Total Avg %Total Count %Total 0: Main Stage: 1.4839e+00 100.0% 2.8574e+09 100.0% 1.240e+02 100.0% 4.071e+03 100.0% 1.630e+02 95.3% ------------------------------------------------------------------------------------------------------------------------ See the 'Profiling' chapter of the users' manual for details on interpreting output. Phase summary info: Count: number of times phase was executed Time and Flop: Max - maximum over all processors Ratio - ratio of maximum to minimum over all processors Mess: number of messages sent AvgLen: average message length (bytes) Reduct: number of global reductions Global: entire computation Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop(). %T - percent time in this phase %F - percent flop in this phase %M - percent messages in this phase %L - percent message lengths in this phase %R - percent reductions in this phase Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors) ------------------------------------------------------------------------------------------------------------------------ Event Count Time (sec) Flop --- Global --- --- Stage ---- Total Max Ratio Max Ratio Max Ratio Mess AvgLen Reduct %T %F %M %L %R %T %F %M %L %R Mflop/s ------------------------------------------------------------------------------------------------------------------------ --- Event Stage 0: Main Stage BuildTwoSided 5 1.0 1.5282e-02 1.7 0.00e+00 0.0 4.0e+00 4.0e+00 0.0e+00 1 0 3 0 0 1 0 3 0 0 0 BuildTwoSidedF 4 1.0 1.1949e-0217.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 MatMult 25 1.0 2.8539e-02 1.0 2.96e+07 1.0 5.0e+01 4.1e+03 0.0e+00 2 2 40 41 0 2 2 40 41 0 2071 MatSolve 26 1.0 2.9259e-01 1.0 3.50e+08 1.0 0.0e+00 0.0e+00 0.0e+00 20 25 0 0 0 20 25 0 0 0 2393 MatLUFactorSym 1 1.0 1.5648e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 10 0 0 0 0 10 0 0 0 0 0 MatLUFactorNum 1 1.0 5.9458e-01 1.1 8.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 60 0 0 0 39 60 0 0 0 2896 MatAssemblyBegin 3 1.0 1.0730e-0282.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 MatAssemblyEnd 3 1.0 9.8794e-03 1.1 0.00e+00 0.0 3.0e+00 1.4e+03 4.0e+00 1 0 2 1 2 1 0 2 1 2 0 MatGetRowIJ 1 1.0 5.0642e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 MatCreateSubMats 1 1.0 3.9036e-02 1.2 0.00e+00 0.0 1.0e+01 7.0e+03 1.0e+00 2 0 8 14 1 2 0 8 14 1 0 MatGetOrdering 1 1.0 8.0494e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 5 0 0 0 0 5 0 0 0 0 0 MatIncreaseOvrlp 1 1.0 1.5691e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00 1 0 0 0 1 1 0 0 0 1 0 KSPSetUp 2 1.0 2.8898e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+01 0 0 0 0 6 0 0 0 0 6 0 KSPSolve 1 1.0 1.2704e+00 1.0 1.43e+09 1.0 1.0e+02 4.1e+03 1.1e+02 86100 82 83 64 86100 82 83 67 2249 KSPGMRESOrthog 25 1.0 8.4230e-02 1.0 1.71e+08 1.0 0.0e+00 0.0e+00 2.5e+01 6 12 0 0 15 6 12 0 0 15 4062 DMCreateMat 1 1.0 6.0364e-02 1.0 0.00e+00 0.0 3.0e+00 1.4e+03 6.0e+00 4 0 2 1 4 4 0 2 1 4 0 SFSetGraph 5 1.0 3.0582e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 SFSetUp 5 1.0 3.2978e-02 1.5 0.00e+00 0.0 1.2e+01 1.4e+03 0.0e+00 2 0 10 3 0 2 0 10 3 0 0 SFBcastOpBegin 51 1.0 7.6917e-03 1.0 0.00e+00 0.0 1.0e+02 4.1e+03 0.0e+00 1 0 82 83 0 1 0 82 83 0 0 SFBcastOpEnd 51 1.0 1.0617e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 1 0 0 0 0 1 0 0 0 0 0 SFReduceBegin 26 1.0 5.9807e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 SFReduceEnd 26 1.0 5.0625e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 VecMDot 25 1.0 4.1009e-02 1.0 8.57e+07 1.0 0.0e+00 0.0e+00 2.5e+01 3 6 0 0 15 3 6 0 0 15 4171 VecNorm 26 1.0 6.5928e-03 1.3 6.86e+06 1.0 0.0e+00 0.0e+00 2.6e+01 0 0 0 0 15 0 0 0 0 16 2076 VecScale 26 1.0 2.2696e-03 1.0 3.43e+06 1.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 3015 VecCopy 1 1.0 1.2067e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 VecSet 85 1.0 6.4445e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 VecAXPY 1 1.0 1.7286e-04 1.0 2.64e+05 1.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 3045 VecMAXPY 26 1.0 4.5977e-02 1.0 9.23e+07 1.0 0.0e+00 0.0e+00 0.0e+00 3 6 0 0 0 3 6 0 0 0 4007 VecAssemblyBegin 2 1.0 1.3040e-03 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 VecAssemblyEnd 2 1.0 4.9600e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 VecScatterBegin 129 1.0 2.7052e-02 1.0 0.00e+00 0.0 1.0e+02 4.1e+03 0.0e+00 2 0 82 83 0 2 0 82 83 0 0 VecScatterEnd 77 1.0 1.5437e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 1 0 0 0 0 1 0 0 0 0 0 VecNormalize 26 1.0 8.8965e-03 1.2 1.03e+07 1.0 0.0e+00 0.0e+00 2.6e+01 1 1 0 0 15 1 1 0 0 16 2307 PCSetUp 2 1.0 8.5827e-01 1.0 8.64e+08 1.0 1.3e+01 5.7e+03 7.0e+00 58 60 10 15 4 58 60 10 15 4 2006 PCSetUpOnBlocks 1 1.0 7.9431e-01 1.0 8.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00 53 60 0 0 0 53 60 0 0 0 2168 PCApply 26 1.0 3.3956e-01 1.0 3.50e+08 1.0 5.2e+01 4.1e+03 0.0e+00 23 25 42 42 0 23 25 42 42 0 2062 PCApplyOnBlocks 26 1.0 2.9531e-01 1.0 3.50e+08 1.0 0.0e+00 0.0e+00 0.0e+00 20 25 0 0 0 20 25 0 0 0 2371 ------------------------------------------------------------------------------------------------------------------------ Memory usage is given in bytes: Object Type Creations Destructions Memory Descendants' Mem. Reports information only for process 0. --- Event Stage 0: Main Stage Krylov Solver 2 2 20056 0. DMKSP interface 1 1 664 0. Matrix 5 5 105275836 0. Distributed Mesh 3 3 15760 0. Index Set 17 17 5309508 0. IS L to G Mapping 3 3 2119704 0. Star Forest Graph 11 11 10648 0. Discrete System 3 3 2856 0. Vector 50 50 45457728 0. Vec Scatter 5 5 4008 0. Preconditioner 2 2 2000 0. Viewer 2 1 848 0. ======================================================================================================================== Average time to get PetscTime(): 3.32e-08 Average time for MPI_Barrier(): 1.404e-06 Average time for zero size MPI_Send(): 8.8545e-06 #PETSc Option Table entries: -da_refine 8 -ksp_converged_reason -log_view -malloc_test -pc_type asm -sub_pc_type lu #End of PETSc Option Table entries Compiled without FORTRAN kernels Compiled with full precision matrices (default) sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4 Configure options: --download-ctetgen --download-exodusii --download-hypre --download-ml --download-mumps --download-netcdf --download-pnetcdf --download-scalapack --download-sundials --download-superlu --download-superlu_dist --download-triangle --with-debugging=0 --with-hdf5 --with-med --with-metis --with-mpi-dir=/home/jed/usr/ccache/ompi --with-parmetis --with-suitesparse --with-x --with-zlib COPTFLAGS=\u0026quot;-O2 -march=native -ftree-vectorize -g\u0026quot; PETSC_ARCH=ompi-optg ----------------------------------------- Libraries compiled on 2019-10-03 21:38:02 on joule Machine characteristics: Linux-5.3.1-arch1-1-ARCH-x86_64-with-arch Using PETSc directory: /home/jed/petsc Using PETSc arch: ompi-optg ----------------------------------------- Using C compiler: /home/jed/usr/ccache/ompi/bin/mpicc -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -O2 -march=native -ftree-vectorize -g Using Fortran compiler: /home/jed/usr/ccache/ompi/bin/mpif90 -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O ----------------------------------------- Using include paths: -I/home/jed/petsc/include -I/home/jed/petsc/ompi-optg/include -I/home/jed/usr/ccache/ompi/include ----------------------------------------- Using C linker: /home/jed/usr/ccache/ompi/bin/mpicc Using Fortran linker: /home/jed/usr/ccache/ompi/bin/mpif90 Using libraries: -Wl,-rpath,/home/jed/petsc/ompi-optg/lib -L/home/jed/petsc/ompi-optg/lib -lpetsc -Wl,-rpath,/home/jed/petsc/ompi-optg/lib -L/home/jed/petsc/ompi-optg/lib -Wl,-rpath,/usr/lib/openmpi -L/usr/lib/openmpi -Wl,-rpath,/usr/lib/gcc/x86_64-pc-linux-gnu/9.1.0 -L/usr/lib/gcc/x86_64-pc-linux-gnu/9.1.0 -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lsuperlu_dist -lml -lsundials_cvode -lsundials_nvecserial -lsundials_nvecparallel -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lmedC -lmed -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lparmetis -lmetis -ltriangle -lm -lz -lX11 -lctetgen -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lquadmath -lstdc++ -ldl -----------------------------------------  Suggested exercises  There is no substitute for experimentation. Try some different methods or a different example. How do the constants and scaling compare? Can you estimate parameters to model the leading costs for this solver?  In your model, how does degrees of freedom solved per second per process depend on discretization size $h$? What would be optimal?    An approach to modeling We have conducted:\n a few experiments to validate the mathematical estimates for condition number and number of iterations  even when the theorem exists, we need to confirm that we're in the asymptotic regime execution time does not matter at all for this; it's checking the math/convergence rates indeed, we ran oversubscribed (more processes than cores)  one performance experiment on representative hardware (-log_view)  this was on my laptop, but could have been on a couple nodes of a cluster   The performance experiment(s) enables us to fill in constants to estimate time. These models could be more sophisticated, accounting for cache sizes, message sizes, and the like. We implement this performance model in code, dropping in the timing constants from the experiment above, then run synthetic experiments to map out estimated performance in various scaling modes.\ndef perf_model(h, P, method, regress_its): n1 = 1/h N = n1**2 n = N / P H = 1/np.sqrt(P) its = regress_its.intercept + regress_its.slope / np.sqrt(H*h) nref = (2**9 + 1)**2/2 # 8 levels of refinement from a 3x3 (2x2 element) grid pc_setup = 8.5827e-01 / nref**1.5 * n**1.5 pc_apply = 3.3956e-01 / 26 / (nref * np.log(nref)) * (n * np.log(n)) matmult = 2.8539e-02 / 25 / nref * n gmres = 8.4230e-02 / 25 / nref * n + 30e-6 total = pc_setup + its*(pc_apply + matmult + gmres) return dict(h=h, H=H, n=n, N=N, P=P, method=method, its=its, pc_setup=pc_setup, pc_apply=its*pc_apply, matmult=its*matmult, gmres=its*gmres) mdf = pandas.DataFrame(columns='h H n N P method its pc_setup pc_apply matmult gmres'.split()) mdf.append(perf_model(.5**9, 2, 'asm', asm_its), ignore_index=True)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    h H n N P method its pc_setup pc_apply matmult gmres     0 0.001953 0.707107 131072.0 262144.0 2 asm 34.445514 0.853261 0.447958 0.039168 0.116635     Weak scaling study: fixed subdomain size def make_stats(df): df['total'] = df.pc_setup + df.pc_apply + df.matmult + df.gmres df['cost'] = df.total * df.P df['efficiency'] = df.N / df.cost df['digits_accuracy'] = -np.log10(100*df.h**2) df['log10_time'] = np.log10(df.total) df['log10_cost'] = np.log10(df.cost) mdf = pandas.DataFrame(columns='h H n N P method its pc_setup pc_apply matmult gmres'.split()) for h in [.002]: for scale in np.geomspace(2, 1e3, 10): for method, its in [('asm', asm_its), ('bjacobi', bjacobi_its)]: mdf = mdf.append(perf_model(h/scale, scale**2, method, its), ignore_index=True) make_stats(mdf) plt.rc('figure', figsize=(16,9)) grid = seaborn.scatterplot(x='P', y='total', style='method', hue='digits_accuracy', size='its', sizes=(30,400), data=mdf) grid.axes.set(xscale='log', xlim=(2, 2e6), yscale='log', ylim=(1, 2e3)) mdf   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    h H n N P method its pc_setup pc_apply matmult gmres total cost efficiency digits_accuracy log10_time log10_cost     0 0.001000 0.500000 250000.0 1.000000e+06 4.000000 asm 54.576298 2.24764 1.427934 0.118369 0.350992 4.144935 1.657974e+01 60314.577430 4.000000 0.617518 1.219578   1 0.001000 0.500000 250000.0 1.000000e+06 4.000000 bjacobi 82.267708 2.24764 2.152452 0.178428 0.529081 5.107601 2.043040e+01 48946.656533 4.000000 0.708217 1.310277   2 0.000501 0.250660 250000.0 3.978974e+06 15.915896 asm 104.851598 2.24764 2.743336 0.227410 0.674323 5.892709 9.378775e+01 42425.307545 4.599771 0.770315 1.972146   3 0.000501 0.250660 250000.0 3.978974e+06 15.915896 bjacobi 155.648886 2.24764 4.072396 0.337583 1.001011 7.658630 1.218940e+02 32642.914395 4.599771 0.884151 2.085982   4 0.000251 0.125661 250000.0 1.583223e+07 63.328940 asm 205.137578 2.24764 5.367218 0.444917 1.319283 9.379058 5.939658e+02 26655.127609 5.199542 0.972159 2.773761   5 0.000251 0.125661 250000.0 1.583223e+07 63.328940 bjacobi 302.025008 2.24764 7.902180 0.655054 1.942386 12.747260 8.072705e+02 19612.057407 5.199542 1.105417 2.907019   6 0.000126 0.062996 250000.0 6.299605e+07 251.984210 asm 405.181693 2.24764 10.601171 0.878787 2.605809 16.333407 4.115761e+03 15306.053492 5.799313 1.213077 3.614450   7 0.000126 0.062996 250000.0 6.299605e+07 251.984210 bjacobi 594.006815 2.24764 15.541589 1.288325 3.820182 22.897737 5.769868e+03 10918.109272 5.799313 1.359793 3.761166   8 0.000063 0.031581 250000.0 2.506597e+08 1002.638645 asm 804.217010 2.24764 21.041527 1.744244 5.172088 30.205500 3.028520e+04 8276.638274 6.399084 1.480086 4.481230   9 0.000063 0.031581 250000.0 2.506597e+08 1002.638645 bjacobi 1176.433613 2.24764 30.780200 2.551534 7.565891 43.145266 4.325911e+04 5794.378472 6.399084 1.634933 4.636078   10 0.000032 0.015832 250000.0 9.973683e+08 3989.473198 asm 1600.187362 2.24764 41.867289 3.470602 10.291141 57.876673 2.308974e+05 4319.529584 6.998856 1.762504 5.363419   11 0.000032 0.015832 250000.0 9.973683e+08 3989.473198 bjacobi 2338.221662 2.24764 61.177213 5.071305 15.037594 83.533752 3.332557e+05 2992.802242 6.998856 1.921862 5.522778   12 0.000016 0.007937 250000.0 3.968503e+09 15874.010520 asm 3187.938555 2.24764 83.409198 6.914232 20.502302 113.073373 1.794928e+06 2210.953766 7.598627 2.053360 6.254047   13 0.000016 0.007937 250000.0 3.968503e+09 15874.010520 bjacobi 4655.682804 2.24764 121.811247 10.097582 29.941673 164.098143 2.604896e+06 1523.478544 7.598627 2.215104 6.415790   14 0.000008 0.003979 250000.0 1.579057e+10 63162.276697 asm 6355.083968 2.24764 166.274365 13.783367 40.870879 223.176251 1.409632e+07 1120.190874 8.198398 2.348648 7.149106   15 0.000008 0.003979 250000.0 1.579057e+10 63162.276697 bjacobi 9278.407360 2.24764 242.760175 20.123682 59.671385 324.802883 2.051529e+07 769.697602 8.198398 2.511620 7.312078   16 0.000004 0.001995 250000.0 6.283027e+10 251321.062979 asm 12672.704838 2.24764 331.568547 27.485481 81.500824 442.802492 1.112856e+08 564.585802 8.798169 2.646210 8.046439   17 0.000004 0.001995 250000.0 6.283027e+10 251321.062979 bjacobi 18499.525217 2.24764 484.021428 40.123111 118.974329 645.366508 1.621942e+08 387.376780 8.798169 2.809806 8.210035   18 0.000002 0.001000 250000.0 2.500000e+11 1000000.000000 asm 25274.694404 2.24764 661.286899 54.817589 162.546864 880.898993 8.808990e+08 283.800983 9.397940 2.944926 8.944926   19 0.000002 0.001000 250000.0 2.500000e+11 1000000.000000 bjacobi 36893.226489 2.24764 965.274079 80.016703 237.268082 1284.806505 1.284807e+09 194.581829 9.397940 3.108838 9.108838     \ngrid = seaborn.scatterplot(x='P', y='efficiency', style='method', hue='digits_accuracy', size='its', sizes=(50,400), data=mdf) grid.axes.set(xscale='log', xlim=(3, 2e6));  \nStrong scaling study: fixed global problem size mdf = pandas.DataFrame(columns='h H n N P method its pc_setup pc_apply matmult gmres'.split()) for h in [1e-5]: for scale in np.geomspace(2, 1e3, 10): for method, its in [('asm', asm_its), ('bjacobi', bjacobi_its)]: mdf = mdf.append(perf_model(h, scale**2, method, its), ignore_index=True) make_stats(mdf) grid = seaborn.scatterplot(x='P', y='total', style='method', size='its', sizes=(30,400), data=mdf) grid.axes.set(xscale='log', xlim=(2, 2e6), yscale='log');  \ngrid = seaborn.scatterplot(x='P', y='efficiency', style='method', size='its', sizes=(30,400), data=mdf) grid.axes.set(xscale='log', xlim=(2, 2e6), ylim=0);  \nAccessible range Now we grid sample the space of possible configurations to understand the shape of the Pareto front -- those configurations that are optimal in terms of the multiple objectives, which we take to be\n number of digits of accuracy  scales with $O(h^2)$ for this discretization sometimes we could change the discretization to converge as $O(h^3)$ or a higher power; these are called \u0026quot;higher order methods\u0026quot;  total execution time: how long we have wait for the parallel computation to complete cost: how many node hours are we charged for efficiency: given the required problem size to reach desired accuracy, how well are the node resources being used  we measure here as number of degrees of freedom solved per second per core    All of these metrics have tangible units and none are input parameters. We'll map out the best configurations possible within a class of algorithms and, after identifying a Pareto optimal case that we would like to run, can check the input parameters to see how to run it on a real machine.\nmdf = pandas.DataFrame(columns='h H n N P method its pc_setup pc_apply matmult gmres'.split()) for h in np.geomspace(1e-3, .1, 10): for scale in np.geomspace(2, 1e3, 10): for method, its in [('asm', asm_its), ('bjacobi', bjacobi_its)]: mdf = mdf.append(perf_model(h/scale, scale**2, method, its), ignore_index=True) make_stats(mdf) grid = seaborn.scatterplot(x='P', y='total', style='method', hue='digits_accuracy', size='its', sizes=(30,400), data=mdf) grid.axes.set(xscale='log', xlim=(3, 2e6), yscale='log', ylim=(1e-3, 2e5));  \ngrid = seaborn.scatterplot(x='total', y='efficiency', style='method', hue='digits_accuracy', size='its', sizes=(30,400), data=mdf) grid.axes.set(xscale='log', xlim=(1e-3, 1e4), yscale='log');  \ngrid = seaborn.scatterplot(x='digits_accuracy', y='efficiency', style='method', hue='log10_time', size='log10_cost', sizes=(30,400), data=mdf) grid.axes.set(yscale='log');  \ngrid = seaborn.scatterplot(x='total', y='digits_accuracy', style='method', size='log10_cost', sizes=(30,400), data=mdf) grid.axes.set(xscale='log', xlim=(1, 1e4), ylim=3);  \nTwo-level methods Two-level methods add a coarse space $P_0$ to the preconditioner\n\\[ M^{-1} = P_0 A_0^{-1} P_0^T + \\sum_{i=1}^{\\text{subdomains}} P_i A_i^{-1} P_i^T \\]\nor a similar combination. In most formulations, the size of the coarse problem $A_0 = P_0^T A P_0$ is proportional to the number of subdomains.\nCompare the figures below with those above to see how two-level methods open up new accessible ranges and bring their own tradeoffs.\ndef perf_model_2level(h, P, method, regress_its): n1 = 1/h N = n1**2 n = N / P H = 1/np.sqrt(P) its = regress_its.intercept + regress_its.slope / np.sqrt(H*h) * H nref = (2**9 + 1)**2/2 # 8 levels of refinement from a 3x3 (2x2 element) grid pc_setup = 8.5827e-01 / nref**1.5 * n**1.5 pc_setup += 8.5827e-01 / nref**1.5 * (4*P)**1.5 pc_apply = 3.3956e-01 / 26 / (nref * np.log(nref)) * (n * np.log(n) + 4*P * np.log(4*P)) matmult = 2.8539e-02 / 25 / nref * n gmres = 8.4230e-02 / 25 / nref * n + 30e-6 total = pc_setup + its*(pc_apply + matmult + gmres) return dict(h=h, H=H, n=n, N=N, P=P, method=method, its=its, pc_setup=pc_setup, pc_apply=its*pc_apply, matmult=its*matmult, gmres=its*gmres)  Strong scaling mdf = pandas.DataFrame(columns='h H n N P method its pc_setup pc_apply matmult gmres'.split()) for h in [1e-5]: for scale in np.geomspace(2, 1e3, 10): for method, its in [('asm', asm_its), ('bjacobi', bjacobi_its)]: mdf = mdf.append(perf_model_2lev(h, scale**2, method, its), ignore_index=True) make_stats(mdf) grid = seaborn.scatterplot(x='P', y='efficiency', style='method', size='its', sizes=(30,400), data=mdf) grid.axes.set(xscale='log', xlim=(2, 2e6), ylim=0);  \n Why does efficiency drop off on the left? Why does it drop off on the right?  Accessible range mdf = pandas.DataFrame(columns='h H n N P method its pc_setup pc_apply matmult gmres'.split()) for h in np.geomspace(1e-3, .1, 10): for scale in np.geomspace(2, 1e3, 10): for method, its in [('asm', asm_its), ('bjacobi', bjacobi_its)]: mdf = mdf.append(perf_model_2level(h/scale, scale**2, method, its), ignore_index=True) make_stats(mdf) grid = seaborn.scatterplot(x='P', y='total', style='method', hue='digits_accuracy', size='its', sizes=(30,400), data=mdf) grid.axes.set(xscale='log', xlim=(3, 2e6), yscale='log', ylim=(1e-3, 2e5));  \ngrid = seaborn.scatterplot(x='total', y='efficiency', style='method', hue='digits_accuracy', size='its', sizes=(30,400), data=mdf) grid.axes.set(xscale='log', xlim=(1e-3, 1e4), yscale='log');  \ngrid = seaborn.scatterplot(x='digits_accuracy', y='efficiency', style='method', hue='log10_time', size='log10_cost', sizes=(30,400), data=mdf) grid.axes.set(yscale='log');  \ngrid = seaborn.scatterplot(x='total', y='digits_accuracy', style='method', size='log10_cost', sizes=(30,400), data=mdf) grid.axes.set(xscale='log', xlim=(1, 1e4), ylim=3);  \n","date":1571402965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571429785,"objectID":"b3f6df203c2b229e7a9488d13fc71106","permalink":"https://cucs-hpsc.github.io/fall2019/mg-preconditioning/","publishdate":"2019-10-18T06:49:25-06:00","relpermalink":"/fall2019/mg-preconditioning/","section":"fall2019","summary":"Recap: Domain Decomposition convergence theory The formal convergence is beyond the scope of this course, but the following estimates are useful. We let $h$ be the element diameter, $H$ be the subdomain diameter, and $\\delta$ be the overlap, each normalized such that the global domain diameter is 1. We express the convergence in terms of the condition number $\\kappa$ for the preconditioned operator.\n (Block) Jacobi: $\\delta=0$, $\\kappa \\sim H^{-2} H/h = (Hh)^{-1}$ Overlapping Schwarz: $\\kappa \\sim H^{-2} H/\\delta = (H \\delta)^{-1}$ 2-level overlapping Schwarz: $\\kappa \\sim H/\\delta$  Hands-on with PETSc: demonstrate these estimates  Linear Poisson with geometric multigrid: src/ksp/ksp/examples/tutorials/ex29.","tags":null,"title":"Multilevel Preconditioning","type":"docs"},{"authors":null,"categories":null,"content":"Recap: Domain decomposition theory Given a linear operator $A : V \\to V$, suppose we have a collection of prolongation operators $P_i : V_i \\to V$. The columns of $P_i$ are \u0026quot;basis functions\u0026quot; for the subspace $V_i$. The Galerkin operator $A_i = P_i^T A P_i$ is the action of the original operator $A$ in the subspace.\nDefine the subspace projection\n\\[ S_i = P_i A_i^{-1} P_i^T A . \\]\n $S_i$ is a projection: $S_i^2 = S_i$ If $A$ is SPD, $S_i$ is SPD with respect to the $A$ inner product $x^T A y$ $I - S_i$ is $A$-orthogonal to the range of $P_i$  Note, the concept of $A$-orthogonality is meaningful only when $A$ is SPD. Does the mathematical expression $ P_i^T A (I - S_i) = 0 $ hold even when $A$ is nonsymmetric?\nThese projections may be applied additively\n\\[ I - \\sum_{i=0}^n S_i, \\]\nmultiplicatively\n\\[ \\prod_{i=0}^n (I - S_i), \\]\nor in some hybrid manner, such as\n\\( (I - S_0) (I - \\sum_{i=1}^n S_i) . \\) In each case above, the action is expressed in terms of the error iteration operator.\nExamples  Jacobi corresponds to the additive preconditioner with $P_i$ as the $i$th column of the identity Gauss-Seidel is the multiplicate preconditioner with $P_i$ as the $i$th column of the identity Block Jacobi corresponds to labeling \u0026quot;subdomains\u0026quot; and $P_i$ as the columns of the identity corresponding to non-overlapping subdomains Overlapping Schwarz corresponds to overlapping subdomains $P_i$ are eigenvectors of $A$ A domain is partitioned into interior $V_{I}$ and interface $V_\\Gamma$ degrees of freedom. $P_{I}$ is embedding of the interior degrees of freedom while $P_\\Gamma$ is \u0026quot;harmonic extension\u0026quot; of the interface degrees of freedom. Consider the multiplicative combination $(I - S_\\Gamma)(I - S_{I})$.  Convergence theory The formal convergence is beyond the scope of this course, but the following estimates are useful. We let $h$ be the element diameter, $H$ be the subdomain diameter, and $\\delta$ be the overlap, each normalized such that the global domain diameter is 1. We express the convergence in terms of the condition number $\\kappa$ for the preconditioned operator.\n (Block) Jacobi: $\\delta=0$, $\\kappa \\sim H^{-2} H/h = (Hh)^{-1}$ Overlapping Schwarz: $\\kappa \\sim H^{-2} H/\\delta = (H \\delta)^{-1}$ 2-level overlapping Schwarz: $\\kappa \\sim H/\\delta$  Hands-on with PETSc: demonstrate these estimates  Linear Poisson with geometric multigrid: src/ksp/ksp/examples/tutorials/ex29.c Nonlinear problems  Symmetric scalar problem: src/snes/examples/tutorials/ex5.c Nonsymmetric system (lid/thermal-driven cavity): src/snes/examples/tutorials/ex19.c  Compare preconditioned versus unpreconditioned norms. Compare BiCG versus GMRES Compare domain decomposition and multigrid preconditioning  -pc_type asm (Additive Schwarz) -pc_asm_type basic (symmetric, versus restrict) -pc_asm_overlap 2 (increase overlap) Effect of direct subdomain solver: -sub_pc_type lu -pc_type mg (Geometric Multigrid)  Use monitors:  -ksp_monitor_true_residual -ksp_monitor_singular_value -ksp_converged_reason  Explain methods: -snes_view Performance info: -log_view  Example: Inhomogeneous Poisson \\[ -\\nabla\\cdot \\Big( \\rho(x,y) \\nabla u(x,y) \\Big) = e^{-10 (x^2 + y^2)} \\]\nin $\\Omega = [0,1]^2$ with variable conductivity\n$$\\rho(x,y) = \\begin{cases}\n\\rho_0 \u0026amp; (x,y) \\in [1/3, 2/3]^2 \\\\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$  where $\\rho_0 \u0026gt; 0$ is a parameter (with default $\\rho_0 = 1$).\n%%bash # You may need to change these for your machine PETSC_DIR=$HOME/petsc PETSC_ARCH=ompi-optg # Build the example make -C $PETSC_DIR -f gmakefile $PETSC_ARCH/tests/ksp/ksp/examples/tutorials/ex29 # Link it from the current directory to make it easy to run below cp -sf $PETSC_DIR/$PETSC_ARCH/tests/ksp/ksp/examples/tutorials/ex29 .  make: Entering directory '/home/jed/petsc' make: 'ompi-optg/tests/ksp/ksp/examples/tutorials/ex29' is up to date. make: Leaving directory '/home/jed/petsc'  # Prints solution DM and then a coordinate DM ! mpiexec -n 2 ./ex29 -da_refine 2 -dm_view  DM Object: 2 MPI processes type: da Processor [0] M 9 N 9 m 1 n 2 w 1 s 1 X range of indices: 0 9, Y range of indices: 0 5 Processor [1] M 9 N 9 m 1 n 2 w 1 s 1 X range of indices: 0 9, Y range of indices: 5 9 DM Object: 2 MPI processes type: da Processor [0] M 9 N 9 m 1 n 2 w 2 s 1 X range of indices: 0 9, Y range of indices: 0 5 Processor [1] M 9 N 9 m 1 n 2 w 2 s 1 X range of indices: 0 9, Y range of indices: 5 9  ! mpiexec -n 2 ./ex29 -rho 1e-1 -da_refine 3 -ksp_view_solution draw -draw_pause 5 -draw_cmap plasma  This problem is nonsymmetric due to boundary conditions, though symmetric solvers like CG and MINRES may still converge\n! mpiexec -n 2 ./ex29 -rho 1e-1 -da_refine 3 -ksp_monitor_true_residual -ksp_view -ksp_type gmres   0 KSP preconditioned resid norm 1.338744788815e-02 true resid norm 1.433852280437e-02 ||r(i)||/||b|| 1.000000000000e+00 1 KSP preconditioned resid norm 6.105013156491e-03 true resid norm 8.819020609674e-03 ||r(i)||/||b|| 6.150578222039e-01 2 KSP preconditioned resid norm 3.380566739974e-03 true resid norm 3.966597605983e-03 ||r(i)||/||b|| 2.766392089410e-01 3 KSP preconditioned resid norm 2.248884854426e-03 true resid norm 1.950654466953e-03 ||r(i)||/||b|| 1.360429169426e-01 4 KSP preconditioned resid norm 1.603958727893e-03 true resid norm 1.729343487982e-03 ||r(i)||/||b|| 1.206082043163e-01 5 KSP preconditioned resid norm 1.017005335066e-03 true resid norm 1.108652090238e-03 ||r(i)||/||b|| 7.731982613301e-02 6 KSP preconditioned resid norm 5.817999897588e-04 true resid norm 7.954596575686e-04 ||r(i)||/||b|| 5.547709958842e-02 7 KSP preconditioned resid norm 3.102671011646e-04 true resid norm 4.651546500795e-04 ||r(i)||/||b|| 3.244090457755e-02 8 KSP preconditioned resid norm 1.547863442961e-04 true resid norm 2.154582266646e-04 ||r(i)||/||b|| 1.502652885547e-02 9 KSP preconditioned resid norm 7.772941255716e-05 true resid norm 1.166482147907e-04 ||r(i)||/||b|| 8.135302107631e-03 10 KSP preconditioned resid norm 3.800559054824e-05 true resid norm 5.777187067722e-05 ||r(i)||/||b|| 4.029136854992e-03 11 KSP preconditioned resid norm 1.694315416916e-05 true resid norm 3.229096611633e-05 ||r(i)||/||b|| 2.252042735288e-03 12 KSP preconditioned resid norm 6.705763692270e-06 true resid norm 1.252406213904e-05 ||r(i)||/||b|| 8.734555372208e-04 13 KSP preconditioned resid norm 2.308568861148e-06 true resid norm 4.636253434420e-06 ||r(i)||/||b|| 3.233424738152e-04 14 KSP preconditioned resid norm 8.946501825242e-07 true resid norm 1.703002880989e-06 ||r(i)||/||b|| 1.187711526650e-04 15 KSP preconditioned resid norm 2.744515348301e-07 true resid norm 5.751960627589e-07 ||r(i)||/||b|| 4.011543382863e-05 16 KSP preconditioned resid norm 1.137618031844e-07 true resid norm 2.081989399152e-07 ||r(i)||/||b|| 1.452025029048e-05 KSP Object: 2 MPI processes type: gmres restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement happy breakdown tolerance 1e-30 maximum iterations=10000, initial guess is zero tolerances: relative=1e-05, absolute=1e-50, divergence=10000. left preconditioning using PRECONDITIONED norm type for convergence test PC Object: 2 MPI processes type: bjacobi number of blocks = 2 Local solve is same for all blocks, in the following KSP and PC objects: KSP Object: (sub_) 1 MPI processes type: preonly maximum iterations=10000, initial guess is zero tolerances: relative=1e-05, absolute=1e-50, divergence=10000. left preconditioning using NONE norm type for convergence test PC Object: (sub_) 1 MPI processes type: ilu out-of-place factorization 0 levels of fill tolerance for zero pivot 2.22045e-14 matrix ordering: natural factor fill ratio given 1., needed 1. Factored matrix follows: Mat Object: 1 MPI processes type: seqaij rows=153, cols=153 package used to perform factorization: petsc total: nonzeros=713, allocated nonzeros=713 total number of mallocs used during MatSetValues calls =0 not using I-node routines linear system matrix = precond matrix: Mat Object: 1 MPI processes type: seqaij rows=153, cols=153 total: nonzeros=713, allocated nonzeros=713 total number of mallocs used during MatSetValues calls =0 not using I-node routines linear system matrix = precond matrix: Mat Object: 2 MPI processes type: mpiaij rows=289, cols=289 total: nonzeros=1377, allocated nonzeros=1377 total number of mallocs used during MatSetValues calls =0  Default parallel solver  Krylov method: GMRES  restart length of 30 to bound memory requirement and orthogonalization cost classical Gram-Schmidt (compare -ksp_gmres_modifiedgramschmidt) left preconditioning, uses preconditioned norm \\( P^{-1} A x = P^{-1} b \\) -ksp_norm_type unpreconditioned \\( A P^{-1} (P x) = b \\) Can estimate condition number using Hessenberg matrix -ksp_monitor_singular_value -ksp_view_singularvalues Contaminated by restarts, so turn off restart -ksp_gmres_restart 1000 for accurate results  Preconditioner: block Jacobi  Expect condition number to scale with $1/(H h)$ where $H$ is the subdomain diameter and $h$ is the element size One block per MPI process No extra memory to create subdomain problems Create two blocks per process: -pc_bjacobi_local_blocks 2 Each subdomain solver can be configured/monitored using the -sub_ prefix -sub_ksp_type preonly (default) means just apply the preconditioner Incomplete LU factorization with zero fill $O(n)$ cost to compute and apply; same memory as matrix $A$ gets weaker as $n$ increases can fail unpredictably at the worst possible time Allow \u0026quot;levels\u0026quot; of fill: -sub_pc_factor_levels 2 Try -sub_pc_type lu   ! mpiexec -n 2 ./ex29 -rho 1e-1 -da_refine 3 -ksp_monitor -ksp_view -sub_pc_factor_levels 3   0 KSP Residual norm 3.321621226957e-02 1 KSP Residual norm 6.488371997792e-03 2 KSP Residual norm 3.872608843511e-03 3 KSP Residual norm 2.258796172567e-03 4 KSP Residual norm 6.146527388370e-04 5 KSP Residual norm 4.540373464970e-04 6 KSP Residual norm 1.994013489521e-04 7 KSP Residual norm 2.170446909144e-05 8 KSP Residual norm 7.079429242940e-06 9 KSP Residual norm 2.372198219605e-06 10 KSP Residual norm 9.203675161062e-07 11 KSP Residual norm 2.924907588760e-07 KSP Object: 2 MPI processes type: gmres restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement happy breakdown tolerance 1e-30 maximum iterations=10000, initial guess is zero tolerances: relative=1e-05, absolute=1e-50, divergence=10000. left preconditioning using PRECONDITIONED norm type for convergence test PC Object: 2 MPI processes type: bjacobi number of blocks = 2 Local solve is same for all blocks, in the following KSP and PC objects: KSP Object: (sub_) 1 MPI processes type: preonly maximum iterations=10000, initial guess is zero tolerances: relative=1e-05, absolute=1e-50, divergence=10000. left preconditioning using NONE norm type for convergence test PC Object: (sub_) 1 MPI processes type: ilu out-of-place factorization 3 levels of fill tolerance for zero pivot 2.22045e-14 matrix ordering: natural factor fill ratio given 1., needed 2.34642 Factored matrix follows: Mat Object: 1 MPI processes type: seqaij rows=153, cols=153 package used to perform factorization: petsc total: nonzeros=1673, allocated nonzeros=1673 total number of mallocs used during MatSetValues calls =0 not using I-node routines linear system matrix = precond matrix: Mat Object: 1 MPI processes type: seqaij rows=153, cols=153 total: nonzeros=713, allocated nonzeros=713 total number of mallocs used during MatSetValues calls =0 not using I-node routines linear system matrix = precond matrix: Mat Object: 2 MPI processes type: mpiaij rows=289, cols=289 total: nonzeros=1377, allocated nonzeros=1377 total number of mallocs used during MatSetValues calls =0  Scaling estimates Dependence on $h$ ! mpiexec -n 16 --oversubscribe ./ex29 -da_refine 3 -sub_pc_type lu -ksp_gmres_restart 1000 -ksp_converged_reason -ksp_view_singularvalues  Linear solve converged due to CONVERGED_RTOL iterations 20 Iteratively computed extreme singular values: max 1.9384 min 0.0694711 max/min 27.9023  %%bash for refine in {4..8}; do mpiexec -n 16 --oversubscribe ./ex29 -da_refine $refine -sub_pc_type lu -ksp_gmres_restart 1000 -ksp_converged_reason -ksp_view_singularvalues done  Linear solve converged due to CONVERGED_RTOL iterations 27 Iteratively computed extreme singular values: max 1.98356 min 0.0338842 max/min 58.5395 Linear solve converged due to CONVERGED_RTOL iterations 36 Iteratively computed extreme singular values: max 2.04703 min 0.0167502 max/min 122.209 Linear solve converged due to CONVERGED_RTOL iterations 47 Iteratively computed extreme singular values: max 2.12834 min 0.00830794 max/min 256.182 Linear solve converged due to CONVERGED_RTOL iterations 62 Iteratively computed extreme singular values: max 2.1865 min 0.00412757 max/min 529.731 Linear solve converged due to CONVERGED_RTOL iterations 82 Iteratively computed extreme singular values: max 2.22724 min 0.00206119 max/min 1080.56  %%bash for refine in {3..8}; do mpiexec -n 16 --oversubscribe ./ex29 -da_refine $refine -pc_type asm -sub_pc_type lu -ksp_gmres_restart 1000 -ksp_converged_reason -ksp_view_singularvalues done  Linear solve converged due to CONVERGED_RTOL iterations 12 Iteratively computed extreme singular values: max 1.39648 min 0.183011 max/min 7.63057 Linear solve converged due to CONVERGED_RTOL iterations 16 Iteratively computed extreme singular values: max 1.68852 min 0.0984075 max/min 17.1584 Linear solve converged due to CONVERGED_RTOL iterations 23 Iteratively computed extreme singular values: max 1.8569 min 0.0494302 max/min 37.5661 Linear solve converged due to CONVERGED_RTOL iterations 31 Iteratively computed extreme singular values: max 1.9503 min 0.0247646 max/min 78.7537 Linear solve converged due to CONVERGED_RTOL iterations 41 Iteratively computed extreme singular values: max 2.03979 min 0.0123563 max/min 165.081 Linear solve converged due to CONVERGED_RTOL iterations 54 Iteratively computed extreme singular values: max 2.12275 min 0.00615712 max/min 344.764  %%bash cat \u0026gt; results.csv \u0026lt;\u0026lt;EOF method,refine,its,cond bjacobi,3,20,27.90 bjacobi,4,27,58.54 bjacobi,5,36,122.2 bjacobi,6,47,256.2 bjacobi,7,62,529.7 bjacobi,8,82,1080.6 asm,3,12,7.63 asm,4,16,17.15 asm,5,23,37.57 asm,6,31,78.75 asm,7,41,165.1 asm,8,54,344.8 EOF  %matplotlib inline import pandas import seaborn df = pandas.read_csv('results.csv') n1 = 2**(df.refine + 1) # number of points per dimension df['P'] = 16 # number of processes df['N'] = n1**2 # number of dofs in global problem df['h'] = 1/n1 df['H'] = 0.25 # 16 procs = 4x4 process grid df['1/Hh'] = 1/(df.H * df.h) seaborn.lmplot(x='1/Hh', y='cond', hue='method', data=df) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    method refine its cond P N h H 1/Hh     0 bjacobi 3 20 27.90 16 256 0.062500 0.25 64.0   1 bjacobi 4 27 58.54 16 1024 0.031250 0.25 128.0   2 bjacobi 5 36 122.20 16 4096 0.015625 0.25 256.0   3 bjacobi 6 47 256.20 16 16384 0.007812 0.25 512.0   4 bjacobi 7 62 529.70 16 65536 0.003906 0.25 1024.0   5 bjacobi 8 82 1080.60 16 262144 0.001953 0.25 2048.0   6 asm 3 12 7.63 16 256 0.062500 0.25 64.0   7 asm 4 16 17.15 16 1024 0.031250 0.25 128.0   8 asm 5 23 37.57 16 4096 0.015625 0.25 256.0   9 asm 6 31 78.75 16 16384 0.007812 0.25 512.0   10 asm 7 41 165.10 16 65536 0.003906 0.25 1024.0   11 asm 8 54 344.80 16 262144 0.001953 0.25 2048.0     \nimport numpy as np df['1/sqrt(Hh)'] = np.sqrt(df['1/Hh']) seaborn.lmplot(x='1/sqrt(Hh)', y='its', hue='method', data=df);  \nCost Let $n = N/P$ be the subdomain size and suppose $k$ iterations are needed.\n Matrix assembly scales like $O(n)$ (perfect parallelism) 2D factorization in each subdomain scales as $O(n^{3/2})$ Preconditioner application scales like $O(n \\log n)$ Matrix multiplication scales like $O(n)$ GMRES scales like $O(k^2 n) + O(k^2 \\log P)$  With restart length $r \\ll k$, GMRES scales with $O(krn) + O(kr\\log P)$   ! mpiexec -n 2 --oversubscribe ./ex29 -da_refine 8 -pc_type asm -sub_pc_type lu -ksp_converged_reason -log_view  Linear solve converged due to CONVERGED_RTOL iterations 25 ************************************************************************************************************************ *** WIDEN YOUR WINDOW TO 120 CHARACTERS. Use 'enscript -r -fCourier9' to print this document *** ************************************************************************************************************************ ---------------------------------------------- PETSc Performance Summary: ---------------------------------------------- ./ex29 on a ompi-optg named joule.int.colorado.edu with 2 processors, by jed Wed Oct 16 10:57:30 2019 Using Petsc Development GIT revision: v3.12-32-g78b8d9f084 GIT Date: 2019-10-03 10:45:44 -0500 Max Max/Min Avg Total Time (sec): 1.484e+00 1.000 1.484e+00 Objects: 1.040e+02 1.000 1.040e+02 Flop: 1.432e+09 1.004 1.429e+09 2.857e+09 Flop/sec: 9.647e+08 1.004 9.628e+08 1.926e+09 MPI Messages: 6.200e+01 1.000 6.200e+01 1.240e+02 MPI Message Lengths: 2.524e+05 1.000 4.071e+03 5.048e+05 MPI Reductions: 1.710e+02 1.000 Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract) e.g., VecAXPY() for real vectors of length N --\u0026gt; 2N flop and VecAXPY() for complex vectors of length N --\u0026gt; 8N flop Summary of Stages: ----- Time ------ ----- Flop ------ --- Messages --- -- Message Lengths -- -- Reductions -- Avg %Total Avg %Total Count %Total Avg %Total Count %Total 0: Main Stage: 1.4839e+00 100.0% 2.8574e+09 100.0% 1.240e+02 100.0% 4.071e+03 100.0% 1.630e+02 95.3% ------------------------------------------------------------------------------------------------------------------------ See the 'Profiling' chapter of the users' manual for details on interpreting output. Phase summary info: Count: number of times phase was executed Time and Flop: Max - maximum over all processors Ratio - ratio of maximum to minimum over all processors Mess: number of messages sent AvgLen: average message length (bytes) Reduct: number of global reductions Global: entire computation Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop(). %T - percent time in this phase %F - percent flop in this phase %M - percent messages in this phase %L - percent message lengths in this phase %R - percent reductions in this phase Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors) ------------------------------------------------------------------------------------------------------------------------ Event Count Time (sec) Flop --- Global --- --- Stage ---- Total Max Ratio Max Ratio Max Ratio Mess AvgLen Reduct %T %F %M %L %R %T %F %M %L %R Mflop/s ------------------------------------------------------------------------------------------------------------------------ --- Event Stage 0: Main Stage BuildTwoSided 5 1.0 1.5282e-02 1.7 0.00e+00 0.0 4.0e+00 4.0e+00 0.0e+00 1 0 3 0 0 1 0 3 0 0 0 BuildTwoSidedF 4 1.0 1.1949e-0217.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 MatMult 25 1.0 2.8539e-02 1.0 2.96e+07 1.0 5.0e+01 4.1e+03 0.0e+00 2 2 40 41 0 2 2 40 41 0 2071 MatSolve 26 1.0 2.9259e-01 1.0 3.50e+08 1.0 0.0e+00 0.0e+00 0.0e+00 20 25 0 0 0 20 25 0 0 0 2393 MatLUFactorSym 1 1.0 1.5648e-01 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 10 0 0 0 0 10 0 0 0 0 0 MatLUFactorNum 1 1.0 5.9458e-01 1.1 8.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00 39 60 0 0 0 39 60 0 0 0 2896 MatAssemblyBegin 3 1.0 1.0730e-0282.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 MatAssemblyEnd 3 1.0 9.8794e-03 1.1 0.00e+00 0.0 3.0e+00 1.4e+03 4.0e+00 1 0 2 1 2 1 0 2 1 2 0 MatGetRowIJ 1 1.0 5.0642e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 MatCreateSubMats 1 1.0 3.9036e-02 1.2 0.00e+00 0.0 1.0e+01 7.0e+03 1.0e+00 2 0 8 14 1 2 0 8 14 1 0 MatGetOrdering 1 1.0 8.0494e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 5 0 0 0 0 5 0 0 0 0 0 MatIncreaseOvrlp 1 1.0 1.5691e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00 1 0 0 0 1 1 0 0 0 1 0 KSPSetUp 2 1.0 2.8898e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+01 0 0 0 0 6 0 0 0 0 6 0 KSPSolve 1 1.0 1.2704e+00 1.0 1.43e+09 1.0 1.0e+02 4.1e+03 1.1e+02 86100 82 83 64 86100 82 83 67 2249 KSPGMRESOrthog 25 1.0 8.4230e-02 1.0 1.71e+08 1.0 0.0e+00 0.0e+00 2.5e+01 6 12 0 0 15 6 12 0 0 15 4062 DMCreateMat 1 1.0 6.0364e-02 1.0 0.00e+00 0.0 3.0e+00 1.4e+03 6.0e+00 4 0 2 1 4 4 0 2 1 4 0 SFSetGraph 5 1.0 3.0582e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 SFSetUp 5 1.0 3.2978e-02 1.5 0.00e+00 0.0 1.2e+01 1.4e+03 0.0e+00 2 0 10 3 0 2 0 10 3 0 0 SFBcastOpBegin 51 1.0 7.6917e-03 1.0 0.00e+00 0.0 1.0e+02 4.1e+03 0.0e+00 1 0 82 83 0 1 0 82 83 0 0 SFBcastOpEnd 51 1.0 1.0617e-02 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 1 0 0 0 0 1 0 0 0 0 0 SFReduceBegin 26 1.0 5.9807e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 SFReduceEnd 26 1.0 5.0625e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 VecMDot 25 1.0 4.1009e-02 1.0 8.57e+07 1.0 0.0e+00 0.0e+00 2.5e+01 3 6 0 0 15 3 6 0 0 15 4171 VecNorm 26 1.0 6.5928e-03 1.3 6.86e+06 1.0 0.0e+00 0.0e+00 2.6e+01 0 0 0 0 15 0 0 0 0 16 2076 VecScale 26 1.0 2.2696e-03 1.0 3.43e+06 1.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 3015 VecCopy 1 1.0 1.2067e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 VecSet 85 1.0 6.4445e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 VecAXPY 1 1.0 1.7286e-04 1.0 2.64e+05 1.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 3045 VecMAXPY 26 1.0 4.5977e-02 1.0 9.23e+07 1.0 0.0e+00 0.0e+00 0.0e+00 3 6 0 0 0 3 6 0 0 0 4007 VecAssemblyBegin 2 1.0 1.3040e-03 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 VecAssemblyEnd 2 1.0 4.9600e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 0 0 0 0 0 0 0 0 0 0 0 VecScatterBegin 129 1.0 2.7052e-02 1.0 0.00e+00 0.0 1.0e+02 4.1e+03 0.0e+00 2 0 82 83 0 2 0 82 83 0 0 VecScatterEnd 77 1.0 1.5437e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 1 0 0 0 0 1 0 0 0 0 0 VecNormalize 26 1.0 8.8965e-03 1.2 1.03e+07 1.0 0.0e+00 0.0e+00 2.6e+01 1 1 0 0 15 1 1 0 0 16 2307 PCSetUp 2 1.0 8.5827e-01 1.0 8.64e+08 1.0 1.3e+01 5.7e+03 7.0e+00 58 60 10 15 4 58 60 10 15 4 2006 PCSetUpOnBlocks 1 1.0 7.9431e-01 1.0 8.64e+08 1.0 0.0e+00 0.0e+00 0.0e+00 53 60 0 0 0 53 60 0 0 0 2168 PCApply 26 1.0 3.3956e-01 1.0 3.50e+08 1.0 5.2e+01 4.1e+03 0.0e+00 23 25 42 42 0 23 25 42 42 0 2062 PCApplyOnBlocks 26 1.0 2.9531e-01 1.0 3.50e+08 1.0 0.0e+00 0.0e+00 0.0e+00 20 25 0 0 0 20 25 0 0 0 2371 ------------------------------------------------------------------------------------------------------------------------ Memory usage is given in bytes: Object Type Creations Destructions Memory Descendants' Mem. Reports information only for process 0. --- Event Stage 0: Main Stage Krylov Solver 2 2 20056 0. DMKSP interface 1 1 664 0. Matrix 5 5 105275836 0. Distributed Mesh 3 3 15760 0. Index Set 17 17 5309508 0. IS L to G Mapping 3 3 2119704 0. Star Forest Graph 11 11 10648 0. Discrete System 3 3 2856 0. Vector 50 50 45457728 0. Vec Scatter 5 5 4008 0. Preconditioner 2 2 2000 0. Viewer 2 1 848 0. ======================================================================================================================== Average time to get PetscTime(): 3.32e-08 Average time for MPI_Barrier(): 1.404e-06 Average time for zero size MPI_Send(): 8.8545e-06 #PETSc Option Table entries: -da_refine 8 -ksp_converged_reason -log_view -malloc_test -pc_type asm -sub_pc_type lu #End of PETSc Option Table entries Compiled without FORTRAN kernels Compiled with full precision matrices (default) sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4 Configure options: --download-ctetgen --download-exodusii --download-hypre --download-ml --download-mumps --download-netcdf --download-pnetcdf --download-scalapack --download-sundials --download-superlu --download-superlu_dist --download-triangle --with-debugging=0 --with-hdf5 --with-med --with-metis --with-mpi-dir=/home/jed/usr/ccache/ompi --with-parmetis --with-suitesparse --with-x --with-zlib COPTFLAGS=\u0026quot;-O2 -march=native -ftree-vectorize -g\u0026quot; PETSC_ARCH=ompi-optg ----------------------------------------- Libraries compiled on 2019-10-03 21:38:02 on joule Machine characteristics: Linux-5.3.1-arch1-1-ARCH-x86_64-with-arch Using PETSc directory: /home/jed/petsc Using PETSc arch: ompi-optg ----------------------------------------- Using C compiler: /home/jed/usr/ccache/ompi/bin/mpicc -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -O2 -march=native -ftree-vectorize -g Using Fortran compiler: /home/jed/usr/ccache/ompi/bin/mpif90 -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O ----------------------------------------- Using include paths: -I/home/jed/petsc/include -I/home/jed/petsc/ompi-optg/include -I/home/jed/usr/ccache/ompi/include ----------------------------------------- Using C linker: /home/jed/usr/ccache/ompi/bin/mpicc Using Fortran linker: /home/jed/usr/ccache/ompi/bin/mpif90 Using libraries: -Wl,-rpath,/home/jed/petsc/ompi-optg/lib -L/home/jed/petsc/ompi-optg/lib -lpetsc -Wl,-rpath,/home/jed/petsc/ompi-optg/lib -L/home/jed/petsc/ompi-optg/lib -Wl,-rpath,/usr/lib/openmpi -L/usr/lib/openmpi -Wl,-rpath,/usr/lib/gcc/x86_64-pc-linux-gnu/9.1.0 -L/usr/lib/gcc/x86_64-pc-linux-gnu/9.1.0 -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu -lsuperlu_dist -lml -lsundials_cvode -lsundials_nvecserial -lsundials_nvecparallel -llapack -lblas -lexodus -lnetcdf -lpnetcdf -lmedC -lmed -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lparmetis -lmetis -ltriangle -lm -lz -lX11 -lctetgen -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lquadmath -lstdc++ -ldl -----------------------------------------  Suggested exercises  There is no substitute for experimentation. Try some different methods or a different example. How do the constants and scaling compare? Can you estimate parameters to model the leading costs for this solver?  In your model, how does degrees of freedom solved per second per process depend on discretization size $h$? What would be optimal?   ","date":1571230165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571249764,"objectID":"c23a0baf743c5e16310096176553cad0","permalink":"https://cucs-hpsc.github.io/fall2019/dd-preconditioning-2/","publishdate":"2019-10-16T06:49:25-06:00","relpermalink":"/fall2019/dd-preconditioning-2/","section":"fall2019","summary":"Recap: Domain decomposition theory Given a linear operator $A : V \\to V$, suppose we have a collection of prolongation operators $P_i : V_i \\to V$. The columns of $P_i$ are \u0026quot;basis functions\u0026quot; for the subspace $V_i$. The Galerkin operator $A_i = P_i^T A P_i$ is the action of the original operator $A$ in the subspace.\nDefine the subspace projection\n\\[ S_i = P_i A_i^{-1} P_i^T A . \\]\n $S_i$ is a projection: $S_i^2 = S_i$ If $A$ is SPD, $S_i$ is SPD with respect to the $A$ inner product $x^T A y$ $I - S_i$ is $A$-orthogonal to the range of $P_i$  Note, the concept of $A$-orthogonality is meaningful only when $A$ is SPD.","tags":null,"title":"DD Preconditioning 2","type":"docs"},{"authors":null,"categories":null,"content":"Classical methods We have discussed the Jacobi preconditioner \\( P_{\\text{Jacobi}}^{-1} = D^{-1} \\) where $D$ is the diagonal of $A$. Gauss-Seidel is \\( P_{GS}^{-1} = (L+D)^{-1} \\) where $L$ is the (strictly) lower triangular part of $A$. The upper triangular part may be used instead, or a symmetric form \\( P_{SGS}^{-1} = (D+U)^{-1} D (L+D)^{-1} . \\)\n[Edit after class: fixed mistake in the above formula and added snippet below.]\nAside: Where does $P_{SGS}^{-1}$ come from? Let's take the error iteration matrix for a forward sweep $I - (L+D)^{-1} A$ followed by a backward sweep $I - (D+U)^{-1}A$ and compute\n\\[\\begin{align} I - P_{SGS}^{-1} A \u0026= \\big( I - (D+U)^{-1} A \\big) \\big(I - (L+D)^{-1} A \\big) \\\\ \u0026= I - (D+U)^{-1} A - (L+D)^{-1} A + (D+U)^{-1} A (L+D)^{-1} A \\\\ \u0026= I - \\Big( (D+U)^{-1} + (L+D)^{-1} - (D+U)^{-1} A (L+D)^{-1} \\Big) A \\\\ \u0026= I - \\Big( (D+U)^{-1} + \\underbrace{\\big[I - (D+U)^{-1} A \\big]}_{I - (D+U)^{-1} (D+U+L)} (L+D)^{-1} \\Big) A \\\\ \u0026= I - \\Big( (D+U)^{-1} - (D+U)^{-1} L (L+D)^{-1} \\Big) A \\\\ \u0026= I - \\Big( (D+U)^{-1} - (D+U)^{-1} \\underbrace{(L+D-D) (L+D)^{-1}}_{I - D (L+D)^{-1}} \\Big) A \\\\ \u0026= I - \\underbrace{(D+U)^{-1} D (L+D)^{-1}}_{P_{SGS}^{-1}} A . \\end{align}\\]\nFurther resources  Saad (2003) Iterative Methods: Chapter 4: Basic Iterative Methods  Domain decomposition Suppose we know how to solve problems on \u0026quot;subdomains\u0026quot;, which may overlap.\n\nThis could be possible because they have special structure (e.g., above) or because they are small enough. We want to use this ability to solve the global problem.\nAlternating Schwarz method bc_circle = guess() while not converged: u_circle = solve(A_circle, bc_circle) bc_rectangle = eval(u_circle, rectangle) u_rectangle = solve(A_rectangle, bc_rectangle) bc_circle = eval(u_rectangle, circle)  This method was proposed in 1870 by Hermann Schwarz as a theoretical tool, and is now called a \u0026quot;multiplicative\u0026quot; Schwarz method because the solves depend on each other. We can see it as a generalization of Gauss-Seidel in which we solve on subdomains instead of at individual grid points. As with Gauss-Seidel, it is difficult to expose parallelism.\nAdditive Schwarz methods The additive Schwarz method is more comparable to Jacobi, with each domain solved in parallel. Our fundamental operation will be an embedding of each subdomain $V_i$ into the global domain $V$, which we call the prolongation\n\\[ P_i : V_i \\to V \\]\nThe transpose of prolongation, $P_i^T$, will sometimes be called restriction. Let's work an example.\n%matplotlib inline import numpy as np import matplotlib.pyplot as plt plt.style.use('seaborn') N = 21 x = np.linspace(-1, 1, N) overlap = 0 domains = [(0,N//3+overlap), (N//3-overlap, 2*N//3+overlap), (2*N//3-overlap, N)] P = [] for i, (start, end) in enumerate(domains): P.append(np.eye(N, end-start, -start)) u = 1 + np.cos(3*x) plt.plot(x, u) u1 = P[1].T @ u # Restrict to subdomain 1 plt.plot(P[1].T @ x, u1, 'o') print(P[1].shape)  (21, 7)  \nplt.plot(x, P[1] @ u1, 'o-'); # Prolong to global domain  \n# Define a Laplacian A = np.eye(N) A[1:-1] = (2*np.eye(N-2, N, 1) - np.eye(N-2, N, 0) - np.eye(N-2, N, 2)) / (N-1)**2 A[:5, :5]  array([[ 1. , 0. , 0. , 0. , 0. ], [-0.0025, 0.005 , -0.0025, 0. , 0. ], [ 0. , -0.0025, 0.005 , -0.0025, 0. ], [ 0. , 0. , -0.0025, 0.005 , -0.0025], [ 0. , 0. , 0. , -0.0025, 0.005 ]])  The first and last rows implement boundary conditions; the interior is the familiar centered difference method for the Laplacian.\nA1 = P[1].T @ A @ P[1] A1  array([[ 0.005 , -0.0025, 0. , 0. , 0. , 0. , 0. ], [-0.0025, 0.005 , -0.0025, 0. , 0. , 0. , 0. ], [ 0. , -0.0025, 0.005 , -0.0025, 0. , 0. , 0. ], [ 0. , 0. , -0.0025, 0.005 , -0.0025, 0. , 0. ], [ 0. , 0. , 0. , -0.0025, 0.005 , -0.0025, 0. ], [ 0. , 0. , 0. , 0. , -0.0025, 0.005 , -0.0025], [ 0. , 0. , 0. , 0. , 0. , -0.0025, 0.005 ]])  rhs = np.zeros_like(x) rhs[0] = 1 # Boundary condition u = np.zeros_like(x) # Initial guess for iter in range(5): r = rhs - A @ u # Residual u_next = u.copy() for Pi in P: Ai = Pi.T @ A @ Pi ui = np.linalg.solve(Ai, Pi.T @ r) u_next += Pi @ ui u = u_next plt.plot(x, u, 'o-', label=f'iter {iter}') plt.legend();  \nHands-on Go back and increase overlap to see how it affects convergence.\nTheory Given a linear operator $A : V \\to V$, suppose we have a collection of prolongation operators $P_i : V_i \\to V$. The columns of $P_i$ are \u0026quot;basis functions\u0026quot; for the subspace $V_i$. The Galerkin operator $A_i = P_i^T A P_i$ is the action of the original operator $A$ in the subspace.\nDefine the subspace projection\n\\[ S_i = P_i A_i^{-1} P_i^T A . \\]\n $S_i$ is a projection: $S_i^2 = S_i$ If $A$ is SPD, $S_i$ is SPD with respect to the $A$ inner product $x^T A y$ $I - S_i$ is $A$-orthogonal to the range of $P_i$  S1 = P[1] @ np.linalg.inv(A1) @ P[1].T @ A np.linalg.norm(S1 @ S1 - S1)  1.2457015262873114e-15  I = np.eye(*S1.shape) np.linalg.norm(P[1].T @ A @ (I - S1))  6.9011609626021565e-18  Note, the concept of $A$-orthogonality is meaningful only when $A$ is SPD. Does the mathematical expression $ P_i^T A (I - S_i) = 0 $ hold even when $A$ is nonsymmetric?\nThese projections may be applied additively\n\\[ I - \\sum_{i=0}^n S_i, \\]\nmultiplicatively\n\\[ \\prod_{i=0}^n (I - S_i), \\]\nor in some hybrid manner, such as\n\\( (I - S_0) (I - \\sum_{i=1}^n S_i) . \\) In each case above, the action is expressed in terms of the error iteration operator.\nExamples  Jacobi corresponds to the additive preconditioner with $P_i$ as the $i$th column of the identity Gauss-Seidel is the multiplicate preconditioner with $P_i$ as the $i$th column of the identity Block Jacobi corresponds to labeling \u0026quot;subdomains\u0026quot; and $P_i$ as the columns of the identity corresponding to non-overlapping subdomains Overlapping Schwarz corresponds to overlapping subdomains $P_i$ are eigenvectors of $A$ A domain is partitioned into interior $V_{I}$ and interface $V_\\Gamma$ degrees of freedom. $P_{I}$ is embedding of the interior degrees of freedom while $P_\\Gamma$ is \u0026quot;harmonic extension\u0026quot; of the interface degrees of freedom. Consider the multiplicative combination $(I - S_\\Gamma)(I - S_{I})$.  Convergence theory The formal convergence is beyond the scope of this course, but the following estimates are useful. We let $h$ be the element diameter, $H$ be the subdomain diameter, and $\\delta$ be the overlap, each normalized such that the global domain diameter is 1. We express the convergence in terms of the condition number $\\kappa$ for the preconditioned operator.\n (Block) Jacobi: $\\delta=0$, $\\kappa \\sim H^{-2} H/h = (Hh)^{-1}$ Overlapping Schwarz: $\\kappa \\sim H^{-2} H/\\delta = (H \\delta)^{-1}$ 2-level overlapping Schwarz: $\\kappa \\sim H/\\delta$  Hands-on with PETSc: demonstrate these estimates  Symmetric example: src/snes/examples/tutorials/ex5.c Nonsymmetric example: src/snes/examples/tutorials/ex19.c Compare preconditioned versus unpreconditioned norms. Compare BiCG versus GMRES Compare domain decomposition and multigrid preconditioning  -pc_type asm (Additive Schwarz) -pc_asm_type basic (symmetric, versus restrict) -pc_asm_overlap 2 (increase overlap) Effect of direct subdomain solver: -sub_pc_type lu -pc_type mg (Geometric Multigrid)  Use monitors:  -ksp_monitor_true_residual -ksp_monitor_singular_value -ksp_converged_reason  Explain methods: -snes_view Performance info: -log_view  Examples mpiexec -n 4 ./ex19 -lidvelocity 2 -snes_monitor -da_refine 5 -ksp_monitor -pc_type asm -sub_pc_type lu  ","date":1571057365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571116375,"objectID":"f50a45ecaf1771f117fb4765152adfc3","permalink":"https://cucs-hpsc.github.io/fall2019/dd-preconditioning/","publishdate":"2019-10-14T06:49:25-06:00","relpermalink":"/fall2019/dd-preconditioning/","section":"fall2019","summary":"Classical methods We have discussed the Jacobi preconditioner \\( P_{\\text{Jacobi}}^{-1} = D^{-1} \\) where $D$ is the diagonal of $A$. Gauss-Seidel is \\( P_{GS}^{-1} = (L+D)^{-1} \\) where $L$ is the (strictly) lower triangular part of $A$. The upper triangular part may be used instead, or a symmetric form \\( P_{SGS}^{-1} = (D+U)^{-1} D (L+D)^{-1} . \\)\n[Edit after class: fixed mistake in the above formula and added snippet below.]","tags":null,"title":"Domain Decomposition Preconditioning","type":"docs"},{"authors":null,"categories":null,"content":" Preconditioning Recall that preconditioning is the act of creating an \u0026ldquo;affordable\u0026rdquo; operation \u0026ldquo;$P^{-1}$\u0026rdquo; such that $P^{-1} A$ (or $A P^{-1}$) is is well-conditoned or otherwise has a \u0026ldquo;nice\u0026rdquo; spectrum. We then solve the system\n$$ P^{-1} A x = P^{-1} b \\quad \\text{or}\\quad A P^{-1} \\underbrace{(P x)}_y = b $$\nin which case the convergence rate depends on the spectrum of the iteration matrix $$ I - \\omega P^{-1} A . $$\n The preconditioner must be applied on each iteration. It is not merely about finding a good initial guess.  Classical methods We have discussed the Jacobi preconditioner\n$$ P_{\\text{Jacobi}}^{-1} = D^{-1} $$\nwhere $D$ is the diagonal of $A$. Gauss-Seidel is\n$$ P_{GS}^{-1} = (L+D)^{-1} $$\nwhere $L$ is the (strictly) lower triangular part of $A$. The upper triangular part may be used instead, or a symmetric form\n$$ P_{SGS}^{-1} = (D+U)^{-1} D (L+D)^{-1} . $$\nDomain decomposition Given a linear operator $A : V \\to V$, suppose we have a collection of prolongation operators $P_i : V_i \\to V$. The columns of $P_i$ are \u0026ldquo;basis functions\u0026rdquo; for the subspace $V_i$. The Galerkin operator $A_i = P_i^T A P_i$ is the action of the original operator $A$ in the subspace.\nDefine the subspace projection\n$$ S_i = P_i A_i^{-1} P_i^T A . $$\n $S_i$ is a projection: $S_i^2 = S_i$ If $A$ is SPD, $S_i$ is SPD with respect to the $A$ inner product $x^T A y$ $I - S_i$ is $A$-orthogonal to the range of $P_i$  These projections may be applied additively\n$$ I - \\sum_{i=0}^n S_i, $$\nmultiplicatively\n$$ \\prod_{i=0}^n (I - S_i), $$\nor in some hybrid manner, such as\n$$ (I - S0) (I - \\sum{i=1}^n S_i) . $$ In each case above, the action is expressed in terms of the error iteration operator.\nExamples  Jacobi corresponds to the additive preconditioner with $P_i$ as the $i$th column of the identity Gauss-Seidel is the multiplicate preconditioner with $P_i$ as the $i$th column of the identity Block Jacobi corresponds to labeling \u0026ldquo;subdomains\u0026rdquo; and $P_i$ as the columns of the identity corresponding to non-overlapping subdomains Overlapping Schwarz corresponds to overlapping subdomains $P_i$ are eigenvectors of $A$ A domain is partitioned into interior $VI$ and interface $V\\Gamma$ degrees of freedom. $PI$ is embedding of the interior degrees of freedom while $P\\Gamma$ is \u0026ldquo;harmonic extension\u0026rdquo; of the interface degrees of freedom. Consider the multiplicative combination $(I - S_\\Gamma)(I - S_I)$.  Convergence theory The formal convergence is beyond the scope of this course, but the following estimates are useful. We let $h$ be the element diameter, $H$ be the subdomain diameter, and $\\delta$ be the overlap, each normalized such that the global domain diameter is 1. We express the convergence in terms of the condition number $\\kappa$ for the preconditioned operator.\n (Block) Jacobi: $\\delta=0$, $\\kappa \\sim H^{-2} H/h = (Hh)^{-1}$ Overlapping Schwarz: $\\kappa \\sim H^{-2} H/\\delta = (H \\delta)^{-1}$ 2-level overlapping Schwarz: $\\kappa \\sim H/\\delta$  Hands-on with PETSc: demonstrate these estimates  Symmetric example: src/snes/examples/tutorials/ex5.c Nonsymmetric example: src/snes/examples/tutorials/ex19.c Compare preconditioned versus unpreconditioned norms. Compare BiCG versus GMRES Compare domain decomposition and multigrid preconditioning  -pc_type asm (Additive Schwarz) -pc_asm_type basic (symmetric, versus restrict) -pc_asm_overlap 2 (increase overlap) Effect of direct subdomain solver: -sub_pc_type lu -pc_type mg (Geometric Multigrid)  Use monitors:  -ksp_monitor_true_residual -ksp_monitor_singular_value -ksp_converged_reason  Explain methods: -snes_view Performance info: -log_view  Examples mpiexec -n 4 ./ex19 -lidvelocity 2 -snes_monitor -da_refine 5 -ksp_monitor -pc_type asm -sub_pc_type lu  ","date":1570798165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571115943,"objectID":"d6b90988696402bd739c458fa4de6235","permalink":"https://cucs-hpsc.github.io/fall2019/preconditioning/","publishdate":"2019-10-11T06:49:25-06:00","relpermalink":"/fall2019/preconditioning/","section":"fall2019","summary":"Preconditioning Recall that preconditioning is the act of creating an \u0026ldquo;affordable\u0026rdquo; operation \u0026ldquo;$P^{-1}$\u0026rdquo; such that $P^{-1} A$ (or $A P^{-1}$) is is well-conditoned or otherwise has a \u0026ldquo;nice\u0026rdquo; spectrum. We then solve the system\n$$ P^{-1} A x = P^{-1} b \\quad \\text{or}\\quad A P^{-1} \\underbrace{(P x)}_y = b $$\nin which case the convergence rate depends on the spectrum of the iteration matrix $$ I - \\omega P^{-1} A .","tags":null,"title":"Preconditioning","type":"docs"},{"authors":null,"categories":null,"content":" Sparse and iterative linear algebra Many matrices in applications, particularly the study of physical systems and graphs/networks, have the property that most entries are zero. We can more efficiently store such systems by storing only the nonzero elements. We will discuss storage and optimized implementations later. Many of the methods for sparse systems apply to solving systems with matrices $A$ that can be applied to a vector ($y \\gets A x$) in significantly less than $O(n^2)$ complexity, or that are \u0026ldquo;well-conditioned\u0026rdquo; such that an iterative method converges in significantly less than $n$ iterations.\nPETSc, the Portable Extensible Toolkit for Scientific computing, is an open source software package for the parallel solution of algebraic and differential-algebraic equations. This includes linear algebra, for which PETSc offers a broad variety of implementations. For general information about PETSc, I refer to this primer.\nDirect solves The complexity of this solve is potentially dominant, so we should understand its cost in terms of the problem size. The standard method for a direct solve is $LU$ (or Cholesky) factorization. Given a $2\\times 2$ block matrix, the algorithm proceeds as \\begin{split} \\begin{bmatrix} A \u0026amp; B \\ C \u0026amp; D \\end{bmatrix} \u0026amp;= \\begin{bmatrix} L_A \u0026amp; \\ C U_A^{-1} \u0026amp; S \\end{bmatrix} \\begin{bmatrix} U_A \u0026amp; L_A^{-1} B \\ \u0026amp; 1 \\end{bmatrix} \\end{split} where $L_A U_A = A$ and $S = D - C A^{-1} B$. The factorization continues by factoring the Schur complement, which may be more dense than its original entries $D$.\nFor a sparse operator, the complexity depends on the ordering of degrees of freedom.\n \u0026ldquo;natural\u0026rdquo; ordering low-bandwidth ordering nested dissection ordering  For a structured grid, the \u0026ldquo;natural\u0026rdquo; ordering is the ordering of the original operator.\nThe red values represent positive entries, blue negative, and cyan stored zeros.\nA sparse direct solve of this system will result in fill up to the bandwidth (showing lower triangular factor here).\nThese plots can be produced in PETSc using -mat_view draw and -pc_type lu -pc_factor_mat_ordering_type natural -mat_factor_view draw (e.g., with -draw_pause 2 to wait 2 seconds for the viewer).\nThe Reverse Cuthill-McKee (rcm) ordering applies a breadth-first search to produce a low-bandwidth ordering.\nCosts for banded solves Suppose we have a grid containing $N = n^d$ points in $d$ dimensions. The corresponding matrix is $N\\times N$. The furthest band is $k = n^{d-1}$ off the diagonal, and factorization creates * Space: $kN = n^{2d-1} = N^{2-1/d}$ * Compute: $k^2N = n^{3d-2} = N^{3-2/d}$\nNested dissection The nested dissection (nd) ordering recursively bisects the domain by finding vertex separators.\nThe separator is ordered last so that each side can be factored independently. That factorization is done by recursive bisection. The Schur complement $S$ on the vertex separator is dense, and its factorization dominates the cost.\nCosts for nested dissection The size of the vertex separator is $n^{d-1}$ so its factorization costs $n^{3(d-1)}$. This leads to * $d=3$ * Space: $N^{4\u0026frasl;3}$ * Time: $N^2$ * $d=2$ * Space: $N \\log N$ * Time: $N^{3\u0026frasl;2}$\nConvergence of stationary iterative methods Richardson iteration The simplest iterative method is Richardson\u0026rsquo;s method, which solves $A x = b$ by the iteration $$ x_{k+1} = x_k + \\omega (b - A x_k) $$ where $\\omega \u0026gt; 0$ is a damping parameter and $x0$ is an initial guess (possibly the zero vector). If $b = A x$, this iteration is equivalent to $$ x{k+1} - x = (xk - x) - \\omega A (xk - x) = (I - \\omega A) (xk - x) .$$ It is convenient for convergence analysis to identify the \u0026ldquo;error\u0026rdquo; $e_k = xk - x$, in which this becomes $$ e_{k+1} = (I - \\omega A) e_k $$ or $$ e_k = (I - \\omega A)^k e_0 $$ in terms of the initial error. Evidently powers of the iteration matrix $I - \\omega A$ tell the whole story. Suppose that the eigendecomposition $$ X \\Lambda X^{-1} = I - \\omega A $$ exists. Then $$ (I - \\omega A)^k = (X \\Lambda X^{-1})^k = X \\Lambda^k X^{-1} $$ and the convergence (or divergence) rate depends only on the largest magnitude eigenvalue. This analysis isn\u0026rsquo;t great for two reasons:\n Not all matrices are diagonalizable. The matrix $X$ may be very ill-conditioned.  We can repair these weaknesses by using the Schur decomposition $$ Q R Q^h = I - \\omega A $$ where $R$ is right-triangular and $Q$ is unitary (i.e., orthogonal if real-valued; $Q^h$ is the Hermitian conjugate of $Q$). The Schur decomposition always exists and $Q$ has a condition number of 1.\n Where are the eigenvalues in $R$?  Evidently we must find $\\omega$ to minimize the maximum eigenvalue of $I - \\omega A$. We can do this if $A$ is well conditioned, but not in general.\nPreconditioning Preconditioning is the act of creating an \u0026ldquo;affordable\u0026rdquo; operation \u0026ldquo;$P^{-1}$\u0026rdquo; such that $P^{-1} A$ (or $A P^{-1}$) is is well-conditoned or otherwise has a \u0026ldquo;nice\u0026rdquo; spectrum. We then solve the system\n$$ P^{-1} A x = P^{-1} b \\quad \\text{or}\\quad A P^{-1} \\underbrace{(P x)}_y = b $$\nin which case the convergence rate depends on the spectrum of the iteration matrix $$ I - \\omega P^{-1} A . $$\n The preconditioner must be applied on each iteration. It is not merely about finding a good initial guess.  There are two complementary techniques necessary for efficient iterative methods:\n \u0026ldquo;accelerators\u0026rdquo; or Krylov methods, which use orthogonality to adaptively converge faster than Richardson preconditioners that improve the spectrum of the preconditioned operator  Although there is ongoing research in Krylov methods and they are immensely useful, I would say preconditioning is 90% of the game for practical applications, particularly as a research area.\nKrylov subspaces All matrix iterations work with approximations in a Krylov subspace, which has the form\n$$ K_n = \\big[ b \\big| Ab \\big| A^2 b \\big| \\dotsm \\big| A^{n-1} b \\big] . $$\nThis matrix is horribly ill-conditioned and cannot stably be computed as written. Instead, we seek an orthogonal basis $Q_n$ that spans the same space as $K_n$. We could write this as a factorization\n$$ K_n = Q_n R_n $$\nwhere the first column $q_0 = b / \\lVert b \\rVert$. The $R_n$ is unnecessary and hopelessly ill-conditioned, so a slightly different procedure is used.\nArnoldi iteration The Arnoldi iteration applies orthogonal similarity transformations to reduce $A$ to Hessenberg form, starting from a vector $q_0 = b$,\n$$ A = Q H Q^h . $$\nLet\u0026rsquo;s multiply on the right by $Q$ and examine the first $n$ columns,\n$$ A Qn = Q{n+1} H_n $$ where $H_n$ is an $(n+1) \\times n$ Hessenberg matrix.\nConditioning This representation is well-conditioned because $Q$ is orthogonal and\n$$ \\lVert Hn \\rVert \\le \\lVert Q{n+1}^h \\rVert \\lVert A \\rVert \\lVert Q_n \\rVert \\le \\lVert A \\rVert $$.\nFor a lower bound, we have\n$$ \\sigma_{\\min}(A)^2 \\le x^h A^h A x $$\nfor all $x$ of norm 1. It must also be true for any $x = Q_n y$ where $\\lVert y\\rVert = 1$, thus\n$$ \\sigma_{\\min}(A)^2 \\le y^h Q_n^h A^h A Q_n y = y^h Hn^h Q{n+1}^h Q_{n+1} H_n y = y^h H_n^h H_n y . $$\nGMRES GMRES (Generalized Minimum Residual) minimizes $$ \\lVert A x - b \\rVert $$ over the subspace $Q_n$. I.e., $x = Qn y$ for some $y$. By the recurrence above, this is equivalent to $$ \\lVert Q{n+1} H_n y - b \\lVert $$ which can be solved by minimizing $$ \\lVert Hn y - Q{n+1}^h b \\rVert . $$ Since $q_0 = b/\\lVert b \\lVert$, the least squares problem is to minimize $$ \\Big\\lVert H_n y - \\lVert b \\rVert e_0 \\Big\\rVert . $$ The solution of this least squares problem is achieved by incrementally updating a $QR$ factorization of $H_n$.\nNotes\n GMRES minimizes the 2-norm of the residual $\\lVert r_n \\rVert$ which is equivalent to the $A^T A$ norm of the error $\\lVert xn - x* \\rVert_{A^T A}$. The solution $x_n$ constructed by GMRES at iteration $n$ is not explicitly available. If a solution is needed, it must be constructed by solving the $(n+1)\\times n$ least squares problem and forming the solution as a linear combination of the $n$ vectors $Q_n$. The leading cost is $2mn$ where $m \\gg n$. The residual vector $r_n = A x_n - b$ is not explicitly available in GMRES. To compute it, first build the solution $x_n$ as above. GMRES needs to store the full $Q_n$, which is unaffordable for large $n$ (many iterations). The standard solution is to choose a \u0026ldquo;restart\u0026rdquo; $k$ and to discard $Q_n$ and start over with an initial guess $x_k$ after each $k$ iterations. This algorithm is called GMRES(k). PETSc\u0026rsquo;s default solver is GMRES(30) and the restart can be controlled using the run-time option -ksp_gmres_restart. Most implementations of GMRES use classical Gram-Schmidt because it is much faster in parallel (one reduction per iteration instead of $n$ reductions per iteration). The PETSc option -ksp_gmres_modifiedgramschmidt can be used when you suspect that classical Gram-Schmidt may be causing instability. There is a very similar (and older) algorithm called GCR that maintains $x_n$ and $r_n$. This is useful, for example, if a convergence tolerance needs to inspect individual entries. GCR requires $2n$ vectors instead of $n$ vectors, and can tolerate a nonlinear preconditioner. FGMRES is a newer algorithm with similar properties to GCR.  Lanczos iteration: the symmetric case If $A$ is symmetric, then $H = Q^T A Q$ is also symmetric. Since $H$ is Hessenberg, this means $H$ is tridiagonal. Instead of storing $Q_n$, it is sufficient to store only the last two columns since the iteration satisfies a 3-term recurrence. The analog of GMRES for the symmetric case is called MINRES and is also useful for solving symmetric indefinite problems.\nConjugate Gradients Instead of minimizing the $A^T A$ norm of the error, the Conjugate Gradient method minimizes the $A$ norm of the error. For $A$ to induce a norm, it must be symmetric positive definite. Jeremy Shewchuck\u0026rsquo;s guide to CG is an excellent resource.\n","date":1570625365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570645203,"objectID":"bdc64ba9a16ec19ddd26b51a2bf88acc","permalink":"https://cucs-hpsc.github.io/fall2019/iterative-solvers/","publishdate":"2019-10-09T06:49:25-06:00","relpermalink":"/fall2019/iterative-solvers/","section":"fall2019","summary":"Sparse and iterative linear algebra Many matrices in applications, particularly the study of physical systems and graphs/networks, have the property that most entries are zero. We can more efficiently store such systems by storing only the nonzero elements. We will discuss storage and optimized implementations later. Many of the methods for sparse systems apply to solving systems with matrices $A$ that can be applied to a vector ($y \\gets A x$) in significantly less than $O(n^2)$ complexity, or that are \u0026ldquo;well-conditioned\u0026rdquo; such that an iterative method converges in significantly less than $n$ iterations.","tags":null,"title":"Sparse and iterative linear algebra","type":"docs"},{"authors":null,"categories":null,"content":" FLAME diagram for Cholesky $A[M_C, M_R]$ distribution The $A[,]$ distribution References  Poulson et al. (2013) Elemental: A New Framework for Distributed Memory Dense Matrix Computations doi:10.1145\u0026frasl;2427023.2427030  ","date":1570193365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570593128,"objectID":"3ac994de92c3582a8749421d6d918b0c","permalink":"https://cucs-hpsc.github.io/fall2019/elemental/","publishdate":"2019-10-04T06:49:25-06:00","relpermalink":"/fall2019/elemental/","section":"fall2019","summary":" FLAME diagram for Cholesky $A[M_C, M_R]$ distribution The $A[,]$ distribution References  Poulson et al. (2013) Elemental: A New Framework for Distributed Memory Dense Matrix Computations doi:10.1145\u0026frasl;2427023.2427030  ","tags":null,"title":"Elemental for distributed dense linear algebra","type":"docs"},{"authors":null,"categories":null,"content":" %matplotlib inline import numpy as np import matplotlib.pyplot as plt plt.style.use('seaborn')  Orthogonalization and QR factorization Given a collection of vectors (columns of a matrix), we can find an orthogonal basis by applying the above procedure one column at a time and saving the result. Let\u0026rsquo;s think of the first two columns, $$ \\Bigg[ a_0 \\, \\Bigg| \\, a_1 \\Bigg] = \\Bigg[ q_0 \\,\\Bigg|\\, q1 \\Bigg] \\begin{bmatrix} r{00} \u0026amp; r{01} \\ 0 \u0026amp; r{11} \\end{bmatrix} . $$\nColumn 0 The equation for column 0 reads $$ a_0 = q0 r{00} $$ and we require that $\\lVert q0 \\rVert = 1$, thus $$ r{00} = \\lVert a_0 \\rVert $$ and $$ q_0 = a0 / r{00} . $$\nColumn 1 This equation reads $$ a_1 = q0 r{01} + q1 r{11} $$ where $a_1$ and $q_0$ are known and we will require that $q_0^T q_1 = 0$. We can find the part of $a_1$ that is orthogonal to $q_0$ via $$ (I - q_0 q_0^T) a_1 = a_1 - q_0 \\underbrace{q_0^T a1}{r_{01}} $$ leaving a sub-problem equivalent to that of column 0.\ndef gram_schmidt_naive(A): \u0026quot;\u0026quot;\u0026quot;Compute a QR factorization of A using the Gram-Schmidt algorithm\u0026quot;\u0026quot;\u0026quot; Q = np.zeros_like(A) R = np.zeros((A.shape[1], A.shape[1])) for i in range(len(Q.T)): v = A[:,i].copy() for j in range(i): r = Q[:,j] @ v R[j,i] = r v -= Q[:,j] * r # \u0026quot;modified Gram-Schmidt\u0026quot; R[i,i] = np.linalg.norm(v) Q[:,i] = v / R[i,i] return Q, R x = np.linspace(-1, 1) A = np.vander(x, 4, increasing=True) Q, R = gram_schmidt_naive(A) print(Q.T @ Q) print(np.linalg.norm(Q @ R - A)) plt.plot(x, Q);  [[ 1.00000000e+00 2.06727448e-17 -7.22457952e-17 -2.05232865e-16] [ 2.06727448e-17 1.00000000e+00 1.13635722e-16 -5.08904737e-16] [-7.22457952e-17 1.13635722e-16 1.00000000e+00 4.66276733e-17] [-2.05232865e-16 -5.08904737e-16 4.66276733e-17 1.00000000e+00]] 4.744563050812836e-16  Theorem: all full-rank $m\\times n$ matrices ($m \\ge n$) have a unique $Q R$ factorization with $R_{j,j} \u0026gt; 0$. m = 20 V = np.vander(np.linspace(-1,1,m), increasing=True) Q, R = gram_schmidt_naive(V) def qr_test(qr, V): Q, R = qr(V) m = len(Q.T) print('{:20} {:.2e} {:.2e}'.format( qr.__name__, np.linalg.norm(Q @ R - V), np.linalg.norm(Q.T @ Q - np.eye(m)))) qr_test(gram_schmidt_naive, V) qr_test(np.linalg.qr, V)  gram_schmidt_naive 9.52e-16 3.04e-09 qr 2.74e-15 2.39e-15  Left-looking algorithms: reducing the number of inner products def gram_schmidt_classical(A): Q = np.zeros_like(A) R = np.zeros((len(A.T),len(A.T))) for i in range(len(Q.T)): v = A[:,i].copy() R[:i,i] = Q[:,:i].T @ v v -= Q[:,:i] @ R[:i,i] R[i,i] = np.linalg.norm(v) Q[:,i] = v / R[i,i] return Q, R qr_test(gram_schmidt_classical, V)  gram_schmidt_classical 9.14e-16 1.42e+00  Classical Gram-Schmidt is highly parallel, but unstable, as evidenced by the lack of orthogonality in $Q$.\nRight-looking algorithms The implementations above have been \u0026ldquo;left-looking\u0026rdquo;; when working on column $i$, we compare it only to columns to the left (i.e., $j \u0026lt; i$). We can reorder the algorithm to look to the right by projecting $q_i$ out of all columns $j \u0026gt; i$. This algorithm is stable while being just as parallel as gram_schmidt_classical.\ndef gram_schmidt_modified(A): Q = A.copy() R = np.zeros((len(A.T), len(A.T))) for i in range(len(Q.T)): R[i,i] = np.linalg.norm(Q[:,i]) Q[:,i] /= R[i,i] R[i,i+1:] = Q[:,i].T @ Q[:,i+1:] Q[:,i+1:] -= np.outer(Q[:,i], R[i,i+1:]) return Q, R qr_test(gram_schmidt_modified, V)  gram_schmidt_modified 8.32e-16 1.32e-08  Stability Since QR factorization is unique (with positive diagonal of $R$), if we were to work in exact arithmetic, classical and modified Gram-Schmidt would produce the same result. Note that modified Gram-Schmidt sequentially applies the projections into the orthogonal complement of each column $q_j$ of $Q$. That is, given a vector $x$, we sequentially project $(I - q_j q_j^T) x$ for each column $j \u0026lt; i$. This is equivalent to projecting all those columns at once due to \\begin{align} (I - q_1 q_1^T) (I - q_0 q_0^T) x \u0026amp;= \\big(I - q_0 q_0^T - q_1 q_1^T + q_1 \\underbrace{q_1^T q0}{=0} q_0^T \\big) x \u0026amp;= (I - q_0 q_0^T - q_1 q_1^T) x \u0026amp;= (I - Q Q^T) x \\end{align} where $Q = [q_0 | q_1 ]$. This identity can be applied recursively to convert modified Gram-Schmidt to classical, but the identity is not exact in finite precision arithmetic.\nv = V[:,-1] print('norm(v)', np.linalg.norm(v)) print('r', R[-1,-1]) plt.semilogy(np.abs(Q.T @ v), 'o') plt.title('Inner products of v with Q');  norm(v) 1.4245900685395503 r 1.7146698318004178e-07  def test_sum(n): def gen(first, n, factor=2/3): l = [first] ifactor = 1-factor for i in range(n): l.append(-first * factor * ifactor**i) return l, first * ifactor**n def sum_seq(numbers): s = 0 for a in numbers: s += a return s def sum_block(numbers): s = 0 for a in numbers[1:]: s += a return numbers[0] + s numbers, exact = gen(1, n) print(numbers) plt.semilogy(np.abs(numbers), 'o') seq_err = sum_seq(numbers) - exact block_err = sum_block(numbers) - exact numpy_err = np.sum(numbers) - exact print('seq abs {:.4e} rel {:.4e}'.format(seq_err, seq_err/exact)) print('block abs {:.4e} rel {:.4e}'.format(block_err, block_err/exact)) print('numpy abs {:.4e} rel {:.4e}'.format(numpy_err, numpy_err/exact)) test_sum(20)  [1, -0.6666666666666666, -0.22222222222222224, -0.07407407407407408, -0.024691358024691364, -0.008230452674897124, -0.0027434842249657075, -0.0009144947416552361, -0.0003048315805517454, -0.00010161052685058181, -3.3870175616860605e-05, -1.1290058538953536e-05, -3.763352846317846e-06, -1.2544509487726156e-06, -4.181503162575385e-07, -1.3938343875251286e-07, -4.6461146250837626e-08, -1.548704875027921e-08, -5.162349583426404e-09, -1.7207831944754682e-09, -5.735943981584894e-10] seq abs 5.9562e-22 rel 2.0768e-12 block abs 5.3534e-17 rel 1.8666e-07 numpy abs 7.4670e-17 rel 2.6036e-07  Householder triangularization Gram-Schmidt methods perform triangular transformations to build an orthogonal matrix. As we have seen, $X = QR$ is satisfied accurately, but $Q$ may not be orthogonal when $X$ is ill-conditioned. Householder triangularization instead applies a sequence of orthogonal transformations to build a triangular matrix.\n$$ \\underbrace{Q_{n-1} \\dotsb Q0}{Q^T} A = R $$\nThe structure of the algorithm is\n$$ \\underbrace{\\begin{bmatrix} * \u0026amp; * \u0026amp; * \\ * \u0026amp; * \u0026amp; * \\ * \u0026amp; * \u0026amp; * \\ * \u0026amp; * \u0026amp; * \\ * \u0026amp; * \u0026amp; * \\ \\end{bmatrix}}{A} \\to \\underbrace{\\begin{bmatrix} * \u0026amp; * \u0026amp; * \\ 0 \u0026amp; * \u0026amp; * \\ 0 \u0026amp; * \u0026amp; * \\ 0 \u0026amp; * \u0026amp; * \\ 0 \u0026amp; * \u0026amp; * \\ \\end{bmatrix}}{Q0 A} \\to \\underbrace{\\begin{bmatrix} * \u0026amp; * \u0026amp; * \\ 0 \u0026amp; * \u0026amp; * \\ 0 \u0026amp; 0 \u0026amp; * \\ 0 \u0026amp; 0 \u0026amp; * \\ 0 \u0026amp; 0 \u0026amp; * \\ \\end{bmatrix}}{Q_1 Q0 A} \\to \\underbrace{\\begin{bmatrix} * \u0026amp; * \u0026amp; * \\ 0 \u0026amp; * \u0026amp; * \\ 0 \u0026amp; 0 \u0026amp; * \\ 0 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 0 \\ \\end{bmatrix}}{Q_2 Q_1 Q_0 A} $$\nwhere the elementary orthogonal matrices $Q_i$ chosen to introduce zeros below the diagonal in the $i$th column of $R$. Each of these transformations will have the form $$Q_i = \\begin{bmatrix} I_i \u0026amp; 0 \\ 0 \u0026amp; F \\end{bmatrix}$$ where $F$ is a \u0026ldquo;reflection\u0026rdquo; that achieves $$ F x = \\begin{bmatrix} \\lVert x \\rVert \\ 0 \\ 0 \\ \\vdots \\end{bmatrix} $$ where $x$ is the column of $R$ from the diagonal down. This transformation is a reflection across a plane with normal $v = Fx - x = \\lVert x \\rVert e_1 - x$.\nThe reflection, as depected above by Trefethen and Bau (1999) can be written $F = I - 2 \\frac{v v^T}{v^T v}$.\nA = np.random.rand(4, 4) A = A + A.T # Random symmetric matrix A  array([[1.71984228, 0.68338128, 1.12543662, 0.59188991], [0.68338128, 1.5609485 , 1.03109546, 1.4707089 ], [1.12543662, 1.03109546, 0.02375504, 0.71686222], [0.59188991, 1.4707089 , 0.71686222, 0.88113581]])  A.T - A  array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]])  from scipy.linalg import block_diag np.set_printoptions(precision=4) def reflector(v): return np.eye(len(v)) - 2*np.outer(v, v) v = A[1:,0].copy() v[0] -= np.linalg.norm(v) v = v / np.linalg.norm(v) F = reflector(v) Q_0 = block_diag(np.eye(1), F) Q_0  array([[ 1. , 0. , 0. , 0. ], [ 0. , 0.4734, 0.7796, 0.41 ], [ 0. , 0.7796, -0.1542, -0.607 ], [ 0. , 0.41 , -0.607 , 0.6808]])  Q_0 @ A  array([[ 1.7198e+00, 6.8338e-01, 1.1254e+00, 5.9189e-01], [ 1.4436e+00, 2.1458e+00, 8.0055e-01, 1.6164e+00], [-6.8853e-17, 1.6526e-01, 3.6506e-01, 5.0122e-01], [-2.8229e-17, 1.0154e+00, 8.9636e-01, 7.6773e-01]])  A @ Q_0  array([[ 1.7198e+00, 1.4436e+00, -6.8853e-17, -2.8229e-17], [ 6.8338e-01, 2.1458e+00, 1.6526e-01, 1.0154e+00], [ 1.1254e+00, 8.0055e-01, 3.6506e-01, 8.9636e-01], [ 5.9189e-01, 1.6164e+00, 5.0122e-01, 7.6773e-01]])  def householder_Q_times(V, x): \u0026quot;\u0026quot;\u0026quot;Apply orthogonal matrix represented as list of Householder reflectors\u0026quot;\u0026quot;\u0026quot; y = x.copy() for i in reversed(range(len(V))): y[i:] -= 2 * V[i] * V[i].dot(y[i:]) return y def qr_householder1(A): \u0026quot;Compute QR factorization using naive Householder reflection\u0026quot; m, n = A.shape R = A.copy() V = [] for i in range(n): x = R[i:,i] v = -x v[0] += np.linalg.norm(x) v = v/np.linalg.norm(v) # Normalized reflector plane R[i:,i:] -= 2 * np.outer(v, v @ R[i:,i:]) V.append(v) # Storing reflectors is equivalent to storing orthogonal matrix Q = np.eye(m, n) for i in range(n): Q[:,i] = householder_Q_times(V, Q[:,i]) return Q, np.triu(R[:n,:]) qr_test(qr_householder1, np.array([[1.,2],[3,4],[5,6]]))  qr_householder1 1.88e-15 3.17e-16  qr_test(qr_householder1, V) qr_test(np.linalg.qr, V)  qr_householder1 3.15e-15 3.48e-15 qr 2.74e-15 2.39e-15  Choice of two projections It turns out our implementation has a nasty deficiency.\nqr_test(qr_householder1, np.eye(1))  qr_householder1 nan nan /usr/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide  qr_test(qr_householder1, np.eye(3,2))  qr_householder1 nan nan /usr/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide  Inside qr_householder1, we have the lines\n x = R[i:,i] v = -x v[0] += numpy.linalg.norm(x) v = v/numpy.linalg.norm(v) # Normalized reflector plane  What happens when $$x = \\begin{bmatrix}1 \\ 0 \\end{bmatrix}$$ (i.e., the column of $R$ is already upper triangular)?\nWe are trying to define a reflector plane (via its normal vector) from the zero vector, $$v = \\lVert x \\rVert e_0 - x .$$ When we try to normalize this vector, we divide zero by zero and the algorithm breaks down (nan). Maybe we just need to test for this special case and \u0026ldquo;skip ahead\u0026rdquo; when no reflection is needed? And if so, how would we define $Q$?\nqr_test(qr_householder1, np.array([[1.,1], [2e-8,1]])) print(qr_householder1(np.array([[1.,1], [2e-8,1]])))  qr_householder1 2.20e-09 4.44e-16 (array([[ 1.0000e+00, -2.2204e-08], [ 2.2204e-08, 1.0000e+00]]), array([[1., 1.], [0., 1.]]))  The error $QR - A$ is still $10^{-8}$ for this very well-conditioned matrix so something else must be at play here.\ndef qr_householder2(A): \u0026quot;Compute QR factorization using Householder reflection\u0026quot; m, n = A.shape R = A.copy() V = [] for i in range(n): v = R[i:,i].copy() v[0] += np.sign(v[0]) * np.linalg.norm(v) # Choose the further of the two reflections v = v/np.linalg.norm(v) # Normalized reflector plane R[i:,i:] -= np.outer(v, 2 * (v.T @ R[i:,i:])) V.append(v) # Storing reflectors is equivalent to storing orthogonal matrix Q = np.eye(m, n) for i in range(n): Q[:,i] = householder_Q_times(V, Q[:,i]) return Q, np.triu(R[:n,:]) qr_test(qr_householder2, np.eye(3,2)) qr_test(qr_householder2, np.array([[1.,1], [1e-8,1]])) for mat in qr_householder2(np.array([[1.,1], [1e-8,1]])): print(mat) qr_test(qr_householder2, V)  qr_householder2 0.00e+00 0.00e+00 qr_householder2 0.00e+00 0.00e+00 [[-1.e+00 1.e-08] [-1.e-08 -1.e+00]] [[-1. -1.] [ 0. -1.]] qr_householder2 5.20e-15 3.58e-15  We now have a usable implementation of Householder QR. There are some further concerns for factoring rank-deficient matrices. We will visit the concept of pivoting later, in the context of LU and Cholesky factorization.\nConditioning Absolute condition number Consider a function $f: X \\to Y$ and define the absolute condition number $$ \\hat\\kappa = \\lim{\\delta \\to 0} \\max{|\\delta x| \u0026lt; \\delta} \\frac{|f(x + \\delta x) - f(x)|}{|\\delta x|} = \\max_{\\delta x} \\frac{|\\delta f|}{|\\delta x|}. $$ If $f$ is differentiable, then $\\hat\\kappa = |f\u0026rsquo;(x)|$.\nFloating point arithmetic Floating point arithmetic $x \\circledast y := \\text{float}(x * y)$ is exact within a relative accuracy $\\epsilon{\\text{machine}}$. Formally, $$ x \\circledast y = (x * y) (1 + \\epsilon) $$ for some $|\\epsilon| \\le \\epsilon{\\text{machine}}$.\neps = 1 while 1 + eps \u0026gt; 1: eps /= 2 eps_machine = eps print('Machine epsilon = {}'.format(eps_machine)) (1 + 1.12e-16) - 1  Machine epsilon = 1.1102230246251565e-16 2.220446049250313e-16  def plot_neighborhood(f, x0, atol=1e-10, rtol=1e-10): width = atol + rtol * np.abs(x0) x = np.linspace(x0 - width, x0 + width, 201) plt.plot(x, f(x)) plt.xlabel('x') plt.ylabel('f(x)') plot_neighborhood(lambda x: (x + 1) - 1, 0, 1e-15)  This function $f(x) = (x + 1) - 1 = x$ is well conditioned for all $x$, but this numerical algorithm is unstable (we\u0026rsquo;ll discuss this below).\nplot_neighborhood(np.log, 1, .5)  The function $f(x) = \\log x$ has $f\u0026rsquo;(1) = 1$. The conditioning is good in an absolute sense, $\\hat \\kappa = 1$. However, the outputs from np.log(1+x) have large relative error relative to the exact value, provided here by the function np.log1p(x).\nplot_neighborhood(lambda x: np.log(1+x), 0, atol=1e-15) plot_neighborhood(np.log1p, 0, atol=1e-15)  Relative condition number Given the relative nature of floating point arithmetic, it is more useful to discuss relative condition number, $$ \\kappa = \\max{\\delta x} \\frac{|\\delta f|/|f|}{|\\delta x|/|x|} = \\max{\\delta x} \\Big[ \\frac{|\\delta f|/|\\delta x|}{|f| / |x|} \\Big] $$ or, if $f$ is differentiable, $$ \\kappa = |f\u0026rsquo;(x)| \\frac{|x|}{|f|} . $$\nHow does a condition number get big?\nTake-home message The relative accuracy of the best-case algorithm will not be reliably better than $\\epsilon{\\text{machine}}$ times the condition number. $$ \\max{\\delta x} \\frac{|\\delta f|}{|f|} \\ge \\kappa \\cdot \\epsilon_{\\text{machine}} .$$\nStability We use the notation $\\tilde f(x)$ to mean a numerical algorithm for approximating $f(x)$. Additionally, $\\tilde x = x (1 + \\epsilon)$ is some \u0026ldquo;good\u0026rdquo; approximation of the exact input $x$.\n(Forward) Stability \u0026ldquo;nearly the right answer to nearly the right question\u0026rdquo; $$ \\frac{\\lvert \\tilde f(x) - f(\\tilde x) \\rvert}{| f(\\tilde x) |} \\in O(\\epsilon_{\\text{machine}}) $$ for some $\\tilde x$ that is close to $x$\nBackward Stability \u0026ldquo;exactly the right answer to nearly the right question\u0026rdquo; $$ \\tilde f(x) = f(\\tilde x) $$ for some $\\tilde x$ that is close to $x$\n Every backward stable algorithm is stable. Not every stable algorithm is backward stable.  Accuracy of backward stable algorithms (Theorem) A backward stable algorithm for computing $f(x)$ has relative accuracy $$ \\left\\lvert \\frac{\\tilde f(x) - f(x)}{f(x)} \\right\\rvert \\in O(\\kappa(f) \\epsilon_{\\text{machine}}) . $$ This is a rewording of a statement made earlier \u0026ndash; backward stability is the best case.\nCondition number of a matrix We may have informally referred to a matrix as \u0026ldquo;ill-conditioned\u0026rdquo; when the columns are nearly linearly dependent, but let\u0026rsquo;s make this concept for precise. Recall the definition of (relative) condition number from the Rootfinding notes,\n$$ \\kappa = \\max_{\\delta x} \\frac{|\\delta f|/|f|}{|\\delta x|/|x|} . $$\nWe understood this definition for scalar problems, but it also makes sense when the inputs and/or outputs are vectors (or matrices, etc.) and absolute value is replaced by vector (or matrix) norms. Let\u0026rsquo;s consider the case of matrix-vector multiplication, for which $f(x) = A x$.\n$$ \\kappa(A) = \\max{\\delta x} \\frac{\\lVert A (x+\\delta x) - A x \\rVert/\\lVert A x \\rVert}{\\lVert \\delta x\\rVert/\\lVert x \\rVert} = \\max{\\delta x} \\frac{\\lVert A \\delta x \\rVert}{\\lVert \\delta x \\rVert} \\, \\frac{\\lVert x \\rVert}{\\lVert A x \\rVert} = \\lVert A \\rVert \\frac{\\lVert x \\rVert}{\\lVert A x \\rVert} . $$\nThere are two problems here:\n I wrote $\\kappa(A)$ but my formula depends on $x$. What is that $\\lVert A \\rVert$ beastie?  Stack push: Matrix norms Vector norms are built into the linear space (and defined in term of the inner product). Matrix norms are induced by vector norms, according to\n$$ \\lVert A \\rVert = \\max_{x \\ne 0} \\frac{\\lVert A x \\rVert}{\\lVert x \\rVert} . $$\n This equation makes sense for non-square matrices \u0026ndash; the vector norms of the input and output spaces may differ. Due to linearity, all that matters is direction of $x$, so it could equivalently be written  $$ \\lVert A \\rVert = \\max_{\\lVert x \\rVert = 1} \\lVert A x \\rVert . $$\nStack pop Now we understand the formula for condition number, but it depends on $x$. Consider the matrix\n$$ A = \\begin{bmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; 0 \\end{bmatrix} . $$\n What is the norm of this matrix? What is the condition number when $x = [1,0]^T$? What is the condition number when $x = [0,1]^T$?  The condition number of matrix-vector multiplication depends on the vector. The condition number of the matrix is the worst case (maximum) of the condition number for any vector, i.e.,\n$$ \\kappa(A) = \\max_{x \\ne 0} \\lVert A \\rVert \\frac{\\lVert x \\rVert}{\\lVert A x \\rVert} .$$\nIf $A$ is invertible, then we can rephrase as\n$$ \\kappa(A) = \\max{x \\ne 0} \\lVert A \\rVert \\frac{\\lVert A^{-1} (A x) \\rVert}{\\lVert A x \\rVert} = \\max{A x \\ne 0} \\lVert A \\rVert \\frac{\\lVert A^{-1} (A x) \\rVert}{\\lVert A x \\rVert} = \\lVert A \\rVert \\lVert A^{-1} \\rVert . $$\nEvidently multiplying by a matrix is just as ill-conditioned of an operation as solving a linear system using that matrix.\ndef R_solve(R, b): \u0026quot;\u0026quot;\u0026quot;Solve Rx = b using back substitution.\u0026quot;\u0026quot;\u0026quot; x = b.copy() m = len(b) for i in reversed(range(m)): x[i] -= R[i,i+1:].dot(x[i+1:]) x[i] /= R[i,i] return x Q, R = np.linalg.qr(A) x = np.array([1,2,3,4]) bfull = A @ x breduced = Q.T @ bfull print(np.linalg.norm(R_solve(R, breduced) - np.linalg.solve(R, breduced))) R_solve(R, breduced)  4.440892098500626e-16 array([1., 2., 3., 4.])  Cost of Householder factorization The dominant cost comes from the line\nR[i:,i:] -= 2 * numpy.outer(v, v.dot(R[i:,i:]))  were R[i:,i:] is an $(m-i)\\times(n-i)$ matrix. This line performs $2(m-i)(n-i)$ operations in v.dot(R[i:,i:]), another $(m-i)(n-i)$ in the \u0026ldquo;outer\u0026rdquo; product and again in subtraction. As written, multiplication by 2 would be another $(m-i)(n-i)$ operations, but is only $m-i$ operations if we rewrite as\nw = 2*v R[i:,i:] -= numpy.outer(w, v.dot(R[i:,i:]))  in which case the leading order cost is $4(m-i)(n-i)$. To compute the total cost, we need to sum over all columns $i$, $$\\begin{split} \\sum{i=1}^n 4(m-i)(n-i) \u0026amp;= 4 \\Big[ \\sum{i=1}^n (m-n)(n-i) + \\sum{i=1}^n (n-i)^2 \\Big] \u0026amp;= 4 (m-n) \\sum{i=1}^n i + 4 \\sum_{i=1}^n i^2 \u0026amp;\\approx 2 (m-n) n^2 + 4 n^3\u0026frasl;3 \u0026amp;= 2 m n^2 - \\frac 2 3 n^3 . \\end{split}$$ Recall that Gram-Schmidt QR cost $2 m n^2$, so Householder costs about the same when $m \\gg n$ and is markedly less expensive when $m \\approx n$.\nBackward Stability of Housholder def qr_test_backward(qr, n): from numpy.linalg import norm from numpy.random import randn R = np.triu(randn(n,n)) Q, _ = np.linalg.qr(randn(n,n)) A = Q @ R Q2, R2 = qr(A) print('# Forward error') print('Q error', norm(Q2 - Q)) print('R error', norm(R2 - R) / norm(R)) print('# Backward error') A2 = Q2 @ R2 print('Q2.T @ Q2 - I', norm(Q2.T @ Q2 - np.eye(n))) print('Q2*R2 - A', norm(A2 - A) / norm(A)) Q3, R3 = Q + 1e-5*randn(n,n), R + 1e-5*np.triu(randn(n,n)) A3 = Q3 @ R3 print('Q3*R3 - A', norm(A3 - A) / norm(A)) qr_test_backward(gram_schmidt_modified, 50) #qr_test_backward(np.linalg.qr, 50)  # Forward error Q error 9.380829776561018 R error 1.3114908995362393 # Backward error Q2.T @ Q2 - I 0.008203364853000912 Q2*R2 - A 2.797766695296299e-16 Q3*R3 - A 7.070141269858638e-05  Back to parallelism: Cholesky QR (one reduction) def chol_qr(A): import scipy.linalg as la B = A.T @ A R = la.cholesky(B) Q = A @ la.inv(R) return Q, R qr_test(chol_qr, V)  chol_qr 8.12e-15 1.07e-01  def chol_qr2(A): import scipy.linalg as la B = A.T @ A R = la.cholesky(B) Q = A @ la.inv(R) R2 = la.cholesky(Q.T @ Q) Q = Q @ la.inv(R2) R = R2 @ R return Q, R qr_test(chol_qr2, V)  chol_qr2 8.36e-15 1.29e-15  TSQR: Tall-Skinny QR Figure from Ballard et al.\n","date":1570193365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570231070,"objectID":"748dd1ac6c72f2939446d777e6e64253","permalink":"https://cucs-hpsc.github.io/fall2019/dense-linalg-3/","publishdate":"2019-10-04T06:49:25-06:00","relpermalink":"/fall2019/dense-linalg-3/","section":"fall2019","summary":"%matplotlib inline import numpy as np import matplotlib.pyplot as plt plt.style.use('seaborn')  Orthogonalization and QR factorization Given a collection of vectors (columns of a matrix), we can find an orthogonal basis by applying the above procedure one column at a time and saving the result. Let\u0026rsquo;s think of the first two columns, $$ \\Bigg[ a_0 \\, \\Bigg| \\, a_1 \\Bigg] = \\Bigg[ q_0 \\,\\Bigg|\\, q1 \\Bigg] \\begin{bmatrix} r{00} \u0026amp; r{01} \\ 0 \u0026amp; r{11} \\end{bmatrix} .","tags":null,"title":"Orthogonality and Conditioning","type":"docs"},{"authors":null,"categories":null,"content":" #pragma omp parallel { for (size_t rep=0; rep\u0026lt;args.repetitions; rep++) { #pragma omp for for (size_t i=0; i\u0026lt;args.array_len; i++) y[i] += 3.14 * x[i]; } }  for (size_t rep=0; rep\u0026lt;args.repetitions; rep++) { #pragma omp parallel for for (size_t i=0; i\u0026lt;args.array_len; i++) y[i] += 3.14 * x[i]; }  ! make CFLAGS='-O3 -march=native -fopenmp -Wall' -B omp-test  cc -O3 -march=native -fopenmp -Wall omp-test.c -o omp-test  ! ./omp-test -r 10000  omp for : 0.770512 ticks per entry omp parallel for: 1.576261 ticks per entry omp for : 0.419312 ticks per entry omp parallel for: 1.475976 ticks per entry omp for : 0.426896 ticks per entry omp parallel for: 1.021856 ticks per entry omp for : 0.494572 ticks per entry omp parallel for: 1.270378 ticks per entry omp for : 0.444213 ticks per entry omp parallel for: 1.009316 ticks per entry omp for : 0.579121 ticks per entry omp parallel for: 1.024148 ticks per entry omp for : 0.531494 ticks per entry omp parallel for: 1.174585 ticks per entry omp for : 0.442223 ticks per entry omp parallel for: 1.147614 ticks per entry omp for : 0.446249 ticks per entry omp parallel for: 1.084162 ticks per entry omp for : 0.576802 ticks per entry omp parallel for: 1.325817 ticks per entry  Matrix-matrix multiply Start local Further reading  http://www.cs.utexas.edu/users/flame/pubs/blis2_toms_rev3.pdf http://www.cs.utexas.edu/users/flame/pubs/blis3_ipdps14.pdf\n%matplotlib inline import numpy as np import matplotlib.pyplot as plt plt.style.use('seaborn')   Orthogonalization and QR factorization Given a collection of vectors (columns of a matrix), we can find an orthogonal basis by applying the above procedure one column at a time and saving the result. Let\u0026rsquo;s think of the first two columns, $$ \\Bigg[ a_0 \\, \\Bigg| \\, a_1 \\Bigg] = \\Bigg[ q_0 \\,\\Bigg|\\, q1 \\Bigg] \\begin{bmatrix} r{00} \u0026amp; r{01} \\ 0 \u0026amp; r{11} \\end{bmatrix} . $$\nColumn 0 The equation for column 0 reads $$ a_0 = q0 r{00} $$ and we require that $\\lVert q0 \\rVert = 1$, thus $$ r{00} = \\lVert a_0 \\rVert $$ and $$ q_0 = a0 / r{00} . $$\nColumn 1 This equation reads $$ a_1 = q0 r{01} + q1 r{11} $$ where $a_1$ and $q_0$ are known and we will require that $q_0^T q_1 = 0$. We can find the part of $a_1$ that is orthogonal to $q_0$ via $$ (I - q_0 q_0^T) a_1 = a_1 - q_0 \\underbrace{q_0^T a1}{r_{01}} $$ leaving a sub-problem equivalent to that of column 0.\ndef gram_schmidt_naive(A): \u0026quot;\u0026quot;\u0026quot;Compute a QR factorization of A using the Gram-Schmidt algorithm\u0026quot;\u0026quot;\u0026quot; Q = np.zeros_like(A) R = np.zeros((A.shape[1], A.shape[1])) for i in range(len(Q.T)): v = A[:,i].copy() for j in range(i): r = Q[:,j] @ v R[j,i] = r v -= Q[:,j] * r # \u0026quot;modified Gram-Schmidt\u0026quot; R[i,i] = np.linalg.norm(v) Q[:,i] = v / R[i,i] return Q, R x = np.linspace(-1, 1) A = np.vander(x, 4, increasing=True) Q, R = gram_schmidt_naive(A) print(Q.T @ Q) print(np.linalg.norm(Q @ R - A)) plt.plot(x, Q);  [[ 1.00000000e+00 2.06727448e-17 -7.22457952e-17 -2.05232865e-16] [ 2.06727448e-17 1.00000000e+00 1.13635722e-16 -5.08904737e-16] [-7.22457952e-17 1.13635722e-16 1.00000000e+00 4.66276733e-17] [-2.05232865e-16 -5.08904737e-16 4.66276733e-17 1.00000000e+00]] 4.744563050812836e-16  Theorem: all full-rank $m\\times n$ matrices ($m \\ge n$) have a unique $Q R$ factorization with $R_{j,j} \u0026gt; 0$. m = 20 V = np.vander(np.linspace(-1,1,m), increasing=True) Q, R = gram_schmidt_naive(V) def qr_test(qr, V): Q, R = qr(V) m = len(Q.T) print('{:20} {:.2e} {:.2e}'.format( qr.__name__, np.linalg.norm(Q @ R - V), np.linalg.norm(Q.T @ Q - np.eye(m)))) qr_test(gram_schmidt_naive, V) qr_test(np.linalg.qr, V)  gram_schmidt_naive 9.52e-16 3.04e-09 qr 2.74e-15 2.39e-15  Left-looking algorithms: reducing the number of inner products def gram_schmidt_classical(A): Q = np.zeros_like(A) R = np.zeros((len(A.T),len(A.T))) for i in range(len(Q.T)): v = A[:,i].copy() R[:i,i] = Q[:,:i].T @ v v -= Q[:,:i] @ R[:i,i] R[i,i] = np.linalg.norm(v) Q[:,i] = v / R[i,i] return Q, R qr_test(gram_schmidt_classical, V)  gram_schmidt_classical 9.14e-16 1.42e+00  Classical Gram-Schmidt is highly parallel, but unstable, as evidenced by the lack of orthogonality in $Q$.\nRight-looking algorithms The implementations above have been \u0026ldquo;left-looking\u0026rdquo;; when working on column $i$, we compare it only to columns to the left (i.e., $j \u0026lt; i$). We can reorder the algorithm to look to the right by projecting $q_i$ out of all columns $j \u0026gt; i$. This algorithm is stable while being just as parallel as gram_schmidt_classical.\ndef gram_schmidt_modified(A): Q = A.copy() R = np.zeros((len(A.T), len(A.T))) for i in range(len(Q.T)): R[i,i] = np.linalg.norm(Q[:,i]) Q[:,i] /= R[i,i] R[i,i+1:] = Q[:,i].T @ Q[:,i+1:] Q[:,i+1:] -= np.outer(Q[:,i], R[i,i+1:]) return Q, R qr_test(gram_schmidt_modified, V)  gram_schmidt_modified 8.32e-16 1.32e-08  One reduction: Cholesky QR def chol_qr(A): import scipy.linalg as la B = A.T @ A R = la.cholesky(B) Q = A @ la.inv(R) return Q, R qr_test(chol_qr, V)  chol_qr 8.12e-15 1.07e-01  def chol_qr2(A): import scipy.linalg as la B = A.T @ A R = la.cholesky(B) Q = A @ la.inv(R) R2 = la.cholesky(Q.T @ Q) Q = Q @ la.inv(R2) R = R2 @ R return Q, R qr_test(chol_qr2, V)  chol_qr2 8.36e-15 1.29e-15  ","date":1570020565,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570052754,"objectID":"08cea43b06bdcbf98d3a52d10da198cf","permalink":"https://cucs-hpsc.github.io/fall2019/dense-linalg-2/","publishdate":"2019-10-02T06:49:25-06:00","relpermalink":"/fall2019/dense-linalg-2/","section":"fall2019","summary":"#pragma omp parallel { for (size_t rep=0; rep\u0026lt;args.repetitions; rep++) { #pragma omp for for (size_t i=0; i\u0026lt;args.array_len; i++) y[i] += 3.14 * x[i]; } }  for (size_t rep=0; rep\u0026lt;args.repetitions; rep++) { #pragma omp parallel for for (size_t i=0; i\u0026lt;args.array_len; i++) y[i] += 3.14 * x[i]; }  ! make CFLAGS='-O3 -march=native -fopenmp -Wall' -B omp-test  cc -O3 -march=native -fopenmp -Wall omp-test.c -o omp-test  ! ./omp-test -r 10000  omp for : 0.","tags":null,"title":"Dense Linear Algebra and Orthogonality","type":"docs"},{"authors":null,"categories":null,"content":" Inner products $$ x^T y = \\sum_{i=1}^N x_i y_i $$\nOpenMP The vectors x and y of length N are stored in a contiguous array in shared memory.\ndouble sum = 0; #pragma omp parallel for reduction(+:sum) for (int i=0; i\u0026lt;N; i++) sum += x[i] * y[i];  MPI The vectors x and y are partitioned into $P$ parts of length $np$ such that $$ N = \\sum{p=1}^P n_p . $$ The inner product is computed via\ndouble sum = 0; for (int i=0; i\u0026lt;n; i++) sum += x[i] * y[i]; MPI_Allreduce(MPI_IN_PLACE, \u0026amp;sum, 1, MPI_DOUBLE, MPI_SUM, comm);   Work: $2N$ flops processed rate $R$ Execution time: $\\frac{2N}{RP} + \\text{latency}$ How big is latency?\n%matplotlib inline import matplotlib.pyplot as plt plt.style.use('seaborn') import numpy as np P = np.geomspace(2, 1e6) N = 1e9 # length of vectors R = 10e9/8 # (10 GB/s per core) (2 flops/16 bytes) = 10/8 GF/s per core t1 = 2e-6 # 2 µs message latency def time_compute(P): return 2*N / (R*P) plt.loglog(P, time_compute(P) + t1*(P-1), label='linear') plt.loglog(P, time_compute(P) + t1*2*(np.sqrt(P)-1), label='2D mesh') plt.loglog(P, time_compute(P) + t1*np.log2(P), label='hypercube') plt.xlabel('P processors') plt.ylabel('Execution time') plt.legend();   Torus topology  3D torus: IBM BlueGene/L (2004) and BlueGene/P (2007) 5D torus: IBM BlueGene/Q (2011) 6D torus: Fujitsu K computer (2011)  Dragonfly topology Today\u0026rsquo;s research: reducing contention and interference Images from this article.\nCompare to BG/Q  Each job gets an electrically isolated 5D torus Excellent performance and reproducibility Awkward constraints on job size, lower system utilization.  Outer product $$ C_{ij} = x_i y_j $$\n Data in: $2N$ Data out: $N^2$  Matrix-vector products $$ yi = \\sum{j} A_{ij} x_j $$\nHow to partition the matrix $A$ across $P$ processors?\n1D row partition  Every process needs entire vector $x$: MPI_Allgather Matrix data does not move Execution time $$ \\underbrace{\\frac{2N^2}{RP}}_{\\text{compute}} + \\underbrace{t_1 \\log2 P}{\\text{latency}} + \\underbrace{tb N \\frac{P-1}{P}}{\\text{bandwidth}} $$  2D partition  Blocks of size $N/\\sqrt{P}$ \u0026ldquo;diagonal\u0026rdquo; ranks hold the input vector Broadcast $x$ along columns: MPI_Bcast Perform local compute Sum y along rows: MPI_Reduce with roots on diagonal Execution time $$ \\underbrace{\\frac{2N^2}{RP}}_{\\text{compute}} + \\underbrace{2 t_1 \\log2 P}{\\text{latency}} + \\underbrace{\\frac{2 tb N}{\\sqrt{P}}}{\\text{bandwidth}} $$  N = 1e4 tb = 8 / 1e9 # 8 bytes / (1 GB/s) ~ bandwidth per core in units of double tb *= 100 plt.loglog(P, (2*N**2)/(R*P) + t1*np.log2(P) + tb*N*(P-1)/P, label='1D distribution') plt.loglog(P, (2*N**2)/(R*P) + 2*t1*np.log2(P) + 2*tb*N/np.sqrt(P), label='2D distribution') plt.xlabel('P processors') plt.ylabel('Execution time') plt.legend();  ","date":1569847765,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569876654,"objectID":"73983c1b9114b055d089bf9919572878","permalink":"https://cucs-hpsc.github.io/fall2019/dense-linalg/","publishdate":"2019-09-30T06:49:25-06:00","relpermalink":"/fall2019/dense-linalg/","section":"fall2019","summary":"Inner products $$ x^T y = \\sum_{i=1}^N x_i y_i $$\nOpenMP The vectors x and y of length N are stored in a contiguous array in shared memory.\ndouble sum = 0; #pragma omp parallel for reduction(+:sum) for (int i=0; i\u0026lt;N; i++) sum += x[i] * y[i];  MPI The vectors x and y are partitioned into $P$ parts of length $np$ such that $$ N = \\sum{p=1}^P n_p .","tags":null,"title":"Dense Linear Algebra and Networks","type":"docs"},{"authors":null,"categories":null,"content":" def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)  Processes and Threads Threads and processes are very similar * Both created via clone system call on Linux * Scheduled in the same way by the operating system * Separate stacks (automatic variables) * Access to same memory before fork() or clone()\nwith some important distinctions\n Threads set CLONE_VM  threads share the same virtual-to-physical address mapping threads can access the same data at the same addresses; private data is private only because other threads don\u0026rsquo;t know its address  Threads set CLONE_FILES  threads share file descriptors  Threads set CLONE_THREAD, CLONE_SIGHAND  process id and signal handlers shared   Myths  Processes can\u0026rsquo;t share memory  mmap(), shm_open(), and MPI_Win_allocate_shared()  Processes are \u0026ldquo;heavy\u0026rdquo;  same data structures and kernel scheduling; no difference in context switching data from parent is inherited copy-on-write at very low overhead startup costs ~100 microseconds to duplicate page tables caches are physically tagged; processes can share L1 cache   MPI: Message Passing Interface  Just a library: plain C, C++, or Fortran compiler  Two active open source libraries: MPICH and Open MPI Numerous vendor implementations modify/extend these open source implementations MVAPICH is an MPICH-derived open source implementation for InfiniBand and related networks  Bindings from many other languages; mpi4py is popular Scales to millions of processes across ~100k nodes  Shared memory systems can be scaled up to ~4000 cores, but latency and price ($) increase  Standard usage: processes are separate on startup Timeline\n MPI-1 (1994) point-to-point messaging, collectives MPI-2 (1997) parallel IO, dynamic processes, one-sided MPI-3 (2012) nonblocking collectives, neighborhood collectives, improved one-sided\nrender_c('mpi-demo.c')  #include \u0026lt;mpi.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main(int argc, char **argv) { MPI_Init(\u0026amp;argc, \u0026amp;argv); // Must call before any other MPI functions int size, rank, sum; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;size); MPI_Allreduce(\u0026amp;rank, \u0026amp;sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD); printf(\u0026quot;I am rank %d of %d: sum=%d\\n\u0026quot;, rank, size, sum); MPI_Finalize(); }    This may remind you of the top-level OpenMP strategy\nint main() { #pragma omp parallel { int rank = omp_get_thread_num(); int size = omp_get_num_threads(); // your code } }   We use the compiler wrapper mpicc, but it just passes some flags to the real compiler.\n! mpicc -show  gcc -pthread -Wl,-rpath -Wl,/usr/lib/openmpi -Wl,\u0026ndash;enable-new-dtags -L/usr/lib/openmpi -lmpi\n! make CC=mpicc CFLAGS=-Wall mpi-demo  mpicc -Wall mpi-demo.c -o mpi-demo\n We use mpiexec to run locally. Clusters/supercomputers often have different job launching programs.\n! mpiexec -n 2 ./mpi-demo  I am rank 0 of 2: sum=1 I am rank 1 of 2: sum=1\n We can run more MPI processes than cores (or hardware threads), but you might need to use the --oversubscribe option because oversubscription is usually expensive.\n! mpiexec -n 6 --oversubscribe ./mpi-demo  I am rank 1 of 6: sum=15 I am rank 3 of 6: sum=15 I am rank 4 of 6: sum=15 I am rank 5 of 6: sum=15 I am rank 0 of 6: sum=15 I am rank 2 of 6: sum=15\n You can use OpenMP within ranks of MPI (but use MPI_Init_thread())\n Everything is private by default\n  Advice from Bill Gropp  You want to think about how you decompose your data structures, how you think about them globally. [\u0026hellip;] If you were building a house, you\u0026rsquo;d start with a set of blueprints that give you a picture of what the whole house looks like. You wouldn\u0026rsquo;t start with a bunch of tiles and say. \u0026ldquo;Well I\u0026rsquo;ll put this tile down on the ground, and then I\u0026rsquo;ll find a tile to go next to it.\u0026rdquo; But all too many people try to build their parallel programs by creating the smallest possible tiles and then trying to have the structure of their code emerge from the chaos of all these little pieces. You have to have an organizing principle if you\u0026rsquo;re going to survive making your code parallel. \u0026ndash; https://www.rce-cast.com/Podcast/rce-28-mpich2.html\n Communicators  MPI_COMM_WORLD contains all ranks in the mpiexec. Those ranks may be on different nodes, even in different parts of the world. MPI_COMM_SELF contains only one rank Can create new communicators from existing ones\nint MPI_Comm_dup(MPI_Comm comm, MPI_Comm *newcomm); int MPI_Comm_split(MPI_Comm comm, int color, int key, MPI_Comm *newcomm); int MPI_Comm_create(MPI_Comm comm, MPI_Group group, MPI_Comm *newcomm);  Can spawn new processes (but not supported on all machines)\nint MPI_Comm_spawn(const char *command, char *argv[], int maxprocs, MPI_Info info, int root, MPI_Comm comm, MPI_Comm *intercomm, int array_of_errcodes[]);  Can attach attributes to communicators (useful for library composition)\n  Collective operations MPI has a rich set of collective operations scoped by communicator, including the following.\nint MPI_Allreduce(const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm); int MPI_Reduce(const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm); int MPI_Scan(const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm); int MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm); int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm);   Implementations are optimized by vendors for their custom networks, and can be very fast.  Notice how the time is basically independent of number of processes $P$, and only a small multiple of the cost to send a single message. Not all networks are this good.\nPoint-to-point messaging In addition to collectives, MPI supports messaging directly between individual ranks.\n Interfaces can be:  blocking like MPI_Send() and MPI_Recv(), or \u0026ldquo;immediate\u0026rdquo; (asynchronous), like MPI_Isend() and MPI_Irecv(). The immediate varliants return an MPI_Request, which must be waited on to complete the send or receive.  Be careful of deadlock when using blocking interfaces.  I never use blocking send/recv. There are also \u0026ldquo;synchronous\u0026rdquo; MPI_Ssend and \u0026ldquo;buffered\u0026rdquo; MPI_Bsend, and nonblocking variants of these, MPI_Issend, etc. I never use these either (with one cool exception that we\u0026rsquo;ll talk about).  Point-to-point messaging is like the assembly of parallel computing  It can be good for building libraries, but it\u0026rsquo;s a headache to use directly for most purposes Better to use collectives when possible, or higher level libraries   Neighbors A common pattern involves communicating with neighbors, often many times in sequence (such as each iteration or time step).\nThis can be achieved with * Point-to-point: MPI_Isend, MPI_Irecv, MPI_Waitall * Persistent: MPI_Send_init (once), MPI_Startall, MPI_Waitall. * Neighborhood collectives (need to create special communicator) * One-sided (need to manage safety yourself)\n","date":1569329365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569431049,"objectID":"1e8091c595a0bafafaf7bb9dea132b67","permalink":"https://cucs-hpsc.github.io/fall2019/intro-mpi/","publishdate":"2019-09-24T06:49:25-06:00","relpermalink":"/fall2019/intro-mpi/","section":"fall2019","summary":"def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)  Processes and Threads Threads and processes are very similar * Both created via clone system call on Linux * Scheduled in the same way by the operating system * Separate stacks (automatic variables) * Access to same memory before fork() or clone()\nwith some important distinctions\n Threads set CLONE_VM  threads share the same virtual-to-physical address mapping threads can access the same data at the same addresses; private data is private only because other threads don\u0026rsquo;t know its address  Threads set CLONE_FILES  threads share file descriptors  Threads set CLONE_THREAD, CLONE_SIGHAND  process id and signal handlers shared   Myths  Processes can\u0026rsquo;t share memory  mmap(), shm_open(), and MPI_Win_allocate_shared()  Processes are \u0026ldquo;heavy\u0026rdquo;  same data structures and kernel scheduling; no difference in context switching data from parent is inherited copy-on-write at very low overhead startup costs ~100 microseconds to duplicate page tables caches are physically tagged; processes can share L1 cache   MPI: Message Passing Interface  Just a library: plain C, C++, or Fortran compiler  Two active open source libraries: MPICH and Open MPI Numerous vendor implementations modify/extend these open source implementations MVAPICH is an MPICH-derived open source implementation for InfiniBand and related networks  Bindings from many other languages; mpi4py is popular Scales to millions of processes across ~100k nodes  Shared memory systems can be scaled up to ~4000 cores, but latency and price ($) increase  Standard usage: processes are separate on startup Timeline","tags":null,"title":"Introduction to MPI","type":"docs"},{"authors":null,"categories":null,"content":" %matplotlib inline import matplotlib.pyplot as plt import pandas import numpy as np import itertools plt.style.use('seaborn')  Bitonic sorting Definition: bitonic sequence A bitonic sequence of length $n$ satisfies $$ x_0 \\le x_1 \\le \\dotsb \\le xk \\ge x{k+1} \\ge \\dotsb \\ge x_{n-1} $$ or a cyclic shift thereof, $$\\hat xi = x{i+c \\bmod n} .$$\nsorting relies on the bitonic swapping operation,\ndef bitonic_split(x): L = len(x) // 2 for i in range(L): # each pair is independent if (x[i] \u0026gt; x[L + i]): x[i], x[L + i] = x[L + i], x[i]  after which the resulting subsequences x[:L] and x[L:] are bitonic and max(x[:L]) \u0026lt;= min(x[L:]). (It is beyond the scope of this class to formally prove this property, but we\u0026rsquo;ll show examples.)\nExample x = list(range(10, 22)) + [16, 13, 10, 7] idx = np.arange(len(x)) plt.plot(x, 's', label='orig') bitonic_split(x) plt.plot(x, '^k', label='swapped') plt.legend();  Note that the bitonic sequence on the right side is cyclicly permuted.\nPutting it together  The blue blocks are forward bitonic merge networks, consisting of bitonic_split followed by recursive splits. The green blocks are reverse networks. The left half of the diagram constructs a global bitonic sequence that is increasing in the top half and decreasing in the bottom half. This bitonic sequence is balanced and not \u0026ldquo;shifted\u0026rdquo;. The right half of the diagram merges a global bitonic sequence. The bitonic sequences produced in each stage of the merge may be unbalanced and/or cyclicly shifted.  Demo def bitonic_sort(up, x, start=0, end=None, plot=[]): if end is None: end = len(x) if end - start \u0026lt;= 1: return mid = start + (end - start) // 2 bitonic_sort(True, x, start, mid) bitonic_sort(False, x, mid, end) if (start, end) in plot: bitonic_plot(x, start, end, 'sort') bitonic_merge(up, x, start, end, plot=plot) def bitonic_merge(up, x, start, end, plot=False): # assume input x is bitonic, and sorted list is returned if end - start == 1: return bitonic_split2(up, x, start, end) if (start, end) in plot: bitonic_plot(x, start, end, 'merge') mid = start + (end - start) // 2 bitonic_merge(up, x, start, mid, plot=plot) bitonic_merge(up, x, mid, end, plot=plot) def bitonic_split2(up, x, start, end): L = (end - start) // 2 for i in range(start, start + L): if (x[i] \u0026gt; x[L + i]) == up: x[i], x[L + i] = x[L + i], x[i] # swap def bitonic_plot(x, start, end, phase): plt.plot(range(start, end), x[start:end], next(marker), label=f'{phase} {start}:{end}') marker = itertools.cycle(['s', 'o', '^', '\u0026lt;', '*']) x = list(range(10, 38)) + [16, 13, 10, 7] plt.plot(x, next(marker), label='orig') bitonic_sort(True, x, plot=[(0, 32), (0,16), (8,16)]) #plt.plot(x, next(marker), label='sorted') plt.legend();  Further resources on Sorting  Chapter 9 of Grama, Gupta, Karypis, Kumar (2003), Introduction to Parallel Computing Grama slides  Graphs An (undirected) graph $(V, E)$ is a set of vertices $V$ and unordered pairs $(u,v) = (v,u) \\in E$ of vertices $u,v \\in V$.\nGraphs are often expressed by their adjacency matrix of dimension $n\\times n$ where $n = |V|$, $$ A_{ij} = \\begin{cases} 1, \u0026amp; \\text{if } (i,j) \\in E 0, \u0026amp; \\text{otherwise} \\end{cases} $$\nimport networkx as nx G = nx.grid_2d_graph(3, 3) nx.draw(G, with_labels=True)  A = nx.adjacency_matrix(G) A.todense()  matrix([[0, 1, 0, 1, 0, 0, 0, 0, 0], [1, 0, 1, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 1, 0, 1, 0, 0], [0, 1, 0, 1, 0, 1, 0, 1, 0], [0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 1, 0, 1, 0]], dtype=int64)  Compressed representation Adjacency matrices often have many zeros so it\u0026rsquo;s common to store a compressed representation. We\u0026rsquo;ll revisit such formats for sparse matrices.\nA.indptr, A.indices  (array([ 0, 2, 5, 7, 10, 14, 17, 19, 22, 24], dtype=int32), array([1, 3, 0, 2, 4, 1, 5, 0, 4, 6, 1, 3, 5, 7, 2, 4, 8, 3, 7, 4, 6, 8, 5, 7], dtype=int32))  for row in range(A.shape[0]): print(A.indices[A.indptr[row]:A.indptr[row+1]])  [1 3] [0 2 4] [1 5] [0 4 6] [1 3 5 7] [2 4 8] [3 7] [4 6 8] [5 7]  Maximal independent set (MIS) An independent set is a set of vertices $S \\subset V$ such that $(u,v) \\notin E$ for any pair $u,v \\in S$.\nmis = nx.maximal_independent_set(G) mis  [(2, 0), (1, 2), (0, 0)]  def plot_mis(G, mis): node_colors = ['red' if n in mis else '#1f78b4' for n in G.nodes()] nx.draw_networkx(G, node_color = node_colors) plot_mis(G, mis)  # Maximal independent sets are not unique plot_mis(G, [(0,0), (0,2), (1,1), (2,0), (2,2)])  # We can coax the greedy algorithm to give a better MIS by specifying # some nodes to include plot_mis(G, nx.maximal_independent_set(G, [(1,1)]))  Greedy Algorithms  Start with all vertices in candidate set $C = V$, empty $S$ While $C \\ne \\emptyset$: Choose a vertex $v \\in C$  Add $v$ to $S$ Remove $v$ and all neighbors of $v$ from $C$   Algorithms differ in how they choose the next vertex $v \\in C$.\nTiebreaking Suppose we index the vertices by integer and allow parallel selection of any $v$ for which $$ v \u0026lt; \\mathcal N(v) . $$\nHash variant Consider a hash function $h(v)$ and allow any time\n$$ h(v) \u0026lt; \\min_{u\\in \\mathcal N(v)} h(u). $$\nG = nx.karate_club_graph() plot_mis(G, nx.maximal_independent_set(G))  /usr/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:579: MatplotlibDeprecationWarning: The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead. if not cb.iterable(width):  Further resources  Chapter 10 of Grama, Gupta, Karypis, Kumar (2003), Introduction to Parallel Computing Grama slides  ","date":1569242965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569262182,"objectID":"0ef6beb9d75e084333d3e364f0700c3c","permalink":"https://cucs-hpsc.github.io/fall2019/sorting-graphs/","publishdate":"2019-09-23T06:49:25-06:00","relpermalink":"/fall2019/sorting-graphs/","section":"fall2019","summary":"%matplotlib inline import matplotlib.pyplot as plt import pandas import numpy as np import itertools plt.style.use('seaborn')  Bitonic sorting Definition: bitonic sequence A bitonic sequence of length $n$ satisfies $$ x_0 \\le x_1 \\le \\dotsb \\le xk \\ge x{k+1} \\ge \\dotsb \\ge x_{n-1} $$ or a cyclic shift thereof, $$\\hat xi = x{i+c \\bmod n} .$$\nsorting relies on the bitonic swapping operation,\ndef bitonic_split(x): L = len(x) // 2 for i in range(L): # each pair is independent if (x[i] \u0026gt; x[L + i]): x[i], x[L + i] = x[L + i], x[i]  after which the resulting subsequences x[:L] and x[L:] are bitonic and max(x[:L]) \u0026lt;= min(x[L:]).","tags":null,"title":"More bitonic sorting, graphs","type":"docs"},{"authors":null,"categories":null,"content":" Reductions double reduce(int n, double x[]) { double y = 0; for (int i=0; i\u0026lt;n; i++) y += x[i]; return y; }  DAG properties  Work $W(n) = n$ Depth $D(n) = n$ Parallelism $P(n) = \\frac{W(n)}{D(n)} = 1$  A 2-level method double reduce(int n, double x[]) { int P = sqrt(n); // ways of parallelism double y[P]; #pragma omp parallel for shared(y) for (int p=0; p\u0026lt;P; p++) { y[p] = 0; for (int i=0; i\u0026lt;n/P; i++) y[p] += x[p*(n/P) + i]; } double sum = 0; for (int p=0; p\u0026lt;P; p++) sum += y[p]; return sum; }  DAG properties  Work $W(n) = n + \\sqrt{n}$ Depth $D(n) = 2 \\sqrt{n}$ Parallelism $P(n) = \\sqrt{n}$  PRAM performance model  Processing units (e.g., OpenMP threads) execute local programs Communication through shared memory with no access cost Synchronous operation on a common clock  Barrier-like constructs are free  Multiple Instruction, Multiple Data (MIMD)  Scheduling How much time does it take to execute a DAG on $p$ processors?\n Sum work of each node $i$ along critical path of length $D(n)$ $$ \\sum_{i=1}^{D(n)} W_i $$\n Partition total work $W(n)$ over $p \\le P(n)$ processors (as though there were no data dependencies) $$ \\left\\lceil \\frac{W(n)}{p} \\right\\rceil $$\n Total time must be at least as large as either of these $$ T(n,p) \\ge \\max\\left( D(n), \\left\\lceil \\frac{W(n)}{p} \\right\\rceil \\right) $$\n  More levels? double reduce(int n, double x[]) { if (n == 1) return x[0]; double y[n/2]; #pragma omp parallel for shared(y) for (int i=0; i\u0026lt;n/2; i++) y[i] = x[2*i] + x[2*i+1]; return reduce(n/2, y); }  DAG properties  $W(n) = n/2 + n/4 + n/8 + \\dotsb = n$ $D(n) = \\log_2 n$ $P(n) = n/2$  Parallel scans void scan(int n, double x[], double y[]) { y[0] = x[0]; for (int i=1; i\u0026lt;n; i++) y[i] = y[i-1] + x[i]; }   What are the DAG properties of this algorithm? How fast can we make it?  void scan_inplace(int n, double y[], int stride) { if (2*stride \u0026gt; n) return; #pragma omp parallel for for (int i=2*stride-1; i\u0026lt;n; i+=2*stride) y[i] += [i - stride]; scan(n, y, 2*stride); #pragma omp parallel for for (int i=3*stride-1; i\u0026lt;n; i+=2*stride) y[i] += y[i - stride]; } // call like scan_inplace(n, x, 1);  Application of scans: parallel select Select elements of array x[] that satisfy a condition.\nint c[n]; #pragma omp parallel for for (int i=0; i\u0026lt;n; i++) c[i] = cond(x[i]); // returns 1 or 0 scan_inplace(n, c, 1); double results[c[n-1]]; // allocate array with total number of items #pragma omp parallel for for (int i=0; i\u0026lt;n; i++) if (cond(x[i])) // Can use `c[i] - c[i-1]` to avoid recomputing results[c[i]-1] = x[i];  Figures courtesy Abtin Rahimian\u0026rsquo;s course notes.\n","date":1568810965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568830401,"objectID":"07d9aa3ba8ce6001dc3023f1d728d037","permalink":"https://cucs-hpsc.github.io/fall2019/strategies/","publishdate":"2019-09-18T06:49:25-06:00","relpermalink":"/fall2019/strategies/","section":"fall2019","summary":"Reductions double reduce(int n, double x[]) { double y = 0; for (int i=0; i\u0026lt;n; i++) y += x[i]; return y; }  DAG properties  Work $W(n) = n$ Depth $D(n) = n$ Parallelism $P(n) = \\frac{W(n)}{D(n)} = 1$  A 2-level method double reduce(int n, double x[]) { int P = sqrt(n); // ways of parallelism double y[P]; #pragma omp parallel for shared(y) for (int p=0; p\u0026lt;P; p++) { y[p] = 0; for (int i=0; i\u0026lt;n/P; i++) y[p] += x[p*(n/P) + i]; } double sum = 0; for (int p=0; p\u0026lt;P; p++) sum += y[p]; return sum; }  DAG properties  Work $W(n) = n + \\sqrt{n}$ Depth $D(n) = 2 \\sqrt{n}$ Parallelism $P(n) = \\sqrt{n}$  PRAM performance model  Processing units (e.","tags":null,"title":"Parallel Reductions and Scans","type":"docs"},{"authors":null,"categories":null,"content":" def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)  Using #pragma omp task Up to now, we\u0026rsquo;ve been expressing parallelism for iterating over an array.\nrender_c('task_dep.4.c')  #include \u0026lt;stdio.h\u0026gt; int main() { int x = 1; #pragma omp parallel #pragma omp single { #pragma omp task shared(x) depend(out: x) x = 2; #pragma omp task shared(x) depend(in: x) printf(\u0026quot;x + 1 = %d. \u0026quot;, x+1); #pragma omp task shared(x) depend(in: x) printf(\u0026quot;x + 2 = %d. \u0026quot;, x+2); } puts(\u0026quot;\u0026quot;); return 0; }  !make CFLAGS=-fopenmp -B task_dep.4  cc -fopenmp task_dep.4.c -o task_dep.4  !for i in {1..10}; do ./task_dep.4; done  x + 2 = 4. x + 1 = 3. x + 1 = 3. x + 2 = 4. x + 2 = 4. x + 1 = 3. x + 1 = 3. x + 2 = 4. x + 2 = 4. x + 1 = 3. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3.  render_c('task_dep.4inout.c')  #include \u0026lt;stdio.h\u0026gt; int main() { int x = 1; #pragma omp parallel #pragma omp single { #pragma omp task shared(x) depend(out: x) x = 2; #pragma omp task shared(x) depend(inout: x) printf(\u0026quot;x + 1 = %d. \u0026quot;, x+1); #pragma omp task shared(x) depend(in: x) printf(\u0026quot;x + 2 = %d. \u0026quot;, x+2); } puts(\u0026quot;\u0026quot;); return 0; }  !make CFLAGS=-fopenmp -B task_dep.4inout  cc -fopenmp task_dep.4inout.c -o task_dep.4inout  !for i in {1..10}; do ./task_dep.4inout; done  x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4.  Computing the Fibonacci numbers with OpenMP Fibonacci numbers are defined by the recurrence \\begin{align} F_0 \u0026amp;= 0 F_1 \u0026amp;= 1 Fn \u0026amp;= F{n-1} + F_{n-2} \\end{align}\nrender_c('fib.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; long fib(long n) { if (n \u0026lt; 2) return n; return fib(n - 1) + fib(n - 2); } int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; #pragma omp parallel for for (long i=0; i\u0026lt;N; i++) fibs[i] = fib(i+1); for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' -B fib  cc -O2 -march=native -fopenmp -Wall fib.c -o fib  !OMP_NUM_THREADS=4 time ./fib 40   1: 1 2: 1 3: 2 4: 3 5: 5 6: 8 7: 13 8: 21 9: 34 10: 55 11: 89 12: 144 13: 233 14: 377 15: 610 16: 987 17: 1597 18: 2584 19: 4181 20: 6765 21: 10946 22: 17711 23: 28657 24: 46368 25: 75025 26: 121393 27: 196418 28: 317811 29: 514229 30: 832040 31: 1346269 32: 2178309 33: 3524578 34: 5702887 35: 9227465 36: 14930352 37: 24157817 38: 39088169 39: 63245986 40: 102334155 0.85user 0.00system 0:00.78elapsed 109%CPU (0avgtext+0avgdata 2044maxresident)k 0inputs+0outputs (0major+99minor)pagefaults 0swaps  Use tasks render_c('fib2.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; long fib(long n) { if (n \u0026lt; 2) return n; long n1, n2; #pragma omp task shared(n1) n1 = fib(n - 1); #pragma omp task shared(n2) n2 = fib(n - 2); #pragma omp taskwait return n1 + n2; } int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; #pragma omp parallel #pragma omp single nowait { for (long i=0; i\u0026lt;N; i++) fibs[i] = fib(i+1); } for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' fib2  make: 'fib2' is up to date.  !OMP_NUM_THREADS=2 time ./fib2 30   1: 1 2: 1 3: 2 4: 3 5: 5 6: 8 7: 13 8: 21 9: 34 10: 55 11: 89 12: 144 13: 233 14: 377 15: 610 16: 987 17: 1597 18: 2584 19: 4181 20: 6765 21: 10946 22: 17711 23: 28657 24: 46368 25: 75025 26: 121393 27: 196418 28: 317811 29: 514229 30: 832040 2.42user 0.81system 0:02.54elapsed 127%CPU (0avgtext+0avgdata 2028maxresident)k 0inputs+0outputs (0major+100minor)pagefaults 0swaps   It\u0026rsquo;s expensive to create tasks when n is small, even with only one thread. How can we cut down on that overhead?\nrender_c('fib3.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; long fib(long n) { if (n \u0026lt; 2) return n; if (n \u0026lt; 30) return fib(n - 1) + fib(n - 2); long n1, n2; #pragma omp task shared(n1) n1 = fib(n - 1); #pragma omp task shared(n2) n2 = fib(n - 2); #pragma omp taskwait return n1 + n2; } int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; #pragma omp parallel #pragma omp single nowait { for (long i=0; i\u0026lt;N; i++) fibs[i] = fib(i+1); } for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' fib3  cc -O2 -march=native -fopenmp -Wall fib3.c -o fib3\n!OMP_NUM_THREADS=3 time ./fib3 40  1: 1 2: 1 3: 2 4: 3 5: 5 6: 8 7: 13 8: 21 9: 34 10: 55 11: 89 12: 144 13: 233 14: 377 15: 610 16: 987 17: 1597 18: 2584 19: 4181 20: 6765 21: 10946 22: 17711 23: 28657 24: 46368 25: 75025 26: 121393 27: 196418 28: 317811 29: 514229 30: 832040 31: 1346269 32: 2178309 33: 3524578 34: 5702887 35: 9227465 36: 14930352 37: 24157817 38: 39088169 39: 63245986 40: 102334155 3.56user 0.00system 0:01.27elapsed 280%CPU (0avgtext+0avgdata 1920maxresident)k 0inputs+0outputs (0major+103minor)pagefaults 0swaps\n This is just slower, even with one thread. Why might that be?\nrender_c('fib4.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; long fib_seq(long n) { if (n \u0026lt; 2) return n; return fib_seq(n - 1) + fib_seq(n - 2); } long fib(long n) { if (n \u0026lt; 30) return fib_seq(n); long n1, n2; #pragma omp task shared(n1) n1 = fib(n - 1); #pragma omp task shared(n2) n2 = fib(n - 2); #pragma omp taskwait return n1 + n2; } int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; #pragma omp parallel #pragma omp single nowait { for (long i=0; i\u0026lt;N; i++) fibs[i] = fib(i+1); } for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' fib4  make: \u0026lsquo;fib4\u0026rsquo; is up to date.\n!OMP_NUM_THREADS=2 time ./fib4 40  1: 1 2: 1 3: 2 4: 3 5: 5 6: 8 7: 13 8: 21 9: 34 10: 55 11: 89 12: 144 13: 233 14: 377 15: 610 16: 987 17: 1597 18: 2584 19: 4181 20: 6765 21: 10946 22: 17711 23: 28657 24: 46368 25: 75025 26: 121393 27: 196418 28: 317811 29: 514229 30: 832040 31: 1346269 32: 2178309 33: 3524578 34: 5702887 35: 9227465 36: 14930352 37: 24157817 38: 39088169 39: 63245986 40: 102334155 0.94user 0.00system 0:00.53elapsed 177%CPU (0avgtext+0avgdata 2040maxresident)k 8inputs+0outputs (0major+97minor)pagefaults 0swaps\n  Alt: schedule(static,1) render_c('fib5.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; long fib(long n) { if (n \u0026lt; 2) return n; return fib(n - 1) + fib(n - 2); } int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; #pragma omp parallel for schedule(static,1) for (long i=0; i\u0026lt;N; i++) fibs[i] = fib(i+1); for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' fib5  make: 'fib5' is up to date.  !OMP_NUM_THREADS=2 time ./fib5 40   1: 1 2: 1 3: 2 4: 3 5: 5 6: 8 7: 13 8: 21 9: 34 10: 55 11: 89 12: 144 13: 233 14: 377 15: 610 16: 987 17: 1597 18: 2584 19: 4181 20: 6765 21: 10946 22: 17711 23: 28657 24: 46368 25: 75025 26: 121393 27: 196418 28: 317811 29: 514229 30: 832040 31: 1346269 32: 2178309 33: 3524578 34: 5702887 35: 9227465 36: 14930352 37: 24157817 38: 39088169 39: 63245986 40: 102334155 0.88user 0.00system 0:00.54elapsed 161%CPU (0avgtext+0avgdata 1908maxresident)k 8inputs+0outputs (0major+93minor)pagefaults 0swaps  Better math render_c('fib6.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; fibs[0] = 1; fibs[1] = 2; for (long i=2; i\u0026lt;N; i++) fibs[i] = fibs[i-1] + fibs[i-2]; for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' fib6  cc -O2 -march=native -fopenmp -Wall fib6.c -o fib6  !time ./fib6 100   1: 1 2: 2 3: 3 4: 5 5: 8 6: 13 7: 21 8: 34 9: 55 10: 89 11: 144 12: 233 13: 377 14: 610 15: 987 16: 1597 17: 2584 18: 4181 19: 6765 20: 10946 21: 17711 22: 28657 23: 46368 24: 75025 25: 121393 26: 196418 27: 317811 28: 514229 29: 832040 30: 1346269 31: 2178309 32: 3524578 33: 5702887 34: 9227465 35: 14930352 36: 24157817 37: 39088169 38: 63245986 39: 102334155 40: 165580141 41: 267914296 42: 433494437 43: 701408733 44: 1134903170 45: 1836311903 46: 2971215073 47: 4807526976 48: 7778742049 49: 12586269025 50: 20365011074 51: 32951280099 52: 53316291173 53: 86267571272 54: 139583862445 55: 225851433717 56: 365435296162 57: 591286729879 58: 956722026041 59: 1548008755920 60: 2504730781961 61: 4052739537881 62: 6557470319842 63: 10610209857723 64: 17167680177565 65: 27777890035288 66: 44945570212853 67: 72723460248141 68: 117669030460994 69: 190392490709135 70: 308061521170129 71: 498454011879264 72: 806515533049393 73: 1304969544928657 74: 2111485077978050 75: 3416454622906707 76: 5527939700884757 77: 8944394323791464 78: 14472334024676221 79: 23416728348467685 80: 37889062373143906 81: 61305790721611591 82: 99194853094755497 83: 160500643816367088 84: 259695496911122585 85: 420196140727489673 86: 679891637638612258 87: 1100087778366101931 88: 1779979416004714189 89: 2880067194370816120 90: 4660046610375530309 91: 7540113804746346429 92: -6246583658587674878 93: 1293530146158671551 94: -4953053512429003327 95: -3659523366270331776 96: -8612576878699335103 97: 6174643828739884737 98: -2437933049959450366 99: 3736710778780434371 100: 1298777728820984005 0.002 real 0.002 user 0.000 sys 99.42 cpu  To fork/join or to task? When the work unit size and compute speed is predictable, we can partition work in advance and schedule with omp for to achieve load balance. Satisfying both criteria is often hard:\n Adaptive algorithms, adaptive physics, implicit constitutive models AVX throttling, thermal throttling, network or file system contention, OS jitter  Fork/join and barriers are also high overhead, so we might want to express data dependencies more precisely.\nFor tasking to be efficient, it relies on overdecomposition, creating more work units than there are processing units. For many numerical algorithms, there is some overhead to overdecomposition. For example, in array processing, a halo/fringe/ghost/overlap region might need to be computed as part of each work patch, leading to time models along the lines of $$ t{\\text{tile}}(n) = t{\\text{latency}} + \\frac{(n+2)^3}{R} $$ where $R$ is the processing rate. In addition to the latency, the overhead fraction is $$ \\frac{(n+2)^3 - n^3}{n^3} \\approx 6/n $$ indicating that larger $n$ should be more efficient.\nHowever, if this overhead is acceptable and you still have load balancing challenges, tasking can be a solution. (Example from a recent blog/talk.)\nComputational depth and the critical path Consider the block Cholesky factorization algorithm (applying to the lower-triangular matrix $A$).\nExpressing essential data dependencies, this results in the following directed acyclic graph (DAG). No parallel algorithm can complete in less time than it takes for a sequential algorithm to perform each operation along the critical path (i.e., the minimum depth of this graph such that all arrows point downward).\nFigures from Agullo et al (2016): Are Static Schedules so Bad? A Case Study on Cholesky Factorization, which is an interesting counterpoint to the common narrative pushing dynamic scheduling.\nQuestion: what is the computational depth of summing an array? $$ \\sum_{i=0}^{N-1} a_i $$\ndouble sum = 0; for (int i=0; i\u0026lt;N; i++) sum += array[i];  Given an arbitrarily large number $P$ of processing units, what is the smallest computational depth to compute this mathematical result? (You\u0026rsquo;re free to use any associativity.)\n","date":1568638165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568670092,"objectID":"d659cdd715e46e4c3fe8dccf47ebd86c","permalink":"https://cucs-hpsc.github.io/fall2019/openmp-3/","publishdate":"2019-09-16T06:49:25-06:00","relpermalink":"/fall2019/openmp-3/","section":"fall2019","summary":"def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)  Using #pragma omp task Up to now, we\u0026rsquo;ve been expressing parallelism for iterating over an array.\nrender_c('task_dep.4.c')  #include \u0026lt;stdio.h\u0026gt; int main() { int x = 1; #pragma omp parallel #pragma omp single { #pragma omp task shared(x) depend(out: x) x = 2; #pragma omp task shared(x) depend(in: x) printf(\u0026quot;x + 1 = %d.","tags":null,"title":"OpenMP Tasks","type":"docs"},{"authors":null,"categories":null,"content":" What does the compiler do when we add #pragma omp parallel?\nstatic double dot_opt3(size_t n, const double *a, const double *b) { double sum = 0; omp_set_num_threads(4); #pragma omp parallel { #pragma omp for reduction(+:sum) for (size_t i=0; i\u0026lt;n; i++) sum += a[i] * b[i]; } return sum; }  gcc -Os -march=native -fopenmp dot.c -o dot objdump -d --prefix-addresses -M intel dot | grep dot_opt3  000000000000129f \u0026lt;main+0x1af\u0026gt; call 0000000000001779 \u0026lt;dot_opt3\u0026gt; 0000000000001779 \u0026lt;dot_opt3\u0026gt; push r12 000000000000177b \u0026lt;dot_opt3+0x2\u0026gt; mov r12,rdx 000000000000177e \u0026lt;dot_opt3+0x5\u0026gt; push rbp 000000000000177f \u0026lt;dot_opt3+0x6\u0026gt; mov rbp,rsi 0000000000001782 \u0026lt;dot_opt3+0x9\u0026gt; push rbx 0000000000001783 \u0026lt;dot_opt3+0xa\u0026gt; mov rbx,rdi 0000000000001786 \u0026lt;dot_opt3+0xd\u0026gt; mov edi,0x4 000000000000178b \u0026lt;dot_opt3+0x12\u0026gt; sub rsp,0x30 000000000000178f \u0026lt;dot_opt3+0x16\u0026gt; mov rax,QWORD PTR fs:0x28 0000000000001798 \u0026lt;dot_opt3+0x1f\u0026gt; mov QWORD PTR [rsp+0x28],rax 000000000000179d \u0026lt;dot_opt3+0x24\u0026gt; xor eax,eax 000000000000179f \u0026lt;dot_opt3+0x26\u0026gt; call 0000000000001070 \u0026lt;omp_set_num_threads@plt\u0026gt; 00000000000017a4 \u0026lt;dot_opt3+0x2b\u0026gt; xor ecx,ecx 00000000000017a6 \u0026lt;dot_opt3+0x2d\u0026gt; xor edx,edx 00000000000017a8 \u0026lt;dot_opt3+0x2f\u0026gt; lea rsi,[rsp+0x8] 00000000000017ad \u0026lt;dot_opt3+0x34\u0026gt; lea rdi,[rip+0xc1] # 0000000000001875 \u0026lt;dot_opt3._omp_fn.0\u0026gt; 00000000000017b4 \u0026lt;dot_opt3+0x3b\u0026gt; mov QWORD PTR [rsp+0x18],r12 00000000000017b9 \u0026lt;dot_opt3+0x40\u0026gt; mov QWORD PTR [rsp+0x10],rbp 00000000000017be \u0026lt;dot_opt3+0x45\u0026gt; mov QWORD PTR [rsp+0x8],rbx 00000000000017c3 \u0026lt;dot_opt3+0x4a\u0026gt; mov QWORD PTR [rsp+0x20],0x0 00000000000017cc \u0026lt;dot_opt3+0x53\u0026gt; call 00000000000010e0 \u0026lt;GOMP_parallel@plt\u0026gt; 00000000000017d1 \u0026lt;dot_opt3+0x58\u0026gt; mov rax,QWORD PTR [rsp+0x28] 00000000000017d6 \u0026lt;dot_opt3+0x5d\u0026gt; xor rax,QWORD PTR fs:0x28 00000000000017df \u0026lt;dot_opt3+0x66\u0026gt; vmovsd xmm0,QWORD PTR [rsp+0x20] 00000000000017e5 \u0026lt;dot_opt3+0x6c\u0026gt; je 00000000000017ec \u0026lt;dot_opt3+0x73\u0026gt; 00000000000017e7 \u0026lt;dot_opt3+0x6e\u0026gt; call 0000000000001080 \u0026lt;__stack_chk_fail@plt\u0026gt; 00000000000017ec \u0026lt;dot_opt3+0x73\u0026gt; add rsp,0x30 00000000000017f0 \u0026lt;dot_opt3+0x77\u0026gt; pop rbx 00000000000017f1 \u0026lt;dot_opt3+0x78\u0026gt; pop rbp 00000000000017f2 \u0026lt;dot_opt3+0x79\u0026gt; pop r12 00000000000017f4 \u0026lt;dot_opt3+0x7b\u0026gt; ret 0000000000001875 \u0026lt;dot_opt3._omp_fn.0\u0026gt; push r12 0000000000001877 \u0026lt;dot_opt3._omp_fn.0+0x2\u0026gt; push rbp 0000000000001878 \u0026lt;dot_opt3._omp_fn.0+0x3\u0026gt; mov rbp,rdi 000000000000187b \u0026lt;dot_opt3._omp_fn.0+0x6\u0026gt; push rbx 000000000000187c \u0026lt;dot_opt3._omp_fn.0+0x7\u0026gt; sub rsp,0x10 0000000000001880 \u0026lt;dot_opt3._omp_fn.0+0xb\u0026gt; mov rbx,QWORD PTR [rdi] 0000000000001883 \u0026lt;dot_opt3._omp_fn.0+0xe\u0026gt; test rbx,rbx 0000000000001886 \u0026lt;dot_opt3._omp_fn.0+0x11\u0026gt; jne 00000000000018b5 \u0026lt;dot_opt3._omp_fn.0+0x40\u0026gt; 0000000000001888 \u0026lt;dot_opt3._omp_fn.0+0x13\u0026gt; vxorpd xmm0,xmm0,xmm0 000000000000188c \u0026lt;dot_opt3._omp_fn.0+0x17\u0026gt; mov rax,QWORD PTR [rbp+0x18] 0000000000001890 \u0026lt;dot_opt3._omp_fn.0+0x1b\u0026gt; lea rdx,[rbp+0x18] 0000000000001894 \u0026lt;dot_opt3._omp_fn.0+0x1f\u0026gt; mov QWORD PTR [rsp],rax 0000000000001898 \u0026lt;dot_opt3._omp_fn.0+0x23\u0026gt; vaddsd xmm1,xmm0,QWORD PTR [rsp] 000000000000189d \u0026lt;dot_opt3._omp_fn.0+0x28\u0026gt; vmovsd QWORD PTR [rsp+0x8],xmm1 00000000000018a3 \u0026lt;dot_opt3._omp_fn.0+0x2e\u0026gt; mov rdi,QWORD PTR [rsp+0x8] 00000000000018a8 \u0026lt;dot_opt3._omp_fn.0+0x33\u0026gt; lock cmpxchg QWORD PTR [rdx],rdi 00000000000018ad \u0026lt;dot_opt3._omp_fn.0+0x38\u0026gt; cmp QWORD PTR [rsp],rax 00000000000018b1 \u0026lt;dot_opt3._omp_fn.0+0x3c\u0026gt; je 000000000000190c \u0026lt;dot_opt3._omp_fn.0+0x97\u0026gt; 00000000000018b3 \u0026lt;dot_opt3._omp_fn.0+0x3e\u0026gt; jmp 0000000000001894 \u0026lt;dot_opt3._omp_fn.0+0x1f\u0026gt; 00000000000018b5 \u0026lt;dot_opt3._omp_fn.0+0x40\u0026gt; call 00000000000010b0 \u0026lt;omp_get_num_threads@plt\u0026gt; 00000000000018ba \u0026lt;dot_opt3._omp_fn.0+0x45\u0026gt; mov r12d,eax 00000000000018bd \u0026lt;dot_opt3._omp_fn.0+0x48\u0026gt; call 0000000000001060 \u0026lt;omp_get_thread_num@plt\u0026gt; 00000000000018c2 \u0026lt;dot_opt3._omp_fn.0+0x4d\u0026gt; movsxd rcx,eax 00000000000018c5 \u0026lt;dot_opt3._omp_fn.0+0x50\u0026gt; movsxd rsi,r12d 00000000000018c8 \u0026lt;dot_opt3._omp_fn.0+0x53\u0026gt; mov rax,rbx 00000000000018cb \u0026lt;dot_opt3._omp_fn.0+0x56\u0026gt; xor edx,edx 00000000000018cd \u0026lt;dot_opt3._omp_fn.0+0x58\u0026gt; div rsi 00000000000018d0 \u0026lt;dot_opt3._omp_fn.0+0x5b\u0026gt; cmp rcx,rdx 00000000000018d3 \u0026lt;dot_opt3._omp_fn.0+0x5e\u0026gt; jb 0000000000001905 \u0026lt;dot_opt3._omp_fn.0+0x90\u0026gt; 00000000000018d5 \u0026lt;dot_opt3._omp_fn.0+0x60\u0026gt; imul rcx,rax 00000000000018d9 \u0026lt;dot_opt3._omp_fn.0+0x64\u0026gt; vxorpd xmm0,xmm0,xmm0 00000000000018dd \u0026lt;dot_opt3._omp_fn.0+0x68\u0026gt; add rdx,rcx 00000000000018e0 \u0026lt;dot_opt3._omp_fn.0+0x6b\u0026gt; add rax,rdx 00000000000018e3 \u0026lt;dot_opt3._omp_fn.0+0x6e\u0026gt; cmp rdx,rax 00000000000018e6 \u0026lt;dot_opt3._omp_fn.0+0x71\u0026gt; jae 000000000000188c \u0026lt;dot_opt3._omp_fn.0+0x17\u0026gt; 00000000000018e8 \u0026lt;dot_opt3._omp_fn.0+0x73\u0026gt; mov rcx,QWORD PTR [rbp+0x10] 00000000000018ec \u0026lt;dot_opt3._omp_fn.0+0x77\u0026gt; mov rsi,QWORD PTR [rbp+0x8] 00000000000018f0 \u0026lt;dot_opt3._omp_fn.0+0x7b\u0026gt; vmovsd xmm2,QWORD PTR [rsi+rdx*8] 00000000000018f5 \u0026lt;dot_opt3._omp_fn.0+0x80\u0026gt; vfmadd231sd xmm0,xmm2,QWORD PTR [rcx+rdx*8] 00000000000018fb \u0026lt;dot_opt3._omp_fn.0+0x86\u0026gt; inc rdx 00000000000018fe \u0026lt;dot_opt3._omp_fn.0+0x89\u0026gt; cmp rax,rdx 0000000000001901 \u0026lt;dot_opt3._omp_fn.0+0x8c\u0026gt; jne 00000000000018f0 \u0026lt;dot_opt3._omp_fn.0+0x7b\u0026gt; 0000000000001903 \u0026lt;dot_opt3._omp_fn.0+0x8e\u0026gt; jmp 000000000000188c \u0026lt;dot_opt3._omp_fn.0+0x17\u0026gt; 0000000000001905 \u0026lt;dot_opt3._omp_fn.0+0x90\u0026gt; inc rax 0000000000001908 \u0026lt;dot_opt3._omp_fn.0+0x93\u0026gt; xor edx,edx 000000000000190a \u0026lt;dot_opt3._omp_fn.0+0x95\u0026gt; jmp 00000000000018d5 \u0026lt;dot_opt3._omp_fn.0+0x60\u0026gt; 000000000000190c \u0026lt;dot_opt3._omp_fn.0+0x97\u0026gt; add rsp,0x10 0000000000001910 \u0026lt;dot_opt3._omp_fn.0+0x9b\u0026gt; pop rbx 0000000000001911 \u0026lt;dot_opt3._omp_fn.0+0x9c\u0026gt; pop rbp 0000000000001912 \u0026lt;dot_opt3._omp_fn.0+0x9d\u0026gt; pop r12 0000000000001914 \u0026lt;dot_opt3._omp_fn.0+0x9f\u0026gt; ret  Anatomy of a parallel region graph LR; A[dot_opt3]--B[GOMP_parallel]; B-- id=0_ --C{dot_opt3._omp_fn.0}; B-- id=1_ --C{dot_opt3._omp_fn.0}; B-- id=2_ --C{dot_opt3._omp_fn.0}; A-. \"Body inside__\" .-C; C--D[omp_get_num_threads]; C--E[omp_get_thread_num]; style A fill:#9b9,stroke:#686,stroke-width:4px; style C fill:#9b9,stroke:#668,stroke-width:8px;  Memory semantics For each variable accessed within the parallel region, we can specify whether it is\n private to the thread, with value undefined inside the region firstprivate, which is like private, but initialized by the value upon entering the parallel region shared, meaning that every thread accesses the same value in memory (but changes are not immediately visible)\nint a=0, b=1, c=2; #pragma omp parallel private(a) firstprivate(b) shared(c) { int id = omp_get_thread_num(); a++; b++; c++; printf(\u0026quot;[%d] %d %d %d\\n\u0026quot;, id, a, b, c); } printf(\u0026quot;END: %d %d %d\\n\u0026quot;, a, b, c);  make CFLAGS='-fopenmp -Wall' -B omp-mem  cc -fopenmp -Wall omp-mem.c -o omp-mem omp-mem.c: In function ‘main._omp_fn.0’: omp-mem.c:8:6: warning: ‘a’ is used uninitialized in this function [-Wuninitialized] 8 | a++; | ~^~ omp-mem.c:5:7: note: ‘a’ was declared here 5 | int a=1, b=2, c=3; | ^   Question: How could the compiler get firstprivate and shared variables into the scope of dot_opt3._omp_fn.0?\nProgramming style I find private semantics unnecessary and error-prone. We can just declare those variables at inner-most scope.\nint b=1, c=2; #pragma omp parallel firstprivate(b) shared(c) { int a = 0; int id = omp_get_thread_num(); a++; b++; c++; printf(\u0026quot;[%d] %d %d %d\\n\u0026quot;, id, a, b, c); } printf(\u0026quot;END: %d %d %d\\n\u0026quot;, a, b, c); // Error: a not in scope here  Updating shared variables We see that the shared variable c has lots of opportunities for conflict.\nsequenceDiagram Thread0--Memory: load c=2 Thread1--Memory: load c=2 Note left of Thread0: c++ (=3) Note right of Thread1: c++ (=3) Note right of Thread1: print c Thread0--Memory: store c=3 Thread1--Memory: store c=3 Note left of Thread0: print c  If we run the above many times, we may sometimes find that multiple processes have the same value of c, each thread observes different increments from others, and the total number of increments may vary.\nWe can define ordering semantics using atomic, critical, and barrier.\nint b=1, c=2; #pragma omp parallel firstprivate(b) shared(c) { int a = 1; int id = omp_get_thread_num(); b++; #pragma omp critical c++; #pragma omp barrier printf(\u0026quot;[%d] %d %d %d\\n\u0026quot;, id, a, b, c); } printf(\u0026quot;END: _ %d %d\\n\u0026quot;, b, c);  A quick demo of perf Linux perf is a kernel interrupt-based profiling tool. It uses performance counters and interrupts to diagnose all sorts of bottlenecks.\n$ perf stat ./dot -n 10000 \u0026gt; /dev/null Performance counter stats for './dot -n 10000': 1.56 msec task-clock:u # 1.201 CPUs utilized 0 context-switches:u # 0.000 K/sec 0 cpu-migrations:u # 0.000 K/sec 124 page-faults:u # 0.079 M/sec 3,041,706 cycles:u # 1.947 GHz 2,272,231 instructions:u # 0.75 insn per cycle 410,889 branches:u # 262.962 M/sec 7,911 branch-misses:u # 1.93% of all branches 0.001301176 seconds time elapsed 0.001970000 seconds user 0.000000000 seconds sys  $ perf record -g ./dot -n 10000 -r 1000 \u0026gt; /dev/null [ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.075 MB perf.data (1098 samples) ]  $ perf report -M intel  Note how GOMP overhead dominates the cost in this experiment. We need more work (longer arrays, etc.) to justify the overhead of distributing and collecting the parallel work.\nWe can drill down into particular functions (especially ours, which we have hopefully compiled with -g to include debugging information).\nFrom this, we see specific instructions, and their corresponding lines of code, that are most frequently being processed when the kernel interrupts to check. In this experiment, we see *sd \u0026ldquo;scalar double\u0026rdquo; instructions, indicating lack of vectorization.\nIn contrast, the following annotation shows use of *pd \u0026ldquo;packed double\u0026rdquo; instructions, indicating that the \u0026ldquo;hot\u0026rdquo; loop has been vectorized.\n(The reason for vectorization can sometimes be determined by -fopt-info -fopt-info-missed, and can be encouraged by techniques like manually splitting accumulators, preventing aliasing by using restrict, directives like #pragma omp simd, and global compiler flags like -ffast-math.)\nFor more on perf, see Brendan Gregg\u0026rsquo;s Linux Performance site.\n","date":1568378965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568408667,"objectID":"93138c81f27ec2d1fa84a97d0e9493af","permalink":"https://cucs-hpsc.github.io/fall2019/openmp-2/","publishdate":"2019-09-13T06:49:25-06:00","relpermalink":"/fall2019/openmp-2/","section":"fall2019","summary":"What does the compiler do when we add #pragma omp parallel?\nstatic double dot_opt3(size_t n, const double *a, const double *b) { double sum = 0; omp_set_num_threads(4); #pragma omp parallel { #pragma omp for reduction(+:sum) for (size_t i=0; i\u0026lt;n; i++) sum += a[i] * b[i]; } return sum; }  gcc -Os -march=native -fopenmp dot.c -o dot objdump -d --prefix-addresses -M intel dot | grep dot_opt3  000000000000129f \u0026lt;main+0x1af\u0026gt; call 0000000000001779 \u0026lt;dot_opt3\u0026gt; 0000000000001779 \u0026lt;dot_opt3\u0026gt; push r12 000000000000177b \u0026lt;dot_opt3+0x2\u0026gt; mov r12,rdx 000000000000177e \u0026lt;dot_opt3+0x5\u0026gt; push rbp 000000000000177f \u0026lt;dot_opt3+0x6\u0026gt; mov rbp,rsi 0000000000001782 \u0026lt;dot_opt3+0x9\u0026gt; push rbx 0000000000001783 \u0026lt;dot_opt3+0xa\u0026gt; mov rbx,rdi 0000000000001786 \u0026lt;dot_opt3+0xd\u0026gt; mov edi,0x4 000000000000178b \u0026lt;dot_opt3+0x12\u0026gt; sub rsp,0x30 000000000000178f \u0026lt;dot_opt3+0x16\u0026gt; mov rax,QWORD PTR fs:0x28 0000000000001798 \u0026lt;dot_opt3+0x1f\u0026gt; mov QWORD PTR [rsp+0x28],rax 000000000000179d \u0026lt;dot_opt3+0x24\u0026gt; xor eax,eax 000000000000179f \u0026lt;dot_opt3+0x26\u0026gt; call 0000000000001070 \u0026lt;omp_set_num_threads@plt\u0026gt; 00000000000017a4 \u0026lt;dot_opt3+0x2b\u0026gt; xor ecx,ecx 00000000000017a6 \u0026lt;dot_opt3+0x2d\u0026gt; xor edx,edx 00000000000017a8 \u0026lt;dot_opt3+0x2f\u0026gt; lea rsi,[rsp+0x8] 00000000000017ad \u0026lt;dot_opt3+0x34\u0026gt; lea rdi,[rip+0xc1] # 0000000000001875 \u0026lt;dot_opt3.","tags":null,"title":"More OpenMP","type":"docs"},{"authors":null,"categories":null,"content":" def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)  What is OpenMP? A community-developed standard Application Programming Interface (with \u0026ldquo;directives\u0026rdquo;) for * multithreaded programming * vectorization * offload to coprocessors (such as GPUs)\nOpenMP is available for C, C++, and Fortran.\nLatest version: OpenMP-5.0, released November 2018. Implementations are still incomplete!\nOpenMP Resources  OpenMP-5.0 Reference Cards (a few pages, printable) OpenMP-5.0 Standard OpenMP-4.5 Examples LLNL Tutorial Mattson: The OpenMP Common Core from ATPESC (video)  #pragma omp parallel The standard is big, but most applications only use a few constructs.\nrender_c('omp-hello.c')  #include \u0026lt;omp.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main() { #pragma omp parallel { int num_threads = omp_get_num_threads(); int my_thread_num = omp_get_thread_num(); printf(\u0026quot;I am %d of %d\\n\u0026quot;, my_thread_num, num_threads); } return 0; }  !make CFLAGS='-fopenmp -Wall' -B omp-hello  cc -fopenmp -Wall omp-hello.c -o omp-hello  !./omp-hello  I am 1 of 4 I am 2 of 4 I am 0 of 4 I am 3 of 4  !OMP_NUM_THREADS=8 ./omp-hello  I am 0 of 8 I am 7 of 8 I am 1 of 8 I am 3 of 8 I am 4 of 8 I am 6 of 8 I am 2 of 8 I am 5 of 8  Parallelizing triad void triad(int N, double *a, const double *b, double scalar, const double *c) { #pragma omp parallel { for (int i=0; i\u0026lt;N; i++) a[i] = b[i] + scalar * c[i]; } }  What does this code do?\nvoid triad(int N, double *a, const double *b, double scalar, const double *c) { #pragma omp parallel { int id = omp_get_thread_num(); int num_threads = omp_get_num_threads(); for (int i=id; i\u0026lt;N; i+=num_threads) a[i] = b[i] + scalar * c[i]; } }  Parallelizing dot static double dot_ref(size_t n, const double *a, const double *b) { double sum = 0; for (size_t i=0; i\u0026lt;n; i++) sum += a[i] * b[i]; return sum; }  !make CFLAGS='-O3 -march=native -fopenmp' -B dot  cc -O3 -march=native -fopenmp dot.c -o dot  !OMP_NUM_THREADS=2 ./dot -r 10 -n 10000   Name flops ticks flops/tick dot_ref 20000 40327 0.50 dot_ref 20000 35717 0.56 dot_ref 20000 36096 0.55 dot_ref 20000 36487 0.55 dot_ref 20000 37157 0.54 dot_ref 20000 36024 0.56 dot_ref 20000 35322 0.57 dot_ref 20000 36601 0.55 dot_ref 20000 72193 0.28 dot_ref 20000 37924 0.53 dot_opt1 20000 51256384 0.00 dot_opt1 20000 23343145 0.00 dot_opt1 20000 4646174 0.00 dot_opt1 20000 16710 1.20 dot_opt1 20000 15512 1.29 dot_opt1 20000 16016 1.25 dot_opt1 20000 16982 1.18 dot_opt1 20000 452064 0.04 dot_opt1 20000 16278 1.23 dot_opt1 20000 16311 1.23 dot_opt2 20000 24616 0.81 dot_opt2 20000 16095 1.24 dot_opt2 20000 17561 1.14 dot_opt2 20000 16270 1.23 dot_opt2 20000 18130 1.10 dot_opt2 20000 16831 1.19 dot_opt2 20000 16968 1.18 dot_opt2 20000 16391 1.22 dot_opt2 20000 17063 1.17 dot_opt2 20000 16315 1.23 dot_opt3 20000 77013 0.26 dot_opt3 20000 12419 1.61 dot_opt3 20000 12124 1.65 dot_opt3 20000 12193 1.64 dot_opt3 20000 12051 1.66 dot_opt3 20000 12009 1.67 dot_opt3 20000 11944 1.67 dot_opt3 20000 12032 1.66 dot_opt3 20000 12687 1.58 dot_opt3 20000 12188 1.64  Vectorization OpenMP-4.0 added the omp simd construct, which is a portable way to request that the compiler vectorize code. An example of a reason why a compiler might fail to vectorize code is aliasing, which we investigate below.\nrender_c('triad.c')  #include \u0026lt;stdlib.h\u0026gt; void triad(size_t N, double *a, const double *b, double scalar, const double *c) { for (size_t i=0; i\u0026lt;N; i++) a[i] = b[i] + scalar * c[i]; }  !gcc -O2 -ftree-vectorize -fopt-info-all -c triad.c  Unit growth for small function inlining: 15-\u0026gt;15 (0%) Inlined 0 calls, eliminated 0 functions triad.c:4:3: optimized: loop vectorized using 16 byte vectors triad.c:4:3: optimized: loop versioned for vectorization because of possible aliasing triad.c:3:6: note: vectorized 1 loops in function. triad.c:4:3: optimized: loop turned into non-loop; it never loops   gcc autovectorization starts at -O3 or if you use -ftree-vectorize options such as -fopt-info give useful diagnostics, but are compiler-dependent and sometimes referring to assembly is useful man gcc with search (/) is your friend  What is aliasing? Is this valid code? What xs x after this call?\ndouble x[5] = {1, 2, 3, 4, 5}; triad(2, x+1, x, 10., x);  C allows memory to overlap arbitrarily. You can inform the compiler of this using the restrict qualifier (C99/C11; __restrict or __restrict__ work with most C++ and CUDA compilers).\nrender_c('triad-restrict.c')  void triad(int N, double *restrict a, const double *restrict b, double scalar, const double *restrict c) { for (int i=0; i\u0026lt;N; i++) a[i] = b[i] + scalar * c[i]; }  !gcc -O2 -march=native -ftree-vectorize -fopt-info-all -c triad-restrict.c  Unit growth for small function inlining: 15-\u0026gt;15 (0%) Inlined 0 calls, eliminated 0 functions triad-restrict.c:2:5: optimized: loop vectorized using 32 byte vectors triad-restrict.c:1:6: note: vectorized 1 loops in function.  Notice how there is no more loop versioned for vectorization because of possible aliasing.\nThe complexity of checking for aliasing can grow combinatorially in the number of arrays being processed, leading to many loop variants and/or preventing vectorization.\nAside: Warnings The -Wrestrict flag (included in -Wall) can catch some programming errors\nvoid foo(double *x) { triad(2, x, x, 10, x); }  !gcc -O2 -Wall -c triad-foo.c  The powers of -Wrestrict are limited, however, and (as of gcc-9) do not even catch\nvoid foo(double *x) { triad(2, x+1, x, 10, x); }  Check the assembly !objdump -d --prefix-addresses -M intel triad-restrict.o  triad-restrict.o: file format elf64-x86-64 Disassembly of section .text: 0000000000000000 \u0026lt;triad\u0026gt; test edi,edi 0000000000000002 \u0026lt;triad+0x2\u0026gt; jle 0000000000000067 \u0026lt;triad+0x67\u0026gt; 0000000000000004 \u0026lt;triad+0x4\u0026gt; lea eax,[rdi-0x1] 0000000000000007 \u0026lt;triad+0x7\u0026gt; cmp eax,0x2 000000000000000a \u0026lt;triad+0xa\u0026gt; jbe 0000000000000074 \u0026lt;triad+0x74\u0026gt; 000000000000000c \u0026lt;triad+0xc\u0026gt; mov r8d,edi 000000000000000f \u0026lt;triad+0xf\u0026gt; shr r8d,0x2 0000000000000013 \u0026lt;triad+0x13\u0026gt; vbroadcastsd ymm2,xmm0 0000000000000018 \u0026lt;triad+0x18\u0026gt; shl r8,0x5 000000000000001c \u0026lt;triad+0x1c\u0026gt; xor eax,eax 000000000000001e \u0026lt;triad+0x1e\u0026gt; xchg ax,ax 0000000000000020 \u0026lt;triad+0x20\u0026gt; vmovupd ymm1,YMMWORD PTR [rcx+rax*1] 0000000000000025 \u0026lt;triad+0x25\u0026gt; vfmadd213pd ymm1,ymm2,YMMWORD PTR [rdx+rax*1] 000000000000002b \u0026lt;triad+0x2b\u0026gt; vmovupd YMMWORD PTR [rsi+rax*1],ymm1 0000000000000030 \u0026lt;triad+0x30\u0026gt; add rax,0x20 0000000000000034 \u0026lt;triad+0x34\u0026gt; cmp rax,r8 0000000000000037 \u0026lt;triad+0x37\u0026gt; jne 0000000000000020 \u0026lt;triad+0x20\u0026gt; 0000000000000039 \u0026lt;triad+0x39\u0026gt; mov eax,edi 000000000000003b \u0026lt;triad+0x3b\u0026gt; and eax,0xfffffffc 000000000000003e \u0026lt;triad+0x3e\u0026gt; test dil,0x3 0000000000000042 \u0026lt;triad+0x42\u0026gt; je 0000000000000070 \u0026lt;triad+0x70\u0026gt; 0000000000000044 \u0026lt;triad+0x44\u0026gt; vzeroupper 0000000000000047 \u0026lt;triad+0x47\u0026gt; cdqe 0000000000000049 \u0026lt;triad+0x49\u0026gt; nop DWORD PTR [rax+0x0] 0000000000000050 \u0026lt;triad+0x50\u0026gt; vmovsd xmm1,QWORD PTR [rcx+rax*8] 0000000000000055 \u0026lt;triad+0x55\u0026gt; vfmadd213sd xmm1,xmm0,QWORD PTR [rdx+rax*8] 000000000000005b \u0026lt;triad+0x5b\u0026gt; vmovsd QWORD PTR [rsi+rax*8],xmm1 0000000000000060 \u0026lt;triad+0x60\u0026gt; inc rax 0000000000000063 \u0026lt;triad+0x63\u0026gt; cmp edi,eax 0000000000000065 \u0026lt;triad+0x65\u0026gt; jg 0000000000000050 \u0026lt;triad+0x50\u0026gt; 0000000000000067 \u0026lt;triad+0x67\u0026gt; ret 0000000000000068 \u0026lt;triad+0x68\u0026gt; nop DWORD PTR [rax+rax*1+0x0] 0000000000000070 \u0026lt;triad+0x70\u0026gt; vzeroupper 0000000000000073 \u0026lt;triad+0x73\u0026gt; ret 0000000000000074 \u0026lt;triad+0x74\u0026gt; xor eax,eax 0000000000000076 \u0026lt;triad+0x76\u0026gt; jmp 0000000000000047 \u0026lt;triad+0x47\u0026gt;   How do the results change if you go up and replace -march=native with -march=skylake-avx512 -mprefer-vector-width=512? Is the assembly qualitatively different without restrict (in which case the compiler \u0026ldquo;versions\u0026rdquo; the loop).  Pragma omp simd An alternative (or supplement) to restrict is #pragma omp simd.\nrender_c('triad-omp-simd.c')  void triad(int N, double *a, const double *b, double scalar, const double *c) { #pragma omp simd for (int i=0; i\u0026lt;N; i++) a[i] = b[i] + scalar * c[i]; }  !gcc -O2 -march=native -ftree-vectorize -fopenmp -fopt-info-all -c triad-omp-simd.c  Unit growth for small function inlining: 15-\u0026gt;15 (0%) Inlined 0 calls, eliminated 0 functions triad-omp-simd.c:4:17: optimized: loop vectorized using 32 byte vectors triad-omp-simd.c:1:6: note: vectorized 1 loops in function.  ","date":1568206165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568230419,"objectID":"0c668380df9385e8019b78a77771b1d4","permalink":"https://cucs-hpsc.github.io/fall2019/openmp/","publishdate":"2019-09-11T06:49:25-06:00","relpermalink":"/fall2019/openmp/","section":"fall2019","summary":"def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)  What is OpenMP? A community-developed standard Application Programming Interface (with \u0026ldquo;directives\u0026rdquo;) for * multithreaded programming * vectorization * offload to coprocessors (such as GPUs)\nOpenMP is available for C, C++, and Fortran.\nLatest version: OpenMP-5.0, released November 2018. Implementations are still incomplete!\nOpenMP Resources  OpenMP-5.0 Reference Cards (a few pages, printable) OpenMP-5.","tags":null,"title":"OpenMP Basics","type":"docs"},{"authors":null,"categories":null,"content":" Programs with more than one part So far, we\u0026rsquo;ve focused on simple programs with only one part, but real programs have several different parts, often with data dependencies. Some parts will be amenable to optimization and/or parallelism and others will not. This principle is called Amdahl\u0026rsquo;s Law.\ndef exec_time(f, p, n=10, latency=1): # Suppose that a fraction f of the total work is amenable to optimization # We run a problem size n with parallelization factor p return latency + (1-f)*n + f*n/p  %matplotlib inline import matplotlib.pyplot as plt import pandas import numpy as np plt.style.use('seaborn') ps = np.geomspace(1, 1000) plt.loglog(ps, exec_time(.99, ps, latency=0)) plt.loglog(ps, exec_time(1, ps, latency=0)) plt.title('Strong scaling') plt.xlabel('p') plt.ylabel('time');  Strong scaling: fixed total problem size Cost = time * p def exec_cost(f, p, **kwargs): return exec_time(f, p, **kwargs) * p plt.loglog(ps, exec_cost(.99, ps)) plt.title('Strong scaling') plt.xlabel('p') plt.ylabel('cost');  Efficiency plt.semilogx(ps, 1/exec_cost(.99, ps, latency=1)) plt.title('Strong scaling') plt.xlabel('p') plt.ylabel('efficiency') plt.ylim(bottom=0);  Speedup $$ S(p) = \\frac{T(1)}{T(p)} $$\nplt.plot(ps, exec_time(.99, 1, latency=1) / exec_time(.99, ps, latency=1)) plt.title('Strong scaling') plt.xlabel('p') plt.ylabel('speedup') plt.ylim(bottom=0);  Stunt 1: Report speedup, not absolute performance! Efficiency-Time spectrum (my preference) People care about two observable properties * Time until job completes * Cost in core-hours or dollars to do job\nMost HPC applications have access to large machines, so don\u0026rsquo;t really care how many processes they use for any given job.\nplt.plot(exec_time(.99, ps), 1/exec_cost(.99, ps), 'o-') plt.title('Strong scaling') plt.xlabel('time') plt.ylabel('efficiency') plt.ylim(bottom=0); plt.xlim(left=0);  Principles  No \u0026ldquo;soft\u0026rdquo; log scale Both axes have tangible units Bigger is better on the $y$ axis  Weak Scaling: Fixed work per processor We\u0026rsquo;ve kept the problem size $n$ fixed thus far, but parallel computers are also used to solve large problems. If we keep the amount of work per processor fixed, we are weak/Gustafson scaling.\nns = 10*ps plt.semilogx(ps, ns/exec_cost(.99, ps, n=ns, latency=1), 'o-') ns = 100*ps plt.semilogx(ps, ns/exec_cost(.99, ps, n=ns, latency=1), 's-') plt.title('Weak scaling') plt.xlabel('procs') plt.ylabel('efficiency') plt.ylim(bottom=0);  for w in np.geomspace(0.1, 1e3, 20): ns = w*ps plt.semilogx(exec_time(.99, ps, n=ns, latency=1), ns/exec_cost(.99, ps, n=ns, latency=1), 'o-') plt.title('Weak scaling') plt.xlabel('time') plt.ylabel('efficiency') plt.ylim(bottom=0);  Fuhrer et al (2018): Near-global climate simulation at 1 km resolution I replotted these data for my talk at the Latsis Symposium last month.\nFurther resources  Hager: Fooling the masses  Learn by counter-examples  Hoefler and Belli: Scientific Benchmarking of Parallel Computing Systems  Recommended best practices, especially for dealing with performance variability   Please read/watch something from this list and be prepared to share on Monday.\n","date":1567778851,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567795563,"objectID":"2926b880e944c22aa6e3f38e1d6b50ee","permalink":"https://cucs-hpsc.github.io/fall2019/intro-parallel-scaling/","publishdate":"2019-09-06T08:07:31-06:00","relpermalink":"/fall2019/intro-parallel-scaling/","section":"fall2019","summary":"Programs with more than one part So far, we\u0026rsquo;ve focused on simple programs with only one part, but real programs have several different parts, often with data dependencies. Some parts will be amenable to optimization and/or parallelism and others will not. This principle is called Amdahl\u0026rsquo;s Law.\ndef exec_time(f, p, n=10, latency=1): # Suppose that a fraction f of the total work is amenable to optimization # We run a problem size n with parallelization factor p return latency + (1-f)*n + f*n/p  %matplotlib inline import matplotlib.","tags":null,"title":"Intro to Parallel Scaling","type":"docs"},{"authors":null,"categories":null,"content":" Why model performance? Models give is a conceptual and roughly quantitative framework by which to answer the following types of questions.\n Why is an implementation exhibiting its observed performance? How will performance change if we:  optimize this component? buy new hardware? (Which new hardware?) run a different configuration?  While conceptualizing a new algorithm, what performance can we expect and what will be bottlenecks?  Models are a guide for performance, but not an absolute.\nTerms    Symbol Meaning     $n$ Input parameter related to problem size   $W$ Amount of work to solve problem $n$   $T$ Execution time   $R$ Rate at which work is done    STREAM Triad for (i=0; i\u0026lt;n; i++) a[i] = b[i] + scalar*c[i];  $n$ is the array size and $$W = 3 \\cdot \\texttt{sizeof(double)} \\cdot n$$ is the number of bytes transferred. The rate $R = W/T$ is measured in bytes per second (or MB/s, etc.).\nDense matrix multiplication To perform the operation $C \\gets C + A B$ where $A,B,C$ are $n\\times n$ matrices.\nfor (i=0; i\u0026lt;n; i++) for (j=0; j\u0026lt;n; j++) for (k=0; k\u0026lt;n; k++) c[i*n+j] += a[i*n+k] * b[k*n+j];   Can you identify two expressions for the total amount of work $W(n)$ and the associated units? Can you think of a context in which one is better than the other and vice-versa?  Estimating time To estimate time, we need to know how fast hardware executes flops and moves bytes.\n%matplotlib inline import matplotlib.pyplot as plt import pandas import numpy as np plt.style.use('seaborn') hardware = pandas.read_csv('data-intel.csv', index_col=\u0026quot;Name\u0026quot;) hardware   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Year GFLOPs-SP GFLOPs-DP Cores Mem-GBps TDP Freq(MHz)   Name            Xeon X5482 2007 102 51 4 26 150 3200   Xeon X5492 2008 108 54 4 26 150 3400   Xeon W5590 2009 106 53 4 32 130 3300   Xeon X5680 2010 160 80 6 32 130 3300   Xeon X5690 2011 166 83 6 32 130 3470   Xeon E5-2690 2012 372 186 8 51 135 2900   Xeon E5-2697 v2 2013 518 259 12 60 130 2700   Xeon E5-2699 v3 2014 1324 662 18 68 145 2300   Xeon E5-2699 v3 2015 1324 662 18 68 145 2300   Xeon E5-2699 v4 2016 1548 774 22 77 145 2200   Xeon Platinum 8180 2017 4480 2240 28 120 205 2500   Xeon Platinum 9282 2018 9320 4660 56 175 400 2600     fig = hardware.plot(x='GFLOPs-DP', y='Mem-GBps', marker='o') fig.set_xlim(left=0) fig.set_ylim(bottom=0);  So we have rates $R_f = 4660 \\cdot 10^9$ flops/second and $R_m = 175 \\cdot 10^9$ bytes/second. Now we need to characterize some algorithms.\nalgs = pandas.read_csv('algs.csv', index_col='Name') algs['intensity'] = algs['flops'] / algs['bytes'] algs = algs.sort_values('intensity') algs   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    bytes flops intensity   Name        Triad 24 2 0.083333   SpMV 12 2 0.166667   Stencil27-cache 24 54 2.250000   MatFree-FEM 2376 15228 6.409091   Stencil27-ideal 8 54 6.750000     def exec_time(machine, alg, n): bytes = n * alg.bytes flops = n * alg.flops T_mem = bytes / (machine['Mem-GBps'] * 1e9) T_flops = flops / (machine['GFLOPs-DP'] * 1e9) return max(T_mem, T_flops) exec_time(hardware.loc['Xeon Platinum 9282'], algs.loc['SpMV'], 1e8)  0.006857142857142857  for _, machine in hardware.iterrows(): for _, alg in algs.iterrows(): ns = np.geomspace(1e4, 1e9, 10) times = np.array([exec_time(machine, alg, n) for n in ns]) flops = np.array([alg.flops * n for n in ns]) rates = flops/times plt.loglog(ns, rates, 'o-') plt.xlabel('n') plt.ylabel('rate');  It looks like performance does not depend on problem size.\nWell, yeah, we chose a model in which flops and bytes were both proportional to $n$, and our machine model has no sense of cache hierarchy or latency, so time is also proportional to $n$. We can divide through by $n$ and yield a more illuminating plot.\nfor _, machine in hardware.iterrows(): times = np.array([exec_time(machine, alg, 1) for _, alg in algs.iterrows()]) rates = algs.flops/times intensities = algs.intensity plt.loglog(intensities, rates, 'o-', label=machine.name) plt.xlabel('intensity') plt.ylabel('rate') plt.legend();  We\u0026rsquo;re seeing the \u0026ldquo;roofline\u0026rdquo; for the older processors while the newer models are memory bandwidth limited for all of these algorithms.\nRecommended reading on single-node performance modeling  Williams, Waterman, Patterson (2009): Roofline: An insightful visual performance model for multicore architectures  ","date":1567610248,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567794459,"objectID":"80593eab3fbd65c9fcac19929daefb89","permalink":"https://cucs-hpsc.github.io/fall2019/intro-modeling/","publishdate":"2019-09-04T09:17:28-06:00","relpermalink":"/fall2019/intro-modeling/","section":"fall2019","summary":"Why model performance? Models give is a conceptual and roughly quantitative framework by which to answer the following types of questions.\n Why is an implementation exhibiting its observed performance? How will performance change if we:  optimize this component? buy new hardware? (Which new hardware?) run a different configuration?  While conceptualizing a new algorithm, what performance can we expect and what will be bottlenecks?  Models are a guide for performance, but not an absolute.","tags":null,"title":"Introduction to Performance Modeling","type":"docs"},{"authors":null,"categories":null,"content":" Remember how single-thread performance has increased significantly since ~2004 when clock frequency stagnated?\nThis is a result of doing more per clock cycle.\nLet\u0026rsquo;s visit some slides:\n Georg Hager (2019): Modern Computer Architucture  Further resources  Intel Intrinsics Guide Wikichip  Intel Xeon: Cascade Lake AMD EPYC gen2: Rome IBM POWER9  Agner Fog\u0026rsquo;s website  ","date":1567184400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567206505,"objectID":"28d2cfe1e2d75d1102ca03a990db79bf","permalink":"https://cucs-hpsc.github.io/fall2019/intro-vectorization/","publishdate":"2019-08-30T11:00:00-06:00","relpermalink":"/fall2019/intro-vectorization/","section":"fall2019","summary":" Remember how single-thread performance has increased significantly since ~2004 when clock frequency stagnated?\nThis is a result of doing more per clock cycle.\nLet\u0026rsquo;s visit some slides:\n Georg Hager (2019): Modern Computer Architucture  Further resources  Intel Intrinsics Guide Wikichip  Intel Xeon: Cascade Lake AMD EPYC gen2: Rome IBM POWER9  Agner Fog\u0026rsquo;s website  ","tags":null,"title":"Vectorization and Instruction-Level Parallelism","type":"docs"},{"authors":null,"categories":null,"content":" Cores, caches, and memory A von Neumann Architecture A contemporary architecture My laptop We can get this kind of information for our machine using hwloc, which provides a library as well as the command-line tool lstopo.\n!lstopo --output-format svg \u0026gt; lstopo-local.svg  A double-socket compute node with two GPUs 2x Xeon Ivy-Bridge-EP E5-2680v2 + 2x NVIDIA GPUs (from 2013, with hwloc v1.11). GPUs are reported as CUDA devices and X11 display :1.0: (from the hwloc gallery) Block diagrams A block diagram from a vendor can include additional information about how cores are physically connected.\nRing bus (Xeon E5-2600 family) Mesh bus (Xeon Scalable family) Multi-socket configurations https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview\nMultiple nodes go into racks or cabinets Terminology  Core (virtual and physical): has a single program counter (logically sequential processing of instructions) Memory channel: e.g., DDR4-2400: transfers 64 bits (8 bytes) at a rate of 2400 MHz = 15.36 GB/s Socket or CPU: contains multiple cores in a single piece* of silicon Non-Uniform Memory Access (NUMA): different channels may be different distances from a core Compute node: one or more sockets, plus memory, network card, etc.  How expensive is it to access memory? What does that mean? How would we measure?\nMcKenney (2013): Laws of Physics\nInteractive\nVariation by vendor\nHow your program accesses memory double a[1000]; void foo() { for (int i=0; i\u0026lt;1000; i++) a[i] = 1.234 * i; }  The compiler turns the loop body into instructions, which we can examine using Godbolt.\npxor xmm0, xmm0 ; zero the xmm0 register cvtsi2sd xmm0, eax ; convert the integer i to double mulsd xmm0, xmm1 ; multiply by 1.234 (held in xmm1) movsd QWORD PTR a[0+rax*8], xmm0 ; store to memory address a[i]  Only one instruction here accesses memory, and the performance will be affected greatly by where that memory resides (which level of cache, where in DRAM).\nMost architectures today have 64-byte cache lines: all transfers from main memory (DRAM) to and from cache operate in units of 64 bytes.\nLet\u0026rsquo;s compare three code samples for (int i=0; i\u0026lt;N; i++) a[i] = b[i];  for (int i=0; i\u0026lt;N; i++) a[i] = b[(i*8) % N];  for (int i=0; i\u0026lt;N; i++) a[i] = b[random() % N];  What happens when you request a cache line? Operating system effects Most systems today use virtual addressing, so every address in your program needs to be translated to a physical address before looking for it (in cache or memory). Fortunately, there is hardware to assist with this: the Translation Lookaside Buffer (TLB).\nFurther resources  Julia Evans (2016): How much memory is my process using? Gustavo Duarte (2009): Cache: a place for concealment and safekeeping Gustavo Duarte (2009): Getting Physical With Memory Ulrich Drepper (2007): What Every Programmer Should Know About Memory  ","date":1567001418,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567206505,"objectID":"78f1d8586b6b21bb4f04a177e8e4340f","permalink":"https://cucs-hpsc.github.io/fall2019/intro-architecture/","publishdate":"2019-08-28T08:10:18-06:00","relpermalink":"/fall2019/intro-architecture/","section":"fall2019","summary":"Cores, caches, and memory A von Neumann Architecture A contemporary architecture My laptop We can get this kind of information for our machine using hwloc, which provides a library as well as the command-line tool lstopo.\n!lstopo --output-format svg \u0026gt; lstopo-local.svg  A double-socket compute node with two GPUs 2x Xeon Ivy-Bridge-EP E5-2680v2 + 2x NVIDIA GPUs (from 2013, with hwloc v1.11). GPUs are reported as CUDA devices and X11 display :1.","tags":null,"title":"Intro to Architecture","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://cucs-hpsc.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":"https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/\nhttps://www.karlrupp.net/2013/06/cpu-gpu-and-mic-hardware-characteristics-over-time/\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"caba13451aa50e0e0e40912f3d66e772","permalink":"https://cucs-hpsc.github.io/fall2019/trends/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/fall2019/trends/","section":"fall2019","summary":"https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/\nhttps://www.karlrupp.net/2013/06/cpu-gpu-and-mic-hardware-characteristics-over-time/","tags":null,"title":"Trends","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://cucs-hpsc.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://cucs-hpsc.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://cucs-hpsc.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://cucs-hpsc.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]