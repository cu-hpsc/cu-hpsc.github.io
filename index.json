[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1566849863,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://cucs-hpsc.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"","tags":null,"title":"Jed Brown","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1566796546,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://cucs-hpsc.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" Where and When  CSCI 5576 / 4576: High Performance Scientific Computing Lectures: Mon/Wed/Fri 11-11:50 in ECCR 150 Labs: Fri 9-10:40 in KOBL 300 Website: https://cucs-hpsc.github.io/fall2019 Issues/Questions/etc.: Browse or Create  Zoom  Meeting ID: 214-104-523 Join via web browser: https://cuboulder.zoom.us/j/214104523 Join via Zoom app (using meeting ID) Join via One tap mobile: +16699006833,,214104523# or +16465588656,,214104523# Join via telephone: 1-669-900-6833 or 1-646-558-8656  Calendar  Instructor: Jed Brown  GitHub: @jedbrown Office hours: See calendar in ECOT 824 (usually Tue 14:30-15:30 and Thu 9:00-10:00)  Teaching Assistant: Camden Elliott-Williams  GitHub: @CamdenCU Office hours: Wed 9:30-10:30 and 13:30-14:30 or by appointment in ECCR 1B \u0026ldquo;Systems Lab\u0026rdquo; (see map)  Homework For each assignment, click the link below to accept via GitHub Classroom. This creates a private repository for you to work in. Then git clone the repository to whatever machine you\u0026rsquo;ll work on and follow instructions in the README. Usually you will be asked to read and edit code, run a range of experiments, and interpret/plot data in a Report.ipynb.\n   Assigned Due Description     2019-09-06 2019-09-16 (part by 2019-09-13) Experiments in vectorization    Videos Videos appear automatically on Canvas and linked below.\n   Date Topic     Aug 26 Course introduction and preview of architectural trends   Aug 28 Intro to architecture   Aug 30 Intro to vectorization and ILP   Sep 4 Intro to performance modeling (roofline)   Sep 6 Intro to parallel scaling   Sep 9 Joel Frahm on CU Research Computing (slides)   Sep 11 OpenMP Basics   Sep 13 OpenMP memory semantics, synchronization, and perf demo    ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1568408667,"objectID":"51928ad4c101e2a8b04b4a7a8650b816","permalink":"https://cucs-hpsc.github.io/fall2019/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/fall2019/","section":"fall2019","summary":"CSCI 5576/4576: Fall 2019: High Performance Scientific Computing","tags":null,"title":"Logistics","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://cucs-hpsc.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Overview This course will develop the skills necessary to reason about performance of applications and modern architectures, to identify opportunities and side-effects of changes, to develop high-performance software, to transfer algorithmic patterns and lessons learned from different domains, and to communicate such analyses with diverse stakeholders. These skills are important for research and development of numerical methods and performance-sensitive science and engineering applications, obtaining allocations via NSF\u0026rsquo;s XSEDE and DOE ASCR facilities, as well as in jobs affiliated with computing facilities at national labs, industry, and academia.\nWe will introduce widely-used parallel programming models such as OpenMP, MPI, and CUDA, as well as ubiquitous parallel libraries, but the purpose of the course is not to teach interfaces, but to develop skills that will be durable and transferrable.\nPreparation This course does not assume prior experience with parallel programming. It will use Linux command-line tools, and some activities will involve batch computing environments (SLURM). Most exercises will use the C programming language, though you can use any appropriate language for projects. Some of the exercises will involve techniques from numerical computing (e.g., CSCI-3656). I will do my best to avoid assuming prior knowledge of these topics, and to provide resources for you to learn or refresh your memory as we use them.\nEveryone here is capable of succeeding in the course, but the effort level will be higher if most of the topics above are new to you. Regardless of your preparation, it is normal to feel lost sometimes. A big part of pragmatic HPC is learning to efficiently answer your questions through documentation, online resources, and even consulting the code or running experiments. (Most of our software stack is open source.) That said, it\u0026rsquo;s easy to lose lots of time in a rabbit hole. My hope is that you will have the courage to dive into that rabbit hole occasionally, but also to ask questions when stuck and to budget your time for such excursions so that you can complete assignments on-time without compromising your work/life balance.\nApproximate timeline    Week Topics     Aug 26 Introduction and modern computer architecture (vectorization and memory hierarchy)   Sep 2 Performance modeling, analysis, and scaling; profiling   Sep 9 Intro to OpenMP and non-numerical algorithms (sorting and searching)   Sep 16 Parallel algorithmic patterns   Sep 23 Dense linear algebra   Sep 30 Intro to MPI and distributed memory parallelism   Oct 7 Sparse and iterative linear algebra   Oct 14 Domain decomposition   Oct 21 Graph algorithms   Oct 28 GPU programming via OpenMP-5 and CUDA   Nov 4 Parallel file systems and IO   Nov 11 Data analysis/machine learning algorithms and dynamic cloud environments   Nov 18 Particles and N-body systems   Nov 25 Fall Break   Dec 2 Multigrid, FFT, and FMM   Dec 9 Special topics    Evaluation    Activity Percentage     Participation 10%   Labs and homework assignments 40%   Community contribution 15%   Community analysis 15%   Final project (written + presentation) 20%    Git and GitHub Homework assignments and in-class activities will be submitted via Git. This class will use GitHub classroom. Homeworks will be completed by cloning GitHub repositories, completing coding and analysis activities, and pushing completed assignments back to GitHub.\nAssignments may be completed using Coding CSEL Hub and/or RMACC Summit (request an account). Assignments will typically have written analysis, for which I recommend Jupyter.\nIt is notoriously difficult to predict the time required to develop quality code and understand its performance, so please start early to give yourself plenty of time. You are welcome to work together on all assignments, but must acknowledge collaborators. You should ensure that your written work is entirely your own.\nCommunity contributions and analysis Over the course of the semester, you will follow the development activities of an active open source project of your choosing. This should be a project with an active developer community from multiple institutions that discuss their rationale in public, such as a mailing list and/or GitHub/GitLab issues and pull requests. You will write and present about the performance and capability needs of key stakeholders, the way project resources are allocated, their metrics for success, and any notable achievements made over the course of the semester.\nYou will also make a contribution to be merged by the project. Adding new examples and/or improving documentation are extremely valuable contributions, but you may also add features or improve implementations. Please respect the time of project maintainers and reviewers by learning about the project and its expectations and process, communicating in advance if appropriate, and leaving plenty of time for multiple rounds of review and revision.\nDistance sections and labs The lectures for this class can be joined synchronously via Zoom (see instructions); they are also recorded and will be posted here (and automatically on Canvas). Some labs will be activities that can be completed within the time period (with group discussion and compare/contrast) while others will be a jump start for homeworks. I envision that distance students will form groups and set a time for virtual discussion in lieu of synchronous discussion during the lab period. In both settings, there will be a peer evaluation component during which each participant credits one or more peers with some specific contributions to the conversation.\nMoodle Moodle will be used to maintain grades. Please enroll yourself at https://moodle.cs.colorado.edu.\nResources This course will use a variety of online resources and papers. There is no required textbook, but the following resources may be helpful.\n Hager and Wellein (2010), Introduction to High Performance Computing for Scientists and Engineers van de Geijn, Myers, Parikh (2019): LAFF on Programming for High Performance (free online) Eijkhout, Chow, van de Geijn (2017), Introduction to High-Performance Scientific Computing (free PDF) Grama, Gupta, Karypis, Kumar (2003), Introduction to Parallel Computing  Additional resources  Greenbaum and Chartier (2012), Numerical Methods Design, Analysis, and Computer Implementation of Algorithms \u0026ndash; an excellent, comprehensive book. Boyd and Vandenberghe (2018), Introduction to Applied Linear Algebra \u0026ndash; practical introduction to linear algebra for computer scientists; free PDF Trefethen and Bau (1997), Numerical Linear Algebra \u0026ndash; fantastic, but limited to numerical linear algebra and covers more advanced topics. Scopatz and Huff (2015), Effective Computation in Physics \u0026ndash; Python language, data science workflow, and computation.  A SIAM Membership is free for CU students and provides a 30% discount on SIAM books.\nDisability Accommodations If you qualify for accommodations because of a disability, please submit to your professor a letter from Disability Services in a timely manner (for exam accommodations provide your letter at least one week prior to the exam) so that your needs can be addressed. Disability Services determines accommodations based on documented disabilities. Contact Disability Services at 303-492-8671 or by e-mail at dsinfo@colorado.edu. If you have a temporary medical condition or injury, see the Temporary Injuries guidelines under the Quick Links at the Disability Services website and discuss your needs with your professor.\nReligious Observances Campus policy regarding religious observances requires that faculty make every effort to deal reasonably and fairly with all students who, because of religious obligations, have conflicts with scheduled exams, assignments or required assignments/attendance. If this applies to you, please speak with me directly as soon as possible at the beginning of the term. See the campus policy regarding religious observances for full details.\nClassroom Behavior Students and faculty each have responsibility for maintaining an appropriate learning environment. Those who fail to adhere to such behavioral standards may be subject to discipline. Professional courtesy and sensitivity are especially important with respect to individuals and topics dealing with differences of race, color, culture, religion, creed, politics, veteran\u0026rsquo;s status, sexual orientation, gender, gender identity and gender expression, age, disability,and nationalities. Class rosters are provided to the instructor with the student\u0026rsquo;s legal name. I will gladly honor your request to address you by an alternate name or gender pronoun. Please advise me of this preference early in the semester so that I may make appropriate changes to my records. For more information, see the policies on classroom behavior and the student code.\nDiscrimination and Harassment The University of Colorado Boulder (CU Boulder) is committed to maintaining a positive learning, working, and living environment. CU Boulder will not tolerate acts of sexual misconduct, discrimination, harassment or related retaliation against or by any employee or student. CU\u0026rsquo;s Sexual Misconduct Policy prohibits sexual assault, sexual exploitation, sexual harassment,intimate partner abuse (dating or domestic violence), stalking or related retaliation. CU Boulder\u0026rsquo;s Discrimination and Harassment Policy prohibits discrimination, harassment or related retaliation based on race, color,national origin, sex, pregnancy, age, disability, creed, religion, sexual orientation, gender identity, gender expression, veteran status, political affiliation or political philosophy. Individuals who believe they have been subject to misconduct under either policy should contact the Office of Institutional Equity and Compliance (OIEC) at 303-492-2127. Information about the OIEC, the above referenced policies, and the campus resources available to assist individuals regarding sexual misconduct, discrimination, harassment or related retaliation can be found at the OIEC website.\nHonor Code All students enrolled in a University of Colorado Boulder course are responsible for knowing and adhering to the academic integrity policy of the institution. Violations of the policy may include: plagiarism, cheating,fabrication, lying, bribery, threat, unauthorized access, clicker fraud,resubmission, and aiding academic dishonesty. All incidents of academic misconduct will be reported to the Honor Code Council (honor@colorado.edu; 303-735-2273). Students who are found responsible for violating the academic integrity policy will be subject to nonacademic sanctions from the Honor Code Council as well as academic sanctions from the faculty member. Additional information regarding the academic integrity policy can be found at http://honorcode.colorado.edu.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567206505,"objectID":"e9c15258b33f23fe5212de16261d2bc2","permalink":"https://cucs-hpsc.github.io/fall2019/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/fall2019/syllabus/","section":"fall2019","summary":"Overview This course will develop the skills necessary to reason about performance of applications and modern architectures, to identify opportunities and side-effects of changes, to develop high-performance software, to transfer algorithmic patterns and lessons learned from different domains, and to communicate such analyses with diverse stakeholders. These skills are important for research and development of numerical methods and performance-sensitive science and engineering applications, obtaining allocations via NSF\u0026rsquo;s XSEDE and DOE ASCR facilities, as well as in jobs affiliated with computing facilities at national labs, industry, and academia.","tags":null,"title":"Syllabus","type":"docs"},{"authors":null,"categories":null,"content":" def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)  Using #pragma omp task Up to now, we\u0026rsquo;ve been expressing parallelism for iterating over an array.\nrender_c('task_dep.4.c')  #include \u0026lt;stdio.h\u0026gt; int main() { int x = 1; #pragma omp parallel #pragma omp single { #pragma omp task shared(x) depend(out: x) x = 2; #pragma omp task shared(x) depend(in: x) printf(\u0026quot;x + 1 = %d. \u0026quot;, x+1); #pragma omp task shared(x) depend(in: x) printf(\u0026quot;x + 2 = %d. \u0026quot;, x+2); } puts(\u0026quot;\u0026quot;); return 0; }  !make CFLAGS=-fopenmp -B task_dep.4  cc -fopenmp task_dep.4.c -o task_dep.4  !for i in {1..10}; do ./task_dep.4; done  x + 2 = 4. x + 1 = 3. x + 1 = 3. x + 2 = 4. x + 2 = 4. x + 1 = 3. x + 1 = 3. x + 2 = 4. x + 2 = 4. x + 1 = 3. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3.  render_c('task_dep.4inout.c')  #include \u0026lt;stdio.h\u0026gt; int main() { int x = 1; #pragma omp parallel #pragma omp single { #pragma omp task shared(x) depend(out: x) x = 2; #pragma omp task shared(x) depend(inout: x) printf(\u0026quot;x + 1 = %d. \u0026quot;, x+1); #pragma omp task shared(x) depend(in: x) printf(\u0026quot;x + 2 = %d. \u0026quot;, x+2); } puts(\u0026quot;\u0026quot;); return 0; }  !make CFLAGS=-fopenmp -B task_dep.4inout  cc -fopenmp task_dep.4inout.c -o task_dep.4inout  !for i in {1..10}; do ./task_dep.4inout; done  x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4. x + 1 = 3. x + 2 = 4.  Computing the Fibonacci numbers with OpenMP Fibonacci numbers are defined by the recurrence \\begin{align} F_0 \u0026amp;= 0 F_1 \u0026amp;= 1 Fn \u0026amp;= F{n-1} + F_{n-2} \\end{align}\nrender_c('fib.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; long fib(long n) { if (n \u0026lt; 2) return n; return fib(n - 1) + fib(n - 2); } int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; #pragma omp parallel for for (long i=0; i\u0026lt;N; i++) fibs[i] = fib(i+1); for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' -B fib  cc -O2 -march=native -fopenmp -Wall fib.c -o fib  !OMP_NUM_THREADS=4 time ./fib 40   1: 1 2: 1 3: 2 4: 3 5: 5 6: 8 7: 13 8: 21 9: 34 10: 55 11: 89 12: 144 13: 233 14: 377 15: 610 16: 987 17: 1597 18: 2584 19: 4181 20: 6765 21: 10946 22: 17711 23: 28657 24: 46368 25: 75025 26: 121393 27: 196418 28: 317811 29: 514229 30: 832040 31: 1346269 32: 2178309 33: 3524578 34: 5702887 35: 9227465 36: 14930352 37: 24157817 38: 39088169 39: 63245986 40: 102334155 0.85user 0.00system 0:00.78elapsed 109%CPU (0avgtext+0avgdata 2044maxresident)k 0inputs+0outputs (0major+99minor)pagefaults 0swaps  Use tasks render_c('fib2.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; long fib(long n) { if (n \u0026lt; 2) return n; long n1, n2; #pragma omp task shared(n1) n1 = fib(n - 1); #pragma omp task shared(n2) n2 = fib(n - 2); #pragma omp taskwait return n1 + n2; } int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; #pragma omp parallel #pragma omp single nowait { for (long i=0; i\u0026lt;N; i++) fibs[i] = fib(i+1); } for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' fib2  make: 'fib2' is up to date.  !OMP_NUM_THREADS=2 time ./fib2 30   1: 1 2: 1 3: 2 4: 3 5: 5 6: 8 7: 13 8: 21 9: 34 10: 55 11: 89 12: 144 13: 233 14: 377 15: 610 16: 987 17: 1597 18: 2584 19: 4181 20: 6765 21: 10946 22: 17711 23: 28657 24: 46368 25: 75025 26: 121393 27: 196418 28: 317811 29: 514229 30: 832040 2.42user 0.81system 0:02.54elapsed 127%CPU (0avgtext+0avgdata 2028maxresident)k 0inputs+0outputs (0major+100minor)pagefaults 0swaps   It\u0026rsquo;s expensive to create tasks when n is small, even with only one thread. How can we cut down on that overhead?\nrender_c('fib3.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; long fib(long n) { if (n \u0026lt; 2) return n; if (n \u0026lt; 30) return fib(n - 1) + fib(n - 2); long n1, n2; #pragma omp task shared(n1) n1 = fib(n - 1); #pragma omp task shared(n2) n2 = fib(n - 2); #pragma omp taskwait return n1 + n2; } int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; #pragma omp parallel #pragma omp single nowait { for (long i=0; i\u0026lt;N; i++) fibs[i] = fib(i+1); } for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' fib3  cc -O2 -march=native -fopenmp -Wall fib3.c -o fib3\n!OMP_NUM_THREADS=3 time ./fib3 40  1: 1 2: 1 3: 2 4: 3 5: 5 6: 8 7: 13 8: 21 9: 34 10: 55 11: 89 12: 144 13: 233 14: 377 15: 610 16: 987 17: 1597 18: 2584 19: 4181 20: 6765 21: 10946 22: 17711 23: 28657 24: 46368 25: 75025 26: 121393 27: 196418 28: 317811 29: 514229 30: 832040 31: 1346269 32: 2178309 33: 3524578 34: 5702887 35: 9227465 36: 14930352 37: 24157817 38: 39088169 39: 63245986 40: 102334155 3.56user 0.00system 0:01.27elapsed 280%CPU (0avgtext+0avgdata 1920maxresident)k 0inputs+0outputs (0major+103minor)pagefaults 0swaps\n This is just slower, even with one thread. Why might that be?\nrender_c('fib4.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; long fib_seq(long n) { if (n \u0026lt; 2) return n; return fib_seq(n - 1) + fib_seq(n - 2); } long fib(long n) { if (n \u0026lt; 30) return fib_seq(n); long n1, n2; #pragma omp task shared(n1) n1 = fib(n - 1); #pragma omp task shared(n2) n2 = fib(n - 2); #pragma omp taskwait return n1 + n2; } int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; #pragma omp parallel #pragma omp single nowait { for (long i=0; i\u0026lt;N; i++) fibs[i] = fib(i+1); } for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' fib4  make: \u0026lsquo;fib4\u0026rsquo; is up to date.\n!OMP_NUM_THREADS=2 time ./fib4 40  1: 1 2: 1 3: 2 4: 3 5: 5 6: 8 7: 13 8: 21 9: 34 10: 55 11: 89 12: 144 13: 233 14: 377 15: 610 16: 987 17: 1597 18: 2584 19: 4181 20: 6765 21: 10946 22: 17711 23: 28657 24: 46368 25: 75025 26: 121393 27: 196418 28: 317811 29: 514229 30: 832040 31: 1346269 32: 2178309 33: 3524578 34: 5702887 35: 9227465 36: 14930352 37: 24157817 38: 39088169 39: 63245986 40: 102334155 0.94user 0.00system 0:00.53elapsed 177%CPU (0avgtext+0avgdata 2040maxresident)k 8inputs+0outputs (0major+97minor)pagefaults 0swaps\n  Alt: schedule(static,1) render_c('fib5.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; long fib(long n) { if (n \u0026lt; 2) return n; return fib(n - 1) + fib(n - 2); } int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; #pragma omp parallel for schedule(static,1) for (long i=0; i\u0026lt;N; i++) fibs[i] = fib(i+1); for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' fib5  make: 'fib5' is up to date.  !OMP_NUM_THREADS=2 time ./fib5 40   1: 1 2: 1 3: 2 4: 3 5: 5 6: 8 7: 13 8: 21 9: 34 10: 55 11: 89 12: 144 13: 233 14: 377 15: 610 16: 987 17: 1597 18: 2584 19: 4181 20: 6765 21: 10946 22: 17711 23: 28657 24: 46368 25: 75025 26: 121393 27: 196418 28: 317811 29: 514229 30: 832040 31: 1346269 32: 2178309 33: 3524578 34: 5702887 35: 9227465 36: 14930352 37: 24157817 38: 39088169 39: 63245986 40: 102334155 0.88user 0.00system 0:00.54elapsed 161%CPU (0avgtext+0avgdata 1908maxresident)k 8inputs+0outputs (0major+93minor)pagefaults 0swaps  Better math render_c('fib6.c')  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main(int argc, char **argv) { if (argc != 2) { fprintf(stderr, \u0026quot;Usage: %s N\\n\u0026quot;, argv[0]); return 1; } long N = atol(argv[1]); long fibs[N]; fibs[0] = 1; fibs[1] = 2; for (long i=2; i\u0026lt;N; i++) fibs[i] = fibs[i-1] + fibs[i-2]; for (long i=0; i\u0026lt;N; i++) printf(\u0026quot;%2ld: %5ld\\n\u0026quot;, i+1, fibs[i]); return 0; }  !make CFLAGS='-O2 -march=native -fopenmp -Wall' fib6  cc -O2 -march=native -fopenmp -Wall fib6.c -o fib6  !time ./fib6 100   1: 1 2: 2 3: 3 4: 5 5: 8 6: 13 7: 21 8: 34 9: 55 10: 89 11: 144 12: 233 13: 377 14: 610 15: 987 16: 1597 17: 2584 18: 4181 19: 6765 20: 10946 21: 17711 22: 28657 23: 46368 24: 75025 25: 121393 26: 196418 27: 317811 28: 514229 29: 832040 30: 1346269 31: 2178309 32: 3524578 33: 5702887 34: 9227465 35: 14930352 36: 24157817 37: 39088169 38: 63245986 39: 102334155 40: 165580141 41: 267914296 42: 433494437 43: 701408733 44: 1134903170 45: 1836311903 46: 2971215073 47: 4807526976 48: 7778742049 49: 12586269025 50: 20365011074 51: 32951280099 52: 53316291173 53: 86267571272 54: 139583862445 55: 225851433717 56: 365435296162 57: 591286729879 58: 956722026041 59: 1548008755920 60: 2504730781961 61: 4052739537881 62: 6557470319842 63: 10610209857723 64: 17167680177565 65: 27777890035288 66: 44945570212853 67: 72723460248141 68: 117669030460994 69: 190392490709135 70: 308061521170129 71: 498454011879264 72: 806515533049393 73: 1304969544928657 74: 2111485077978050 75: 3416454622906707 76: 5527939700884757 77: 8944394323791464 78: 14472334024676221 79: 23416728348467685 80: 37889062373143906 81: 61305790721611591 82: 99194853094755497 83: 160500643816367088 84: 259695496911122585 85: 420196140727489673 86: 679891637638612258 87: 1100087778366101931 88: 1779979416004714189 89: 2880067194370816120 90: 4660046610375530309 91: 7540113804746346429 92: -6246583658587674878 93: 1293530146158671551 94: -4953053512429003327 95: -3659523366270331776 96: -8612576878699335103 97: 6174643828739884737 98: -2437933049959450366 99: 3736710778780434371 100: 1298777728820984005 0.002 real 0.002 user 0.000 sys 99.42 cpu  To fork/join or to task? When the work unit size and compute speed is predictable, we can partition work in advance and schedule with omp for to achieve load balance. Satisfying both criteria is often hard:\n Adaptive algorithms, adaptive physics, implicit constitutive models AVX throttling, thermal throttling, network or file system contention, OS jitter  Fork/join and barriers are also high overhead, so we might want to express data dependencies more precisely.\nFor tasking to be efficient, it relies on overdecomposition, creating more work units than there are processing units. For many numerical algorithms, there is some overhead to overdecomposition. For example, in array processing, a halo/fringe/ghost/overlap region might need to be computed as part of each work patch, leading to time models along the lines of $$ t{\\text{tile}}(n) = t{\\text{latency}} + \\frac{(n+2)^3}{R} $$ where $R$ is the processing rate. In addition to the latency, the overhead fraction is $$ \\frac{(n+2)^3 - n^3}{n^3} \\approx 6/n $$ indicating that larger $n$ should be more efficient.\nHowever, if this overhead is acceptable and you still have load balancing challenges, tasking can be a solution. (Example from a recent blog/talk.)\nComputational depth and the critical path Consider the block Cholesky factorization algorithm (applying to the lower-triangular matrix $A$).\nExpressing essential data dependencies, this results in the following directed acyclic graph (DAG). No parallel algorithm can complete in less time than it takes for a sequential algorithm to perform each operation along the critical path (i.e., the minimum depth of this graph such that all arrows point downward).\nFigures from Agullo et al (2016): Are Static Schedules so Bad? A Case Study on Cholesky Factorization, which is an interesting counterpoint to the common narrative pushing dynamic scheduling.\nQuestion: what is the computational depth of summing an array? $$ \\sum_{i=0}^{N-1} a_i $$\ndouble sum = 0; for (int i=0; i\u0026lt;N; i++) sum += array[i];  Given an arbitrarily large number $P$ of processing units, what is the smallest computational depth to compute this mathematical result? (You\u0026rsquo;re free to use any associativity.)\n","date":1568638165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568638165,"objectID":"d659cdd715e46e4c3fe8dccf47ebd86c","permalink":"https://cucs-hpsc.github.io/fall2019/openmp-3/","publishdate":"2019-09-16T06:49:25-06:00","relpermalink":"/fall2019/openmp-3/","section":"fall2019","summary":"def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)  Using #pragma omp task Up to now, we\u0026rsquo;ve been expressing parallelism for iterating over an array.\nrender_c('task_dep.4.c')  #include \u0026lt;stdio.h\u0026gt; int main() { int x = 1; #pragma omp parallel #pragma omp single { #pragma omp task shared(x) depend(out: x) x = 2; #pragma omp task shared(x) depend(in: x) printf(\u0026quot;x + 1 = %d.","tags":null,"title":"OpenMP Tasks","type":"docs"},{"authors":null,"categories":null,"content":" What does the compiler do when we add #pragma omp parallel?\nstatic double dot_opt3(size_t n, const double *a, const double *b) { double sum = 0; omp_set_num_threads(4); #pragma omp parallel { #pragma omp for reduction(+:sum) for (size_t i=0; i\u0026lt;n; i++) sum += a[i] * b[i]; } return sum; }  gcc -Os -march=native -fopenmp dot.c -o dot objdump -d --prefix-addresses -M intel dot | grep dot_opt3  000000000000129f \u0026lt;main+0x1af\u0026gt; call 0000000000001779 \u0026lt;dot_opt3\u0026gt; 0000000000001779 \u0026lt;dot_opt3\u0026gt; push r12 000000000000177b \u0026lt;dot_opt3+0x2\u0026gt; mov r12,rdx 000000000000177e \u0026lt;dot_opt3+0x5\u0026gt; push rbp 000000000000177f \u0026lt;dot_opt3+0x6\u0026gt; mov rbp,rsi 0000000000001782 \u0026lt;dot_opt3+0x9\u0026gt; push rbx 0000000000001783 \u0026lt;dot_opt3+0xa\u0026gt; mov rbx,rdi 0000000000001786 \u0026lt;dot_opt3+0xd\u0026gt; mov edi,0x4 000000000000178b \u0026lt;dot_opt3+0x12\u0026gt; sub rsp,0x30 000000000000178f \u0026lt;dot_opt3+0x16\u0026gt; mov rax,QWORD PTR fs:0x28 0000000000001798 \u0026lt;dot_opt3+0x1f\u0026gt; mov QWORD PTR [rsp+0x28],rax 000000000000179d \u0026lt;dot_opt3+0x24\u0026gt; xor eax,eax 000000000000179f \u0026lt;dot_opt3+0x26\u0026gt; call 0000000000001070 \u0026lt;omp_set_num_threads@plt\u0026gt; 00000000000017a4 \u0026lt;dot_opt3+0x2b\u0026gt; xor ecx,ecx 00000000000017a6 \u0026lt;dot_opt3+0x2d\u0026gt; xor edx,edx 00000000000017a8 \u0026lt;dot_opt3+0x2f\u0026gt; lea rsi,[rsp+0x8] 00000000000017ad \u0026lt;dot_opt3+0x34\u0026gt; lea rdi,[rip+0xc1] # 0000000000001875 \u0026lt;dot_opt3._omp_fn.0\u0026gt; 00000000000017b4 \u0026lt;dot_opt3+0x3b\u0026gt; mov QWORD PTR [rsp+0x18],r12 00000000000017b9 \u0026lt;dot_opt3+0x40\u0026gt; mov QWORD PTR [rsp+0x10],rbp 00000000000017be \u0026lt;dot_opt3+0x45\u0026gt; mov QWORD PTR [rsp+0x8],rbx 00000000000017c3 \u0026lt;dot_opt3+0x4a\u0026gt; mov QWORD PTR [rsp+0x20],0x0 00000000000017cc \u0026lt;dot_opt3+0x53\u0026gt; call 00000000000010e0 \u0026lt;GOMP_parallel@plt\u0026gt; 00000000000017d1 \u0026lt;dot_opt3+0x58\u0026gt; mov rax,QWORD PTR [rsp+0x28] 00000000000017d6 \u0026lt;dot_opt3+0x5d\u0026gt; xor rax,QWORD PTR fs:0x28 00000000000017df \u0026lt;dot_opt3+0x66\u0026gt; vmovsd xmm0,QWORD PTR [rsp+0x20] 00000000000017e5 \u0026lt;dot_opt3+0x6c\u0026gt; je 00000000000017ec \u0026lt;dot_opt3+0x73\u0026gt; 00000000000017e7 \u0026lt;dot_opt3+0x6e\u0026gt; call 0000000000001080 \u0026lt;__stack_chk_fail@plt\u0026gt; 00000000000017ec \u0026lt;dot_opt3+0x73\u0026gt; add rsp,0x30 00000000000017f0 \u0026lt;dot_opt3+0x77\u0026gt; pop rbx 00000000000017f1 \u0026lt;dot_opt3+0x78\u0026gt; pop rbp 00000000000017f2 \u0026lt;dot_opt3+0x79\u0026gt; pop r12 00000000000017f4 \u0026lt;dot_opt3+0x7b\u0026gt; ret 0000000000001875 \u0026lt;dot_opt3._omp_fn.0\u0026gt; push r12 0000000000001877 \u0026lt;dot_opt3._omp_fn.0+0x2\u0026gt; push rbp 0000000000001878 \u0026lt;dot_opt3._omp_fn.0+0x3\u0026gt; mov rbp,rdi 000000000000187b \u0026lt;dot_opt3._omp_fn.0+0x6\u0026gt; push rbx 000000000000187c \u0026lt;dot_opt3._omp_fn.0+0x7\u0026gt; sub rsp,0x10 0000000000001880 \u0026lt;dot_opt3._omp_fn.0+0xb\u0026gt; mov rbx,QWORD PTR [rdi] 0000000000001883 \u0026lt;dot_opt3._omp_fn.0+0xe\u0026gt; test rbx,rbx 0000000000001886 \u0026lt;dot_opt3._omp_fn.0+0x11\u0026gt; jne 00000000000018b5 \u0026lt;dot_opt3._omp_fn.0+0x40\u0026gt; 0000000000001888 \u0026lt;dot_opt3._omp_fn.0+0x13\u0026gt; vxorpd xmm0,xmm0,xmm0 000000000000188c \u0026lt;dot_opt3._omp_fn.0+0x17\u0026gt; mov rax,QWORD PTR [rbp+0x18] 0000000000001890 \u0026lt;dot_opt3._omp_fn.0+0x1b\u0026gt; lea rdx,[rbp+0x18] 0000000000001894 \u0026lt;dot_opt3._omp_fn.0+0x1f\u0026gt; mov QWORD PTR [rsp],rax 0000000000001898 \u0026lt;dot_opt3._omp_fn.0+0x23\u0026gt; vaddsd xmm1,xmm0,QWORD PTR [rsp] 000000000000189d \u0026lt;dot_opt3._omp_fn.0+0x28\u0026gt; vmovsd QWORD PTR [rsp+0x8],xmm1 00000000000018a3 \u0026lt;dot_opt3._omp_fn.0+0x2e\u0026gt; mov rdi,QWORD PTR [rsp+0x8] 00000000000018a8 \u0026lt;dot_opt3._omp_fn.0+0x33\u0026gt; lock cmpxchg QWORD PTR [rdx],rdi 00000000000018ad \u0026lt;dot_opt3._omp_fn.0+0x38\u0026gt; cmp QWORD PTR [rsp],rax 00000000000018b1 \u0026lt;dot_opt3._omp_fn.0+0x3c\u0026gt; je 000000000000190c \u0026lt;dot_opt3._omp_fn.0+0x97\u0026gt; 00000000000018b3 \u0026lt;dot_opt3._omp_fn.0+0x3e\u0026gt; jmp 0000000000001894 \u0026lt;dot_opt3._omp_fn.0+0x1f\u0026gt; 00000000000018b5 \u0026lt;dot_opt3._omp_fn.0+0x40\u0026gt; call 00000000000010b0 \u0026lt;omp_get_num_threads@plt\u0026gt; 00000000000018ba \u0026lt;dot_opt3._omp_fn.0+0x45\u0026gt; mov r12d,eax 00000000000018bd \u0026lt;dot_opt3._omp_fn.0+0x48\u0026gt; call 0000000000001060 \u0026lt;omp_get_thread_num@plt\u0026gt; 00000000000018c2 \u0026lt;dot_opt3._omp_fn.0+0x4d\u0026gt; movsxd rcx,eax 00000000000018c5 \u0026lt;dot_opt3._omp_fn.0+0x50\u0026gt; movsxd rsi,r12d 00000000000018c8 \u0026lt;dot_opt3._omp_fn.0+0x53\u0026gt; mov rax,rbx 00000000000018cb \u0026lt;dot_opt3._omp_fn.0+0x56\u0026gt; xor edx,edx 00000000000018cd \u0026lt;dot_opt3._omp_fn.0+0x58\u0026gt; div rsi 00000000000018d0 \u0026lt;dot_opt3._omp_fn.0+0x5b\u0026gt; cmp rcx,rdx 00000000000018d3 \u0026lt;dot_opt3._omp_fn.0+0x5e\u0026gt; jb 0000000000001905 \u0026lt;dot_opt3._omp_fn.0+0x90\u0026gt; 00000000000018d5 \u0026lt;dot_opt3._omp_fn.0+0x60\u0026gt; imul rcx,rax 00000000000018d9 \u0026lt;dot_opt3._omp_fn.0+0x64\u0026gt; vxorpd xmm0,xmm0,xmm0 00000000000018dd \u0026lt;dot_opt3._omp_fn.0+0x68\u0026gt; add rdx,rcx 00000000000018e0 \u0026lt;dot_opt3._omp_fn.0+0x6b\u0026gt; add rax,rdx 00000000000018e3 \u0026lt;dot_opt3._omp_fn.0+0x6e\u0026gt; cmp rdx,rax 00000000000018e6 \u0026lt;dot_opt3._omp_fn.0+0x71\u0026gt; jae 000000000000188c \u0026lt;dot_opt3._omp_fn.0+0x17\u0026gt; 00000000000018e8 \u0026lt;dot_opt3._omp_fn.0+0x73\u0026gt; mov rcx,QWORD PTR [rbp+0x10] 00000000000018ec \u0026lt;dot_opt3._omp_fn.0+0x77\u0026gt; mov rsi,QWORD PTR [rbp+0x8] 00000000000018f0 \u0026lt;dot_opt3._omp_fn.0+0x7b\u0026gt; vmovsd xmm2,QWORD PTR [rsi+rdx*8] 00000000000018f5 \u0026lt;dot_opt3._omp_fn.0+0x80\u0026gt; vfmadd231sd xmm0,xmm2,QWORD PTR [rcx+rdx*8] 00000000000018fb \u0026lt;dot_opt3._omp_fn.0+0x86\u0026gt; inc rdx 00000000000018fe \u0026lt;dot_opt3._omp_fn.0+0x89\u0026gt; cmp rax,rdx 0000000000001901 \u0026lt;dot_opt3._omp_fn.0+0x8c\u0026gt; jne 00000000000018f0 \u0026lt;dot_opt3._omp_fn.0+0x7b\u0026gt; 0000000000001903 \u0026lt;dot_opt3._omp_fn.0+0x8e\u0026gt; jmp 000000000000188c \u0026lt;dot_opt3._omp_fn.0+0x17\u0026gt; 0000000000001905 \u0026lt;dot_opt3._omp_fn.0+0x90\u0026gt; inc rax 0000000000001908 \u0026lt;dot_opt3._omp_fn.0+0x93\u0026gt; xor edx,edx 000000000000190a \u0026lt;dot_opt3._omp_fn.0+0x95\u0026gt; jmp 00000000000018d5 \u0026lt;dot_opt3._omp_fn.0+0x60\u0026gt; 000000000000190c \u0026lt;dot_opt3._omp_fn.0+0x97\u0026gt; add rsp,0x10 0000000000001910 \u0026lt;dot_opt3._omp_fn.0+0x9b\u0026gt; pop rbx 0000000000001911 \u0026lt;dot_opt3._omp_fn.0+0x9c\u0026gt; pop rbp 0000000000001912 \u0026lt;dot_opt3._omp_fn.0+0x9d\u0026gt; pop r12 0000000000001914 \u0026lt;dot_opt3._omp_fn.0+0x9f\u0026gt; ret  Anatomy of a parallel region graph LR; A[dot_opt3]--B[GOMP_parallel]; B-- id=0_ --C{dot_opt3._omp_fn.0}; B-- id=1_ --C{dot_opt3._omp_fn.0}; B-- id=2_ --C{dot_opt3._omp_fn.0}; A-. \"Body inside__\" .-C; C--D[omp_get_num_threads]; C--E[omp_get_thread_num]; style A fill:#9b9,stroke:#686,stroke-width:4px; style C fill:#9b9,stroke:#668,stroke-width:8px;  Memory semantics For each variable accessed within the parallel region, we can specify whether it is\n private to the thread, with value undefined inside the region firstprivate, which is like private, but initialized by the value upon entering the parallel region shared, meaning that every thread accesses the same value in memory (but changes are not immediately visible)\nint a=0, b=1, c=2; #pragma omp parallel private(a) firstprivate(b) shared(c) { int id = omp_get_thread_num(); a++; b++; c++; printf(\u0026quot;[%d] %d %d %d\\n\u0026quot;, id, a, b, c); } printf(\u0026quot;END: %d %d %d\\n\u0026quot;, a, b, c);  make CFLAGS='-fopenmp -Wall' -B omp-mem  cc -fopenmp -Wall omp-mem.c -o omp-mem omp-mem.c: In function ‘main._omp_fn.0’: omp-mem.c:8:6: warning: ‘a’ is used uninitialized in this function [-Wuninitialized] 8 | a++; | ~^~ omp-mem.c:5:7: note: ‘a’ was declared here 5 | int a=1, b=2, c=3; | ^   Question: How could the compiler get firstprivate and shared variables into the scope of dot_opt3._omp_fn.0?\nProgramming style I find private semantics unnecessary and error-prone. We can just declare those variables at inner-most scope.\nint b=1, c=2; #pragma omp parallel firstprivate(b) shared(c) { int a = 0; int id = omp_get_thread_num(); a++; b++; c++; printf(\u0026quot;[%d] %d %d %d\\n\u0026quot;, id, a, b, c); } printf(\u0026quot;END: %d %d %d\\n\u0026quot;, a, b, c); // Error: a not in scope here  Updating shared variables We see that the shared variable c has lots of opportunities for conflict.\nsequenceDiagram Thread0--Memory: load c=2 Thread1--Memory: load c=2 Note left of Thread0: c++ (=3) Note right of Thread1: c++ (=3) Note right of Thread1: print c Thread0--Memory: store c=3 Thread1--Memory: store c=3 Note left of Thread0: print c  If we run the above many times, we may sometimes find that multiple processes have the same value of c, each thread observes different increments from others, and the total number of increments may vary.\nWe can define ordering semantics using atomic, critical, and barrier.\nint b=1, c=2; #pragma omp parallel firstprivate(b) shared(c) { int a = 1; int id = omp_get_thread_num(); b++; #pragma omp critical c++; #pragma omp barrier printf(\u0026quot;[%d] %d %d %d\\n\u0026quot;, id, a, b, c); } printf(\u0026quot;END: _ %d %d\\n\u0026quot;, b, c);  A quick demo of perf Linux perf is a kernel interrupt-based profiling tool. It uses performance counters and interrupts to diagnose all sorts of bottlenecks.\n$ perf stat ./dot -n 10000 \u0026gt; /dev/null Performance counter stats for './dot -n 10000': 1.56 msec task-clock:u # 1.201 CPUs utilized 0 context-switches:u # 0.000 K/sec 0 cpu-migrations:u # 0.000 K/sec 124 page-faults:u # 0.079 M/sec 3,041,706 cycles:u # 1.947 GHz 2,272,231 instructions:u # 0.75 insn per cycle 410,889 branches:u # 262.962 M/sec 7,911 branch-misses:u # 1.93% of all branches 0.001301176 seconds time elapsed 0.001970000 seconds user 0.000000000 seconds sys  $ perf record -g ./dot -n 10000 -r 1000 \u0026gt; /dev/null [ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.075 MB perf.data (1098 samples) ]  $ perf report -M intel  Note how GOMP overhead dominates the cost in this experiment. We need more work (longer arrays, etc.) to justify the overhead of distributing and collecting the parallel work.\nWe can drill down into particular functions (especially ours, which we have hopefully compiled with -g to include debugging information).\nFrom this, we see specific instructions, and their corresponding lines of code, that are most frequently being processed when the kernel interrupts to check. In this experiment, we see *sd \u0026ldquo;scalar double\u0026rdquo; instructions, indicating lack of vectorization.\nIn contrast, the following annotation shows use of *pd \u0026ldquo;packed double\u0026rdquo; instructions, indicating that the \u0026ldquo;hot\u0026rdquo; loop has been vectorized.\n(The reason for vectorization can sometimes be determined by -fopt-info -fopt-info-missed, and can be encouraged by techniques like manually splitting accumulators, preventing aliasing by using restrict, directives like #pragma omp simd, and global compiler flags like -ffast-math.)\nFor more on perf, see Brendan Gregg\u0026rsquo;s Linux Performance site.\n","date":1568378965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568408667,"objectID":"93138c81f27ec2d1fa84a97d0e9493af","permalink":"https://cucs-hpsc.github.io/fall2019/openmp-2/","publishdate":"2019-09-13T06:49:25-06:00","relpermalink":"/fall2019/openmp-2/","section":"fall2019","summary":"What does the compiler do when we add #pragma omp parallel?\nstatic double dot_opt3(size_t n, const double *a, const double *b) { double sum = 0; omp_set_num_threads(4); #pragma omp parallel { #pragma omp for reduction(+:sum) for (size_t i=0; i\u0026lt;n; i++) sum += a[i] * b[i]; } return sum; }  gcc -Os -march=native -fopenmp dot.c -o dot objdump -d --prefix-addresses -M intel dot | grep dot_opt3  000000000000129f \u0026lt;main+0x1af\u0026gt; call 0000000000001779 \u0026lt;dot_opt3\u0026gt; 0000000000001779 \u0026lt;dot_opt3\u0026gt; push r12 000000000000177b \u0026lt;dot_opt3+0x2\u0026gt; mov r12,rdx 000000000000177e \u0026lt;dot_opt3+0x5\u0026gt; push rbp 000000000000177f \u0026lt;dot_opt3+0x6\u0026gt; mov rbp,rsi 0000000000001782 \u0026lt;dot_opt3+0x9\u0026gt; push rbx 0000000000001783 \u0026lt;dot_opt3+0xa\u0026gt; mov rbx,rdi 0000000000001786 \u0026lt;dot_opt3+0xd\u0026gt; mov edi,0x4 000000000000178b \u0026lt;dot_opt3+0x12\u0026gt; sub rsp,0x30 000000000000178f \u0026lt;dot_opt3+0x16\u0026gt; mov rax,QWORD PTR fs:0x28 0000000000001798 \u0026lt;dot_opt3+0x1f\u0026gt; mov QWORD PTR [rsp+0x28],rax 000000000000179d \u0026lt;dot_opt3+0x24\u0026gt; xor eax,eax 000000000000179f \u0026lt;dot_opt3+0x26\u0026gt; call 0000000000001070 \u0026lt;omp_set_num_threads@plt\u0026gt; 00000000000017a4 \u0026lt;dot_opt3+0x2b\u0026gt; xor ecx,ecx 00000000000017a6 \u0026lt;dot_opt3+0x2d\u0026gt; xor edx,edx 00000000000017a8 \u0026lt;dot_opt3+0x2f\u0026gt; lea rsi,[rsp+0x8] 00000000000017ad \u0026lt;dot_opt3+0x34\u0026gt; lea rdi,[rip+0xc1] # 0000000000001875 \u0026lt;dot_opt3.","tags":null,"title":"More OpenMP","type":"docs"},{"authors":null,"categories":null,"content":" def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)  What is OpenMP? A community-developed standard Application Programming Interface (with \u0026ldquo;directives\u0026rdquo;) for * multithreaded programming * vectorization * offload to coprocessors (such as GPUs)\nOpenMP is available for C, C++, and Fortran.\nLatest version: OpenMP-5.0, released November 2018. Implementations are still incomplete!\nOpenMP Resources  OpenMP-5.0 Reference Cards (a few pages, printable) OpenMP-5.0 Standard OpenMP-4.5 Examples LLNL Tutorial Mattson: The OpenMP Common Core from ATPESC (video)  #pragma omp parallel The standard is big, but most applications only use a few constructs.\nrender_c('omp-hello.c')  #include \u0026lt;omp.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main() { #pragma omp parallel { int num_threads = omp_get_num_threads(); int my_thread_num = omp_get_thread_num(); printf(\u0026quot;I am %d of %d\\n\u0026quot;, my_thread_num, num_threads); } return 0; }  !make CFLAGS='-fopenmp -Wall' -B omp-hello  cc -fopenmp -Wall omp-hello.c -o omp-hello  !./omp-hello  I am 1 of 4 I am 2 of 4 I am 0 of 4 I am 3 of 4  !OMP_NUM_THREADS=8 ./omp-hello  I am 0 of 8 I am 7 of 8 I am 1 of 8 I am 3 of 8 I am 4 of 8 I am 6 of 8 I am 2 of 8 I am 5 of 8  Parallelizing triad void triad(int N, double *a, const double *b, double scalar, const double *c) { #pragma omp parallel { for (int i=0; i\u0026lt;N; i++) a[i] = b[i] + scalar * c[i]; } }  What does this code do?\nvoid triad(int N, double *a, const double *b, double scalar, const double *c) { #pragma omp parallel { int id = omp_get_thread_num(); int num_threads = omp_get_num_threads(); for (int i=id; i\u0026lt;N; i+=num_threads) a[i] = b[i] + scalar * c[i]; } }  Parallelizing dot static double dot_ref(size_t n, const double *a, const double *b) { double sum = 0; for (size_t i=0; i\u0026lt;n; i++) sum += a[i] * b[i]; return sum; }  !make CFLAGS='-O3 -march=native -fopenmp' -B dot  cc -O3 -march=native -fopenmp dot.c -o dot  !OMP_NUM_THREADS=2 ./dot -r 10 -n 10000   Name flops ticks flops/tick dot_ref 20000 40327 0.50 dot_ref 20000 35717 0.56 dot_ref 20000 36096 0.55 dot_ref 20000 36487 0.55 dot_ref 20000 37157 0.54 dot_ref 20000 36024 0.56 dot_ref 20000 35322 0.57 dot_ref 20000 36601 0.55 dot_ref 20000 72193 0.28 dot_ref 20000 37924 0.53 dot_opt1 20000 51256384 0.00 dot_opt1 20000 23343145 0.00 dot_opt1 20000 4646174 0.00 dot_opt1 20000 16710 1.20 dot_opt1 20000 15512 1.29 dot_opt1 20000 16016 1.25 dot_opt1 20000 16982 1.18 dot_opt1 20000 452064 0.04 dot_opt1 20000 16278 1.23 dot_opt1 20000 16311 1.23 dot_opt2 20000 24616 0.81 dot_opt2 20000 16095 1.24 dot_opt2 20000 17561 1.14 dot_opt2 20000 16270 1.23 dot_opt2 20000 18130 1.10 dot_opt2 20000 16831 1.19 dot_opt2 20000 16968 1.18 dot_opt2 20000 16391 1.22 dot_opt2 20000 17063 1.17 dot_opt2 20000 16315 1.23 dot_opt3 20000 77013 0.26 dot_opt3 20000 12419 1.61 dot_opt3 20000 12124 1.65 dot_opt3 20000 12193 1.64 dot_opt3 20000 12051 1.66 dot_opt3 20000 12009 1.67 dot_opt3 20000 11944 1.67 dot_opt3 20000 12032 1.66 dot_opt3 20000 12687 1.58 dot_opt3 20000 12188 1.64  Vectorization OpenMP-4.0 added the omp simd construct, which is a portable way to request that the compiler vectorize code. An example of a reason why a compiler might fail to vectorize code is aliasing, which we investigate below.\nrender_c('triad.c')  #include \u0026lt;stdlib.h\u0026gt; void triad(size_t N, double *a, const double *b, double scalar, const double *c) { for (size_t i=0; i\u0026lt;N; i++) a[i] = b[i] + scalar * c[i]; }  !gcc -O2 -ftree-vectorize -fopt-info-all -c triad.c  Unit growth for small function inlining: 15-\u0026gt;15 (0%) Inlined 0 calls, eliminated 0 functions triad.c:4:3: optimized: loop vectorized using 16 byte vectors triad.c:4:3: optimized: loop versioned for vectorization because of possible aliasing triad.c:3:6: note: vectorized 1 loops in function. triad.c:4:3: optimized: loop turned into non-loop; it never loops   gcc autovectorization starts at -O3 or if you use -ftree-vectorize options such as -fopt-info give useful diagnostics, but are compiler-dependent and sometimes referring to assembly is useful man gcc with search (/) is your friend  What is aliasing? Is this valid code? What xs x after this call?\ndouble x[5] = {1, 2, 3, 4, 5}; triad(2, x+1, x, 10., x);  C allows memory to overlap arbitrarily. You can inform the compiler of this using the restrict qualifier (C99/C11; __restrict or __restrict__ work with most C++ and CUDA compilers).\nrender_c('triad-restrict.c')  void triad(int N, double *restrict a, const double *restrict b, double scalar, const double *restrict c) { for (int i=0; i\u0026lt;N; i++) a[i] = b[i] + scalar * c[i]; }  !gcc -O2 -march=native -ftree-vectorize -fopt-info-all -c triad-restrict.c  Unit growth for small function inlining: 15-\u0026gt;15 (0%) Inlined 0 calls, eliminated 0 functions triad-restrict.c:2:5: optimized: loop vectorized using 32 byte vectors triad-restrict.c:1:6: note: vectorized 1 loops in function.  Notice how there is no more loop versioned for vectorization because of possible aliasing.\nThe complexity of checking for aliasing can grow combinatorially in the number of arrays being processed, leading to many loop variants and/or preventing vectorization.\nAside: Warnings The -Wrestrict flag (included in -Wall) can catch some programming errors\nvoid foo(double *x) { triad(2, x, x, 10, x); }  !gcc -O2 -Wall -c triad-foo.c  The powers of -Wrestrict are limited, however, and (as of gcc-9) do not even catch\nvoid foo(double *x) { triad(2, x+1, x, 10, x); }  Check the assembly !objdump -d --prefix-addresses -M intel triad-restrict.o  triad-restrict.o: file format elf64-x86-64 Disassembly of section .text: 0000000000000000 \u0026lt;triad\u0026gt; test edi,edi 0000000000000002 \u0026lt;triad+0x2\u0026gt; jle 0000000000000067 \u0026lt;triad+0x67\u0026gt; 0000000000000004 \u0026lt;triad+0x4\u0026gt; lea eax,[rdi-0x1] 0000000000000007 \u0026lt;triad+0x7\u0026gt; cmp eax,0x2 000000000000000a \u0026lt;triad+0xa\u0026gt; jbe 0000000000000074 \u0026lt;triad+0x74\u0026gt; 000000000000000c \u0026lt;triad+0xc\u0026gt; mov r8d,edi 000000000000000f \u0026lt;triad+0xf\u0026gt; shr r8d,0x2 0000000000000013 \u0026lt;triad+0x13\u0026gt; vbroadcastsd ymm2,xmm0 0000000000000018 \u0026lt;triad+0x18\u0026gt; shl r8,0x5 000000000000001c \u0026lt;triad+0x1c\u0026gt; xor eax,eax 000000000000001e \u0026lt;triad+0x1e\u0026gt; xchg ax,ax 0000000000000020 \u0026lt;triad+0x20\u0026gt; vmovupd ymm1,YMMWORD PTR [rcx+rax*1] 0000000000000025 \u0026lt;triad+0x25\u0026gt; vfmadd213pd ymm1,ymm2,YMMWORD PTR [rdx+rax*1] 000000000000002b \u0026lt;triad+0x2b\u0026gt; vmovupd YMMWORD PTR [rsi+rax*1],ymm1 0000000000000030 \u0026lt;triad+0x30\u0026gt; add rax,0x20 0000000000000034 \u0026lt;triad+0x34\u0026gt; cmp rax,r8 0000000000000037 \u0026lt;triad+0x37\u0026gt; jne 0000000000000020 \u0026lt;triad+0x20\u0026gt; 0000000000000039 \u0026lt;triad+0x39\u0026gt; mov eax,edi 000000000000003b \u0026lt;triad+0x3b\u0026gt; and eax,0xfffffffc 000000000000003e \u0026lt;triad+0x3e\u0026gt; test dil,0x3 0000000000000042 \u0026lt;triad+0x42\u0026gt; je 0000000000000070 \u0026lt;triad+0x70\u0026gt; 0000000000000044 \u0026lt;triad+0x44\u0026gt; vzeroupper 0000000000000047 \u0026lt;triad+0x47\u0026gt; cdqe 0000000000000049 \u0026lt;triad+0x49\u0026gt; nop DWORD PTR [rax+0x0] 0000000000000050 \u0026lt;triad+0x50\u0026gt; vmovsd xmm1,QWORD PTR [rcx+rax*8] 0000000000000055 \u0026lt;triad+0x55\u0026gt; vfmadd213sd xmm1,xmm0,QWORD PTR [rdx+rax*8] 000000000000005b \u0026lt;triad+0x5b\u0026gt; vmovsd QWORD PTR [rsi+rax*8],xmm1 0000000000000060 \u0026lt;triad+0x60\u0026gt; inc rax 0000000000000063 \u0026lt;triad+0x63\u0026gt; cmp edi,eax 0000000000000065 \u0026lt;triad+0x65\u0026gt; jg 0000000000000050 \u0026lt;triad+0x50\u0026gt; 0000000000000067 \u0026lt;triad+0x67\u0026gt; ret 0000000000000068 \u0026lt;triad+0x68\u0026gt; nop DWORD PTR [rax+rax*1+0x0] 0000000000000070 \u0026lt;triad+0x70\u0026gt; vzeroupper 0000000000000073 \u0026lt;triad+0x73\u0026gt; ret 0000000000000074 \u0026lt;triad+0x74\u0026gt; xor eax,eax 0000000000000076 \u0026lt;triad+0x76\u0026gt; jmp 0000000000000047 \u0026lt;triad+0x47\u0026gt;   How do the results change if you go up and replace -march=native with -march=skylake-avx512 -mprefer-vector-width=512? Is the assembly qualitatively different without restrict (in which case the compiler \u0026ldquo;versions\u0026rdquo; the loop).  Pragma omp simd An alternative (or supplement) to restrict is #pragma omp simd.\nrender_c('triad-omp-simd.c')  void triad(int N, double *a, const double *b, double scalar, const double *c) { #pragma omp simd for (int i=0; i\u0026lt;N; i++) a[i] = b[i] + scalar * c[i]; }  !gcc -O2 -march=native -ftree-vectorize -fopenmp -fopt-info-all -c triad-omp-simd.c  Unit growth for small function inlining: 15-\u0026gt;15 (0%) Inlined 0 calls, eliminated 0 functions triad-omp-simd.c:4:17: optimized: loop vectorized using 32 byte vectors triad-omp-simd.c:1:6: note: vectorized 1 loops in function.  ","date":1568206165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568230419,"objectID":"0c668380df9385e8019b78a77771b1d4","permalink":"https://cucs-hpsc.github.io/fall2019/openmp/","publishdate":"2019-09-11T06:49:25-06:00","relpermalink":"/fall2019/openmp/","section":"fall2019","summary":"def render_c(filename): from IPython.display import Markdown with open(filename) as f: contents = f.read() return Markdown(\u0026quot;```c\\n\u0026quot; + contents + \u0026quot;```\\n\u0026quot;)  What is OpenMP? A community-developed standard Application Programming Interface (with \u0026ldquo;directives\u0026rdquo;) for * multithreaded programming * vectorization * offload to coprocessors (such as GPUs)\nOpenMP is available for C, C++, and Fortran.\nLatest version: OpenMP-5.0, released November 2018. Implementations are still incomplete!\nOpenMP Resources  OpenMP-5.0 Reference Cards (a few pages, printable) OpenMP-5.","tags":null,"title":"OpenMP Basics","type":"docs"},{"authors":null,"categories":null,"content":" Programs with more than one part So far, we\u0026rsquo;ve focused on simple programs with only one part, but real programs have several different parts, often with data dependencies. Some parts will be amenable to optimization and/or parallelism and others will not. This principle is called Amdahl\u0026rsquo;s Law.\ndef exec_time(f, p, n=10, latency=1): # Suppose that a fraction f of the total work is amenable to optimization # We run a problem size n with parallelization factor p return latency + (1-f)*n + f*n/p  %matplotlib inline import matplotlib.pyplot as plt import pandas import numpy as np plt.style.use('seaborn') ps = np.geomspace(1, 1000) plt.loglog(ps, exec_time(.99, ps, latency=0)) plt.loglog(ps, exec_time(1, ps, latency=0)) plt.title('Strong scaling') plt.xlabel('p') plt.ylabel('time');  Strong scaling: fixed total problem size Cost = time * p def exec_cost(f, p, **kwargs): return exec_time(f, p, **kwargs) * p plt.loglog(ps, exec_cost(.99, ps)) plt.title('Strong scaling') plt.xlabel('p') plt.ylabel('cost');  Efficiency plt.semilogx(ps, 1/exec_cost(.99, ps, latency=1)) plt.title('Strong scaling') plt.xlabel('p') plt.ylabel('efficiency') plt.ylim(bottom=0);  Speedup $$ S(p) = \\frac{T(1)}{T(p)} $$\nplt.plot(ps, exec_time(.99, 1, latency=1) / exec_time(.99, ps, latency=1)) plt.title('Strong scaling') plt.xlabel('p') plt.ylabel('speedup') plt.ylim(bottom=0);  Stunt 1: Report speedup, not absolute performance! Efficiency-Time spectrum (my preference) People care about two observable properties * Time until job completes * Cost in core-hours or dollars to do job\nMost HPC applications have access to large machines, so don\u0026rsquo;t really care how many processes they use for any given job.\nplt.plot(exec_time(.99, ps), 1/exec_cost(.99, ps), 'o-') plt.title('Strong scaling') plt.xlabel('time') plt.ylabel('efficiency') plt.ylim(bottom=0); plt.xlim(left=0);  Principles  No \u0026ldquo;soft\u0026rdquo; log scale Both axes have tangible units Bigger is better on the $y$ axis  Weak Scaling: Fixed work per processor We\u0026rsquo;ve kept the problem size $n$ fixed thus far, but parallel computers are also used to solve large problems. If we keep the amount of work per processor fixed, we are weak/Gustafson scaling.\nns = 10*ps plt.semilogx(ps, ns/exec_cost(.99, ps, n=ns, latency=1), 'o-') ns = 100*ps plt.semilogx(ps, ns/exec_cost(.99, ps, n=ns, latency=1), 's-') plt.title('Weak scaling') plt.xlabel('procs') plt.ylabel('efficiency') plt.ylim(bottom=0);  for w in np.geomspace(0.1, 1e3, 20): ns = w*ps plt.semilogx(exec_time(.99, ps, n=ns, latency=1), ns/exec_cost(.99, ps, n=ns, latency=1), 'o-') plt.title('Weak scaling') plt.xlabel('time') plt.ylabel('efficiency') plt.ylim(bottom=0);  Fuhrer et al (2018): Near-global climate simulation at 1 km resolution I replotted these data for my talk at the Latsis Symposium last month.\nFurther resources  Hager: Fooling the masses  Learn by counter-examples  Hoefler and Belli: Scientific Benchmarking of Parallel Computing Systems  Recommended best practices, especially for dealing with performance variability   Please read/watch something from this list and be prepared to share on Monday.\n","date":1567778851,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567795563,"objectID":"2926b880e944c22aa6e3f38e1d6b50ee","permalink":"https://cucs-hpsc.github.io/fall2019/intro-parallel-scaling/","publishdate":"2019-09-06T08:07:31-06:00","relpermalink":"/fall2019/intro-parallel-scaling/","section":"fall2019","summary":"Programs with more than one part So far, we\u0026rsquo;ve focused on simple programs with only one part, but real programs have several different parts, often with data dependencies. Some parts will be amenable to optimization and/or parallelism and others will not. This principle is called Amdahl\u0026rsquo;s Law.\ndef exec_time(f, p, n=10, latency=1): # Suppose that a fraction f of the total work is amenable to optimization # We run a problem size n with parallelization factor p return latency + (1-f)*n + f*n/p  %matplotlib inline import matplotlib.","tags":null,"title":"Intro to Parallel Scaling","type":"docs"},{"authors":null,"categories":null,"content":" Why model performance? Models give is a conceptual and roughly quantitative framework by which to answer the following types of questions.\n Why is an implementation exhibiting its observed performance? How will performance change if we:  optimize this component? buy new hardware? (Which new hardware?) run a different configuration?  While conceptualizing a new algorithm, what performance can we expect and what will be bottlenecks?  Models are a guide for performance, but not an absolute.\nTerms    Symbol Meaning     $n$ Input parameter related to problem size   $W$ Amount of work to solve problem $n$   $T$ Execution time   $R$ Rate at which work is done    STREAM Triad for (i=0; i\u0026lt;n; i++) a[i] = b[i] + scalar*c[i];  $n$ is the array size and $$W = 3 \\cdot \\texttt{sizeof(double)} \\cdot n$$ is the number of bytes transferred. The rate $R = W/T$ is measured in bytes per second (or MB/s, etc.).\nDense matrix multiplication To perform the operation $C \\gets C + A B$ where $A,B,C$ are $n\\times n$ matrices.\nfor (i=0; i\u0026lt;n; i++) for (j=0; j\u0026lt;n; j++) for (k=0; k\u0026lt;n; k++) c[i*n+j] += a[i*n+k] * b[k*n+j];   Can you identify two expressions for the total amount of work $W(n)$ and the associated units? Can you think of a context in which one is better than the other and vice-versa?  Estimating time To estimate time, we need to know how fast hardware executes flops and moves bytes.\n%matplotlib inline import matplotlib.pyplot as plt import pandas import numpy as np plt.style.use('seaborn') hardware = pandas.read_csv('data-intel.csv', index_col=\u0026quot;Name\u0026quot;) hardware   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Year GFLOPs-SP GFLOPs-DP Cores Mem-GBps TDP Freq(MHz)   Name            Xeon X5482 2007 102 51 4 26 150 3200   Xeon X5492 2008 108 54 4 26 150 3400   Xeon W5590 2009 106 53 4 32 130 3300   Xeon X5680 2010 160 80 6 32 130 3300   Xeon X5690 2011 166 83 6 32 130 3470   Xeon E5-2690 2012 372 186 8 51 135 2900   Xeon E5-2697 v2 2013 518 259 12 60 130 2700   Xeon E5-2699 v3 2014 1324 662 18 68 145 2300   Xeon E5-2699 v3 2015 1324 662 18 68 145 2300   Xeon E5-2699 v4 2016 1548 774 22 77 145 2200   Xeon Platinum 8180 2017 4480 2240 28 120 205 2500   Xeon Platinum 9282 2018 9320 4660 56 175 400 2600     fig = hardware.plot(x='GFLOPs-DP', y='Mem-GBps', marker='o') fig.set_xlim(left=0) fig.set_ylim(bottom=0);  So we have rates $R_f = 4660 \\cdot 10^9$ flops/second and $R_m = 175 \\cdot 10^9$ bytes/second. Now we need to characterize some algorithms.\nalgs = pandas.read_csv('algs.csv', index_col='Name') algs['intensity'] = algs['flops'] / algs['bytes'] algs = algs.sort_values('intensity') algs   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    bytes flops intensity   Name        Triad 24 2 0.083333   SpMV 12 2 0.166667   Stencil27-cache 24 54 2.250000   MatFree-FEM 2376 15228 6.409091   Stencil27-ideal 8 54 6.750000     def exec_time(machine, alg, n): bytes = n * alg.bytes flops = n * alg.flops T_mem = bytes / (machine['Mem-GBps'] * 1e9) T_flops = flops / (machine['GFLOPs-DP'] * 1e9) return max(T_mem, T_flops) exec_time(hardware.loc['Xeon Platinum 9282'], algs.loc['SpMV'], 1e8)  0.006857142857142857  for _, machine in hardware.iterrows(): for _, alg in algs.iterrows(): ns = np.geomspace(1e4, 1e9, 10) times = np.array([exec_time(machine, alg, n) for n in ns]) flops = np.array([alg.flops * n for n in ns]) rates = flops/times plt.loglog(ns, rates, 'o-') plt.xlabel('n') plt.ylabel('rate');  It looks like performance does not depend on problem size.\nWell, yeah, we chose a model in which flops and bytes were both proportional to $n$, and our machine model has no sense of cache hierarchy or latency, so time is also proportional to $n$. We can divide through by $n$ and yield a more illuminating plot.\nfor _, machine in hardware.iterrows(): times = np.array([exec_time(machine, alg, 1) for _, alg in algs.iterrows()]) rates = algs.flops/times intensities = algs.intensity plt.loglog(intensities, rates, 'o-', label=machine.name) plt.xlabel('intensity') plt.ylabel('rate') plt.legend();  We\u0026rsquo;re seeing the \u0026ldquo;roofline\u0026rdquo; for the older processors while the newer models are memory bandwidth limited for all of these algorithms.\nRecommended reading on single-node performance modeling  Williams, Waterman, Patterson (2009): Roofline: An insightful visual performance model for multicore architectures  ","date":1567610248,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567794459,"objectID":"80593eab3fbd65c9fcac19929daefb89","permalink":"https://cucs-hpsc.github.io/fall2019/intro-modeling/","publishdate":"2019-09-04T09:17:28-06:00","relpermalink":"/fall2019/intro-modeling/","section":"fall2019","summary":"Why model performance? Models give is a conceptual and roughly quantitative framework by which to answer the following types of questions.\n Why is an implementation exhibiting its observed performance? How will performance change if we:  optimize this component? buy new hardware? (Which new hardware?) run a different configuration?  While conceptualizing a new algorithm, what performance can we expect and what will be bottlenecks?  Models are a guide for performance, but not an absolute.","tags":null,"title":"Introduction to Performance Modeling","type":"docs"},{"authors":null,"categories":null,"content":" Remember how single-thread performance has increased significantly since ~2004 when clock frequency stagnated?\nThis is a result of doing more per clock cycle.\nLet\u0026rsquo;s visit some slides:\n Georg Hager (2019): Modern Computer Architucture  Further resources  Intel Intrinsics Guide Wikichip  Intel Xeon: Cascade Lake AMD EPYC gen2: Rome IBM POWER9  Agner Fog\u0026rsquo;s website  ","date":1567184400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567206505,"objectID":"28d2cfe1e2d75d1102ca03a990db79bf","permalink":"https://cucs-hpsc.github.io/fall2019/intro-vectorization/","publishdate":"2019-08-30T11:00:00-06:00","relpermalink":"/fall2019/intro-vectorization/","section":"fall2019","summary":" Remember how single-thread performance has increased significantly since ~2004 when clock frequency stagnated?\nThis is a result of doing more per clock cycle.\nLet\u0026rsquo;s visit some slides:\n Georg Hager (2019): Modern Computer Architucture  Further resources  Intel Intrinsics Guide Wikichip  Intel Xeon: Cascade Lake AMD EPYC gen2: Rome IBM POWER9  Agner Fog\u0026rsquo;s website  ","tags":null,"title":"Vectorization and Instruction-Level Parallelism","type":"docs"},{"authors":null,"categories":null,"content":" Cores, caches, and memory A von Neumann Architecture A contemporary architecture My laptop We can get this kind of information for our machine using hwloc, which provides a library as well as the command-line tool lstopo.\n!lstopo --output-format svg \u0026gt; lstopo-local.svg  A double-socket compute node with two GPUs 2x Xeon Ivy-Bridge-EP E5-2680v2 + 2x NVIDIA GPUs (from 2013, with hwloc v1.11). GPUs are reported as CUDA devices and X11 display :1.0: (from the hwloc gallery) Block diagrams A block diagram from a vendor can include additional information about how cores are physically connected.\nRing bus (Xeon E5-2600 family) Mesh bus (Xeon Scalable family) Multi-socket configurations https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview\nMultiple nodes go into racks or cabinets Terminology  Core (virtual and physical): has a single program counter (logically sequential processing of instructions) Memory channel: e.g., DDR4-2400: transfers 64 bits (8 bytes) at a rate of 2400 MHz = 15.36 GB/s Socket or CPU: contains multiple cores in a single piece* of silicon Non-Uniform Memory Access (NUMA): different channels may be different distances from a core Compute node: one or more sockets, plus memory, network card, etc.  How expensive is it to access memory? What does that mean? How would we measure?\nMcKenney (2013): Laws of Physics\nInteractive\nVariation by vendor\nHow your program accesses memory double a[1000]; void foo() { for (int i=0; i\u0026lt;1000; i++) a[i] = 1.234 * i; }  The compiler turns the loop body into instructions, which we can examine using Godbolt.\npxor xmm0, xmm0 ; zero the xmm0 register cvtsi2sd xmm0, eax ; convert the integer i to double mulsd xmm0, xmm1 ; multiply by 1.234 (held in xmm1) movsd QWORD PTR a[0+rax*8], xmm0 ; store to memory address a[i]  Only one instruction here accesses memory, and the performance will be affected greatly by where that memory resides (which level of cache, where in DRAM).\nMost architectures today have 64-byte cache lines: all transfers from main memory (DRAM) to and from cache operate in units of 64 bytes.\nLet\u0026rsquo;s compare three code samples for (int i=0; i\u0026lt;N; i++) a[i] = b[i];  for (int i=0; i\u0026lt;N; i++) a[i] = b[(i*8) % N];  for (int i=0; i\u0026lt;N; i++) a[i] = b[random() % N];  What happens when you request a cache line? Operating system effects Most systems today use virtual addressing, so every address in your program needs to be translated to a physical address before looking for it (in cache or memory). Fortunately, there is hardware to assist with this: the Translation Lookaside Buffer (TLB).\nFurther resources  Julia Evans (2016): How much memory is my process using? Gustavo Duarte (2009): Cache: a place for concealment and safekeeping Gustavo Duarte (2009): Getting Physical With Memory Ulrich Drepper (2007): What Every Programmer Should Know About Memory  ","date":1567001418,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567206505,"objectID":"78f1d8586b6b21bb4f04a177e8e4340f","permalink":"https://cucs-hpsc.github.io/fall2019/intro-architecture/","publishdate":"2019-08-28T08:10:18-06:00","relpermalink":"/fall2019/intro-architecture/","section":"fall2019","summary":"Cores, caches, and memory A von Neumann Architecture A contemporary architecture My laptop We can get this kind of information for our machine using hwloc, which provides a library as well as the command-line tool lstopo.\n!lstopo --output-format svg \u0026gt; lstopo-local.svg  A double-socket compute node with two GPUs 2x Xeon Ivy-Bridge-EP E5-2680v2 + 2x NVIDIA GPUs (from 2013, with hwloc v1.11). GPUs are reported as CUDA devices and X11 display :1.","tags":null,"title":"Intro to Architecture","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://cucs-hpsc.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":"https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/\nhttps://www.karlrupp.net/2013/06/cpu-gpu-and-mic-hardware-characteristics-over-time/\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"caba13451aa50e0e0e40912f3d66e772","permalink":"https://cucs-hpsc.github.io/fall2019/trends/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/fall2019/trends/","section":"fall2019","summary":"https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/\nhttps://www.karlrupp.net/2013/06/cpu-gpu-and-mic-hardware-characteristics-over-time/","tags":null,"title":"Trends","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://cucs-hpsc.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://cucs-hpsc.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://cucs-hpsc.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566796546,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://cucs-hpsc.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]